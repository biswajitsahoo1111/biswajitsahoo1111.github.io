<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Biswajit Sahoo</title>
    <link>https://biswajitsahoo1111.github.io/category/python/</link>
      <atom:link href="https://biswajitsahoo1111.github.io/category/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2023 Biswajit Sahoo</copyright><lastBuildDate>Sat, 29 Jun 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://biswajitsahoo1111.github.io/media/icon_hu9aa3fb3801987e3e09a10d02b6e0c908_114644_512x512_fill_lanczos_center_3.png</url>
      <title>Python</title>
      <link>https://biswajitsahoo1111.github.io/category/python/</link>
    </image>
    
    <item>
      <title>Using Python Generators</title>
      <link>https://biswajitsahoo1111.github.io/post/using-python-generators/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/using-python-generators/</guid>
      <description>&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
  &lt;td&gt;
    &lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb&#34;&gt;
    &lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
    Run in Google Colab&lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb&#34;&gt;
    &lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
    View source on GitHub&lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/ax24jc2rdmg4dlo/Using_python_generators.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
  &lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can&amp;rsquo;t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful for beginners.&lt;/p&gt;
&lt;p&gt;While functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It&amp;rsquo;s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.&lt;/p&gt;
&lt;p&gt;First let&amp;rsquo;s create artificial data that we will extract later batch by batch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;, size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permutation(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels:&amp;#34;&lt;/span&gt;, labels)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]

 [[126 129]
  [116 146]]

 [[145 104]
  [143 143]]

 [[114 122]
  [102 148]]

 [[122 118]
  [145 134]]

 [[131 134]
  [122 104]]

 [[145 103]
  [136 138]]

 [[128 119]
  [141 118]]

 [[106 115]
  [124 130]]]
labels: [3 5 8 4 0 9 1 6 7 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;my_gen&lt;/span&gt;(data, labels, batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; len(labels):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permutation(len(labels))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            data, labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[idx], labels[idx]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch_size:(i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch_size,:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; labels[i&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch_size:(i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch_size]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; X,y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found &lt;a href=&#34;https://stackoverflow.com/questions/42727412/efficient-way-to-partially-read-large-numpy-file&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using &amp;ldquo;np.shuffle()&amp;rdquo;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;get_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; my_gen(data,labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X,y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next(get_data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(X,y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape, y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;=========================&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]] [3 5]
(2, 2, 2) (2,)
=========================
[[[126 129]
  [116 146]]

 [[145 104]
  [143 143]]] [8 4]
(2, 2, 2) (2,)
=========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
=========================
[[[131 134]
  [122 104]]

 [[145 103]
  [136 138]]] [1 6]
(2, 2, 2) (2,)
=========================
[[[128 119]
  [141 118]]

 [[106 115]
  [124 130]]] [7 2]
(2, 2, 2) (2,)
=========================
[[[132 119]
  [126 119]]

 [[145 104]
  [143 143]]] [3 4]
(2, 2, 2) (2,)
=========================
[[[131 134]
  [122 104]]

 [[126 129]
  [116 146]]] [1 8]
(2, 2, 2) (2,)
=========================
[[[133 126]
  [144 140]]

 [[106 115]
  [124 130]]] [5 2]
(2, 2, 2) (2,)
=========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
=========================
[[[128 119]
  [141 118]]

 [[145 103]
  [136 138]]] [7 6]
(2, 2, 2) (2,)
=========================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Tensorflow Version: &amp;#34;&lt;/span&gt;, tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__version__)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Tensorflow Version:  2.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;my_new_gen&lt;/span&gt;(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequence):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data, labels, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; ):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data, labels
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices[idx &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size:(idx &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x[inds]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y[inds]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; batch_x, batch_y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_epoch_end&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case we must add &lt;code&gt;len&lt;/code&gt; method and &lt;code&gt;getitem&lt;/code&gt; method within the class and if we want to shuffle data between epochs, we have to add &lt;code&gt;on_epoch_end()&lt;/code&gt; method. &lt;code&gt;len&lt;/code&gt; finds out the number of batches possible in an epoch and &lt;code&gt;getitem&lt;/code&gt; extracts batches one by one. When one epoch is complete, &lt;code&gt;on_epoch_end()&lt;/code&gt; shuffles the data and the process continues. We will test it with an example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;get_new_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; my_new_gen(data, labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        get_new_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;on_epoch_end()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    dat,labs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_new_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__getitem__(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(dat,labs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(dat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape, labs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;===========================&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]] [3 5]
(2, 2, 2) (2,)
===========================
[[[126 129]
  [116 146]]

 [[145 104]
  [143 143]]] [8 4]
(2, 2, 2) (2,)
===========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
===========================
[[[131 134]
  [122 104]]

 [[145 103]
  [136 138]]] [1 6]
(2, 2, 2) (2,)
===========================
[[[128 119]
  [141 118]]

 [[106 115]
  [124 130]]] [7 2]
(2, 2, 2) (2,)
===========================
[[[145 103]
  [136 138]]

 [[133 126]
  [144 140]]] [6 5]
(2, 2, 2) (2,)
===========================
[[[126 129]
  [116 146]]

 [[122 118]
  [145 134]]] [8 9]
(2, 2, 2) (2,)
===========================
[[[145 104]
  [143 143]]

 [[128 119]
  [141 118]]] [4 7]
(2, 2, 2) (2,)
===========================
[[[131 134]
  [122 104]]

 [[114 122]
  [102 148]]] [1 0]
(2, 2, 2) (2,)
===========================
[[[132 119]
  [126 119]]

 [[106 115]
  [124 130]]] [3 2]
(2, 2, 2) (2,)
===========================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both the generators work fine. Now we will use it to implement a CNN model on MNIST data. Note that this example is bit stretched and strange. We don&amp;rsquo;t need generators to implement small data sets like MNIST. Whole of MNIST can be loaded into RAM. By this example the aim is just to show a different way of implementing it using generators. Of course the codes can be modified to handle cases where we indeed need generators to do analysis.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras.models &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Sequential
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; layers
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(train_data, train_labels),(test_data,test_labels) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datasets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mnist&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_data()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;60000&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permutation(len(train_labels))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;training_data, training_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_data[id[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;48000&lt;/span&gt;]], train_labels[id[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;48000&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val_data, val_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_data[id[&lt;span style=&#34;color:#ae81ff&#34;&gt;48000&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;60000&lt;/span&gt;]], train_labels[id[&lt;span style=&#34;color:#ae81ff&#34;&gt;48000&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;60000&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Sequential([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, activation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;, input_shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MaxPool2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,activation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MaxPool2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Flatten(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;,activation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, activation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;categorical_crossentropy&amp;#39;&lt;/span&gt;, metrics &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Keras requires the generator to run indefinitely&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data_gen&lt;/span&gt;(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequence):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data, labels, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data, labels
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; int(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ceil(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices[idx &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size:(idx &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x[inds]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;y[inds]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; batch_x, tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_categorical(batch_y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_epoch_end&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;indices)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_gen(train_data, train_labels,batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val_gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_gen(val_data, val_labels,batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;steps_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(len(train_labels)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;batch_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val_steps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(len(val_labels)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;batch_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_gen, steps_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; steps_per_epoch, epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          validation_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; val_gen, validation_steps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; val_steps)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Epoch 1/10
468/468 [==============================] - 10s 10ms/step - loss: 0.5769 - accuracy: 0.8351 - val_loss: 0.0858 - val_accuracy: 0.9716
Epoch 2/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0454 - val_accuracy: 0.9860
Epoch 3/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0512 - accuracy: 0.9839 - val_loss: 0.0377 - val_accuracy: 0.9883
Epoch 4/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0389 - accuracy: 0.9879 - val_loss: 0.0278 - val_accuracy: 0.9908
Epoch 5/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.0279 - val_accuracy: 0.9899
Epoch 6/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0170 - val_accuracy: 0.9950
Epoch 7/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0118 - val_accuracy: 0.9966
Epoch 8/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0146 - val_accuracy: 0.9952
Epoch 9/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0107 - val_accuracy: 0.9974
Epoch 10/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0078 - val_accuracy: 0.9977

&amp;lt;tensorflow.python.keras.callbacks.History at 0x2543f4e64c0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_loss, test_accuracy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;evaluate(test_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;, tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_categorical(test_labels), verbose &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test Loss:&amp;#34;&lt;/span&gt;, test_loss)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test Accuracy:&amp;#34;&lt;/span&gt;, test_accuracy)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;313/313 - 1s - loss: 0.0282 - accuracy: 0.9922
Test Loss: 0.0281691811978817
Test Accuracy: 0.9922000169754028
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have reached close to 99% accuracy which is not bad! This example might seem a bit stretched as we don&amp;rsquo;t need generators for small datasets like MNIST. The aim of the example is just to show different implementation using generators.&lt;/p&gt;
&lt;p&gt;Perhaps the most detailed blog about using generators for deep learning is &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt;. I also found &lt;a href=&#34;https://github.com/keras-team/keras/issues/9707#issuecomment-374609666&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these comments&lt;/a&gt; helpful.&lt;/p&gt;
&lt;p&gt;Update 1: With the release of &lt;code&gt;Tensorflow-2.0&lt;/code&gt;, it is much easier to use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API for handling large datasets. Generators can still be used for training using &lt;code&gt;tf.keras&lt;/code&gt;. As a final note, use generators if it is absolutely essential to do so. Otherwise, use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API. Check out &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Update 2: See &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog&lt;/a&gt; for a complete workflow for reading multiple files using &lt;code&gt;Tensorflow Sequence&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part III</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/</guid>
      <description>&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/PCA_Abdi_in_python.ipynb&#34;&gt;
    &lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
    Run Python code in Google Colab&lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/gn415uh3quou3a8/PCA_Abdi_in_python.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download Python code&lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/tp56l6v7upa1apf/PCA_blog_Biswajit_Sahoo_part_3.Rmd?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download R code (R Markdown)&lt;/a&gt;
  &lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will reproduce the results of a popular paper on PCA. The paper is titled &amp;lsquo;&lt;a href=&#34;https://personal.utdallas.edu/~herve/abdi-awPCA2010.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Principal component analysis&lt;/a&gt;&amp;rsquo; and is authored by &lt;a href=&#34;https://personal.utdallas.edu/~herve/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Herve Abdi&lt;/a&gt; and &lt;a href=&#34;https://ljwilliams.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lynne J. Williams&lt;/a&gt;. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).&lt;/p&gt;
&lt;p&gt;This post is Part-III of a three part series on PCA. Other parts of the series can be found at the links below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-I: Basic Theory of PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II: PCA Implementation with and without using built-in functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post contains code snippets in R. Equivalent &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MATLAB codes&lt;/a&gt; can be written using commands of &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt;. For figures, the reader has to write his/her own code in MATLAB.&lt;/p&gt;
&lt;h3 id=&#34;structure-of-the-paper&#34;&gt;Structure of the paper&lt;/h3&gt;
&lt;p&gt;Along with basic theory, the paper contains three examples on PCA, one example on correspondence analysis, and one example on multiple factor analysis. We will only focus on PCA examples in this post.&lt;/p&gt;
&lt;p&gt;To run following R codes seamlessly, readers have to load following packages. If these packages have not been installed previously, use &lt;code&gt;install.packages(&amp;quot;package_name&amp;quot;)&lt;/code&gt; to install those.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;knitr::opts_chunk$set(comment = NA)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggrepel)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;how-to-get-data&#34;&gt;How to get data&lt;/h3&gt;
&lt;p&gt;Data for the examples have been taken from the paper [1]. The datasets are pretty small. So one way to read the data is to create a dataframe itself in R using the values given in paper. Otherwise, the values can first be stored in a csv file and then read into R. To make this post self-sufficient, we will adopt the former approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Throughout this article, additional comments have been made beside code segments. It would be a good idea to read those commented lines along with the codes.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 1
# Create a dataframe
Words = c(&amp;#34;Bag&amp;#34;, &amp;#34;Across&amp;#34;, &amp;#34;On&amp;#34;, &amp;#34;Insane&amp;#34;, &amp;#34;By&amp;#34;, &amp;#34;Monastery&amp;#34;, &amp;#34;Relief&amp;#34;, &amp;#34;Slope&amp;#34;, &amp;#34;Scoundrel&amp;#34;, &amp;#34;With&amp;#34;, &amp;#34;Neither&amp;#34;, &amp;#34;Pretentious&amp;#34;, &amp;#34;Solid&amp;#34;, &amp;#34;This&amp;#34;, &amp;#34;For&amp;#34;, &amp;#34;Therefore&amp;#34;, &amp;#34;Generality&amp;#34;, &amp;#34;Arise&amp;#34;, &amp;#34;Blot&amp;#34;, &amp;#34;Infectious&amp;#34;)
Word_length = c(3, 6, 2, 6, 2, 9, 6, 5, 9, 4, 7, 11, 5, 4, 3, 9, 10, 5, 4, 10)
Lines_in_dict = c(14, 7, 11, 9, 9, 4, 8, 11, 5, 8, 2, 4, 12, 9, 8, 1, 4, 13, 15, 6)
words = data.frame(Words, Word_length, Lines_in_dict, stringsAsFactors = F)
words
(words_centered = scale(words[,2:3],scale = F)) # Centering after reemoving the first column
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;covariance-pca&#34;&gt;Covariance PCA&lt;/h3&gt;
&lt;p&gt;Covariance PCA uses centered data matrix. But data matrix is not scaled. &lt;code&gt;prcomp()&lt;/code&gt; centers data by default.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA
factor_scores_words = pca_words_cov$x
round(factor_scores_words,2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Observer that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as principal directions are orthogonal.&lt;/p&gt;
&lt;h3 id=&#34;principal-directions-are-orthogonal&#34;&gt;Principal directions are orthogonal&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#  It can also be checked that both the principal components are orthogonal.
sum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;contribution-of-each-factor&#34;&gt;Contribution of each factor&lt;/h3&gt;
&lt;p&gt;It is defined as square of factor score divided by sum of squares of factor scores in that column.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;round(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)
round(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The calculations in above two lines can be done in a single line&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;round(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-distance-to-center-of-gravity&#34;&gt;Squared distance to center of gravity&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(dist = rowSums(factor_scores_words^2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-cosine-of-observations&#34;&gt;Squared cosine of observations&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(sq_cos = round(factor_scores_words^2/rowSums(factor_scores_words^2)*100))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Nan&amp;rsquo;s are produced because of division by zero.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Figue 1
p = ggplot(words,aes(x = Lines_in_dict,y = Word_length,label = Words))+
  geom_point()+ geom_text_repel()+ 
  geom_hline(yintercept = 6)+geom_vline(xintercept = 8)+
  labs(x = &amp;#34;Lines in dictionary&amp;#34;,y = &amp;#34;Word length&amp;#34;)
print(p)
# Show directions of PCs
# Note that intercept argument in geom_abline considers the line to be at the origin. In our case the data are mean shifted.
# So we have to adjust the intercept taking new origin into consideration. These adjustments have been made below.
slope1 = pca_words_cov$rotation[1,1]/pca_words_cov$rotation[2,1] # Slope of first PC
slope2 = pca_words_cov$rotation[1,2]/pca_words_cov$rotation[2,2] # Slope of second PC
(new_origin = c(mean(words$Lines_in_dict),mean(words$Word_length)))
intercept1 = 6 - slope1*8
intercept2 = 6 - slope2*8
p+geom_abline(slope = slope1,intercept = intercept1,linetype = &amp;#34;dashed&amp;#34;,size = 1.2,col = &amp;#34;red&amp;#34;)+
  geom_abline(slope = slope2,intercept = intercept2,linetype = &amp;#34;dashed&amp;#34;,size = 1.2,col = &amp;#34;blue&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the above figure red dashed line is the 1st principal component (PC) and blue dashed line is the 2nd PC.&lt;/p&gt;
&lt;h3 id=&#34;rotated-pcs&#34;&gt;Rotated PCs&lt;/h3&gt;
&lt;p&gt;This figure is obtained by plotting factor scores. Note that we will plot negative of the factor scores of 1st PC to make the figure consistent with the paper.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ggplot(as.data.frame(pca_words_cov$x),aes(-pca_words_cov$x[,1],pca_words_cov$x[,2],label = words$Words))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&amp;#34;Factor score along PC1&amp;#34;)+ylab(&amp;#34;Factor score along PC2&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;with-supplementary-data&#34;&gt;With supplementary data&lt;/h3&gt;
&lt;p&gt;Given a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix.&lt;/p&gt;
&lt;h3 id=&#34;factor-score-of-the-new-word-sur&#34;&gt;Factor score of the new word &amp;lsquo;sur&amp;rsquo;&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sur = c(3,12) # It has 3 letter and 12 lines of dictionary entry
(sur_centered = sur - colMeans(words[,2:3]))
(factor_scores_sur = round(sur_centered %*% pca_words_cov$rotation,2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;eigenvalues-and-variance&#34;&gt;Eigenvalues and variance&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id=&#34;total-variance-before-transformation&#34;&gt;Total variance before transformation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(total_var_before = round(sum(diag(var(words_centered))),3))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;total-variance-after-transformation&#34;&gt;Total variance after transformation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(total_var_after = round(sum(diag(var(pca_words_cov$x))),3))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Correlation between principal components and original variables
(In the paper,this correlation is also termed loading. But we will strictly reserve the loading term to mean loading matrix $\textbf{P}$ (see &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-I&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The sum of correlation coefficients between variables and principal components is 1. Intuitively, this means that variables are orthogonally projected onto the principal components.&lt;/p&gt;
&lt;h3 id=&#34;correlation-matrix&#34;&gt;Correlation matrix&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Correlation between PCs and original variables
(cor(pca_words_cov$x,words_centered))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the answers for correlation coefficients don&amp;rsquo;t match with that of the paper. Readers who get actual answers as given in paper are encouraged to send me an email using my contact details. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below.&lt;/p&gt;
&lt;h3 id=&#34;squared-correlation&#34;&gt;Squared correlation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(cor(pca_words_cov$x,words_centered)^2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Sum of correlation coefficients between variables and principal components is 1.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;colSums((cor(pca_words_cov$x,words_centered)^2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;loading-matrix&#34;&gt;Loading matrix&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(loading_matrix = pca_words_cov$rotation)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-score-for-supplementary-variables&#34;&gt;Correlation score for supplementary variables&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Supplementary variable (Table 4)
Frequency = c(8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1)
Num_entries = c(6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2)
supp_data = data.frame(Frequency,Num_entries) # Supplementary data
supp_data
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;centered-supplementary-data&#34;&gt;Centered supplementary data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;supp_data_cent = scale(supp_data,scale = F) # Centered supplementary data
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-score-for-supplementary-data&#34;&gt;Correlation score for supplementary data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(corr_score_supp = round(cor(pca_words_cov$x,supp_data),4))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that correlation score doesn&amp;rsquo;t depend on whether supplementary data is centered or not.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(cor(pca_words_cov$x,supp_data_cent),4))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-correlation-1&#34;&gt;Squared correlation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(cor(pca_words_cov$x,supp_data_cent)^2,4))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;column-sums-of-squared-correlation-for-support-data&#34;&gt;Column sums of squared correlation for support data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(colSums(cor(pca_words_cov$x,supp_data_cent)^2),4))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-circle-plot&#34;&gt;Correlation circle plot&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# First plot correlation circle
x = seq(0,2*pi,length.out = 300)
circle = ggplot() + geom_path(data = data.frame(a = cos(x),b = sin(x)),
                     aes(cos(x),sin(x)),alpha = 0.3, size = 1.5)+
            geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  annotate(&amp;#34;text&amp;#34;,x = c(1.08,0.05),y = c(0.05,1.08),label = c(&amp;#34;PC1&amp;#34;,&amp;#34;PC2&amp;#34;),angle = c(0,90))+
            xlab(NULL)+ylab(NULL)
# Plotting original variables
cor_score = as.data.frame(cor(words_centered,pca_words_cov$x))
variable_plot_original = circle + geom_point(data = cor_score,  aes(cor_score[,1],cor_score[,2]))+
  geom_text_repel(aes(cor_score[,1],cor_score[,2],
                      label = c(&amp;#34;Length of words&amp;#34;,&amp;#34;Number of lines in Dict.&amp;#34;))) 
print(variable_plot_original)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;plotting-supplementary-variables&#34;&gt;Plotting supplementary variables&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;variable_plot_original+
  geom_point(data = as.data.frame(corr_score_supp),
             aes(corr_score_supp[,1],corr_score_supp[,2]))+
  geom_text_repel(aes(corr_score_supp[,1],corr_score_supp[,2],
                      label = c(&amp;#34;Frequency&amp;#34;,&amp;#34;Number of entries&amp;#34;))) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Observe that our correlation circle plot is flipped about y-axis (i.e., PC2) when compared to the plot given in paper. This is because our first principal component is negative of the one given in paper. So while computing correlation score, this negative principal component results in negative correlation scores. Hence, our plot flips about y-axis.&lt;/p&gt;
&lt;h2 id=&#34;example-2-wine-example&#34;&gt;Example 2 (Wine example)&lt;/h2&gt;
&lt;h3 id=&#34;correlation-pca-with-wine-data&#34;&gt;Correlation PCA with wine data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 6
wine_type = c(paste(&amp;#34;wine&amp;#34;, 1:5, sep = &amp;#34;_&amp;#34;))
hedonic = c(14, 10, 8, 2, 6)
for_meat = c(7, 7, 5, 4, 2)
for_dessert = c(8, 6, 5, 7, 4)
price = c(7, 4, 10, 16, 13)
sugar = c(7, 3, 5, 7, 3)
alcohol = c(13, 14, 12, 11, 10)
acidity = c(7, 7, 5, 3, 3)
wine = data.frame(wine_type, hedonic, for_meat, for_dessert, price, sugar, alcohol, acidity, stringsAsFactors = F)
wine
pca_wine_cor = prcomp(wine[2:8],scale = T)
ggplot(as.data.frame(pca_wine_cor$x),aes(x = pca_wine_cor$x[,1],y =  pca_wine_cor$x[,2], label = paste0(&amp;#34;wine &amp;#34;,1:5)))+
  geom_point()+geom_text_repel()+ geom_vline(xintercept = 0)+ geom_hline(yintercept = 0)+
  xlab(&amp;#34;Factor score along PC1&amp;#34;)+ylab(&amp;#34;Factor score along PC2&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again our figure seems upside down than that of the paper. This is a minor discrepancy. Our 2nd eigenvector is negative of the one considered in paper. We can match the plot with that of the paper by just flipping the second principal component but we will not do that here.&lt;/p&gt;
&lt;h3 id=&#34;factor-scores-along-1st-and-2nd-pc&#34;&gt;Factor scores along 1st and 2nd PC&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 7
(pca_wine_cor$x[,1:2])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;contribution-of-each-observation-to-principal-component&#34;&gt;Contribution of each observation to principal component&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;round(pca_wine_cor$x[,1:2]^2/matrix(rep(colSums(pca_wine_cor$x[,1:2]^2),nrow(wine)),ncol = 2,byrow = T)*100,2)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-cosine-of-observations-of-first-pc&#34;&gt;Squared cosine of observations of first PC&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(sq_cos = round(pca_wine_cor$x[,1:2]^2/rowSums(pca_wine_cor$x^2)*100))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;loading-scores-corresponding-to-first-two-principal-components&#34;&gt;Loading scores corresponding to first two principal components&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2],2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-score-variables-with-first-two-principal-components&#34;&gt;Correlation score variables with first two principal components&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(corr_score_wine = round(cor(pca_wine_cor$x,wine[,2:8])[1:2,],2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-circle-for-wine-data&#34;&gt;Correlation circle for wine data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Figure 6
corr_score_wine = t(corr_score_wine)
circle + 
  geom_point(data = as.data.frame(corr_score_wine),
             aes(corr_score_wine[,1],corr_score_wine[,2]))+
  geom_text_repel(aes(corr_score_wine[,1],corr_score_wine[,2],
                      label = c(&amp;#34;Hedonic&amp;#34;,&amp;#34;For Meat&amp;#34;,&amp;#34;For dessert&amp;#34;,&amp;#34;Price&amp;#34;,&amp;#34;Sugar&amp;#34;,&amp;#34;Alcohol&amp;#34;,&amp;#34;Acidity&amp;#34;)))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;varimax-rotation&#34;&gt;Varimax rotation&lt;/h3&gt;
&lt;p&gt;Rotation is applied to loading matrix such that after rotation principal components are interpretable. By interpretable, we mean, some of the loading scores will have higher values and some other loading scores will have lower values. So it can be said that the variables whose loading scores have higher value, contribute significantly towards principal components as compared to other variables with lesser loading scores. Though rotation works in certain cases, it must be remembered that it is no magic wand for principal component interpretability. One of the popular rotations is Varimax rotation. R has a built-in command to perform varimax rotation.&lt;/p&gt;
&lt;p&gt;Varimax rotation can be performed on the whole loading matrix or on a few components only. In the paper, varimax has been applied to first two principal components.&lt;/p&gt;
&lt;h3 id=&#34;loading-scores-of-first-two-principal-components&#34;&gt;Loading scores of first two principal components&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2],2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;varimax-applied-to-first-two-principal-components&#34;&gt;Varimax applied to first two principal components&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rotated_loading_scores = varimax(pca_wine_cor$rotation[,1:2])
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;loading-scores-after-rotation&#34;&gt;Loading scores after rotation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 10
(round(rotated_loading_scores$loadings[,1:2],2))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The same result can also be obtained by multiplying the original loading matrix by the rotation matrix obtained from varimax.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2] %*% rotated_loading_scores$rotmat,2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;plot-of-loading-scores-before-rotation&#34;&gt;Plot of loading scores before rotation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#Figure 7
ggplot(as.data.frame(pca_wine_cor$rotation[,1:2]),aes(x = pca_wine_cor$rotation[,1],y = pca_wine_cor$rotation[,2],
                                                      label = c(&amp;#34;Hedonic&amp;#34;,&amp;#34;For Meat&amp;#34;,&amp;#34;For dessert&amp;#34;,&amp;#34;Price&amp;#34;,&amp;#34;Sugar&amp;#34;,&amp;#34;Alcohol&amp;#34;,&amp;#34;Acidity&amp;#34;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&amp;#34;Loading score along PC1&amp;#34;)+ylab(&amp;#34;Loading score along PC2&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;plot-of-loading-scores-after-rotation&#34;&gt;Plot of loading scores after rotation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ggplot(as.data.frame(rotated_loading_scores$loadings[,1:2]),
                     aes(x = rotated_loading_scores$loadings[,1],
                         y = rotated_loading_scores$loadings[,2],
                         label = c(&amp;#34;Hedonic&amp;#34;,&amp;#34;For Meat&amp;#34;,&amp;#34;For dessert&amp;#34;,&amp;#34;Price&amp;#34;,&amp;#34;Sugar&amp;#34;,&amp;#34;Alcohol&amp;#34;,&amp;#34;Acidity&amp;#34;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
    xlab(&amp;#34;Loading score along PC1 after rotation&amp;#34;)+
    ylab(&amp;#34;Loading score along PC2 after rotation&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;example-3&#34;&gt;Example 3&lt;/h2&gt;
&lt;h3 id=&#34;french-food-example-covariance-pca-example&#34;&gt;French food example (Covariance PCA example)&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 11 
class = rep(c(&amp;#34;Blue_collar&amp;#34;, &amp;#34;White_collar&amp;#34;, &amp;#34;Upper_class&amp;#34;), times = 4)
children = rep(c(2,3,4,5), each = 3)
bread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)
vegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)
fruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)
meat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)
poultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)
milk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)
wine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)
food = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)
food
pca_food_cov = prcomp(food[,3:9],scale = F)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;factor-scores&#34;&gt;Factor scores&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 12
(factor_scores_food = round(pca_food_cov$x[,1:2],2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;contribution-of-each-observation-to-principal-component-1&#34;&gt;Contribution of each observation to principal component&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;round(pca_food_cov$x[,1:2]^2/matrix(rep(colSums(pca_food_cov$x[,1:2]^2),nrow(food)),ncol = 2,byrow = T)*100,2)
dist = pca_food_cov$x[,1]^2+pca_food_cov$x[,2]^2
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-cosine-of-observations-1&#34;&gt;Squared cosine of observations&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(sq_cos = round(pca_food_cov$x[,1:2]^2/rowSums(pca_food_cov$x^2)*100))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-loading-scores&#34;&gt;Squared loading scores&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Table 13
(round(pca_food_cov$rotation[,1:2]^2,2))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that this table doesn&amp;rsquo;t match with that of the paper. We will stick to our analysis.&lt;/p&gt;
&lt;h3 id=&#34;correlation-score&#34;&gt;Correlation score&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(corr_score_food = round((cor(pca_food_cov$x,food[,3:9])[1:2,]),2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;squared-correlation-score&#34;&gt;Squared correlation score&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round((cor(pca_food_cov$x,food[,3:9])[1:2,])^2,2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correlation-circle-for-food-data&#34;&gt;Correlation circle for food data&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Figure 9
corr_score_food = t(corr_score_food)
circle + geom_point(data = as.data.frame(corr_score_food), 
                    aes(x = corr_score_food[,1],y = corr_score_food[,2]))+
  geom_text_repel(data = as.data.frame(corr_score_food), 
                  aes(x = corr_score_food[,1],y = corr_score_food[,2],
                      label = c(&amp;#34;Bread&amp;#34;,&amp;#34;Vegetables&amp;#34;,&amp;#34;Fruit&amp;#34;,&amp;#34;Meat&amp;#34;,&amp;#34;Poultry&amp;#34;,&amp;#34;Milk&amp;#34;,&amp;#34;Wine&amp;#34;)))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now observe that our correlation circle plot is almost close to that of the papers (though in opposite quadrants. But this is not a problem as we have previously mentioned).&lt;/p&gt;
&lt;h3 id=&#34;eigenvalues&#34;&gt;Eigenvalues&lt;/h3&gt;
&lt;p&gt;Eigenvalues of data covariance matrix is square of singular values of centered data matrix. Hence eigenvalues of data covariance matrix can be obtained as below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## Table 14
cent_food = food[,3:9]-matrix(rep(colMeans(food[,3:9]),times = 12),nrow = 12,
                              byrow = T)
svd_food = svd(cent_food)
(Eigenvalues = (svd_food$d)^2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; These eigenvalues are not the same as variance of factor scores in principal components. Variance of principal component factor scores can be obtained by dividing the eigenvalues by $(n-1)$, where $n$ is number of data points (in this case $n = 12$). If this point is still not clear, refer to &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;percentage-contribution-of-each-principal-component&#34;&gt;Percentage contribution of each principal component&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(Eigenvalues/sum(Eigenvalues),2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;cumulative-sum-of-eigenvalues&#34;&gt;Cumulative sum of eigenvalues&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(cumsum(Eigenvalues),2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;cumulative-percentage-contribution&#34;&gt;Cumulative percentage contribution&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(round(cumsum(Eigenvalues)/sum(Eigenvalues),2))
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;ress-refer-to-the-paper-for-a-description&#34;&gt;RESS (Refer to the paper for a description)&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;RESS = array(rep(0,7))
for (i in 1:7){
  RESS[i] = sum(Eigenvalues)-sum(Eigenvalues[1:i])
}
RESS
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;ratio-of-ress-and-sum-of-eigenvalues&#34;&gt;Ratio of RESS and sum of eigenvalues&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;round(RESS/sum(Eigenvalues),2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We will not calculate the value of PRESS in this post as it requires us to consider random models. We will not pursue that here.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Though unusually long, I hope, this post will be of help to (courageous) readers who work there way through it till end. Comments regarding any errors or omissions may be sent to &lt;a href=&#34;https://biswajitsahoo1111.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;author&amp;rsquo;s&lt;/a&gt; email.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last updated: 19th January, 2020&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part II</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/</guid>
      <description>&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Principal_Component_Analysis_Part_II.ipynb&#34;&gt;
    &lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
    Run Python code in Google Colab&lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/zkftnkv31neuxgq/Principal_Component_Analysis_Part_II.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download Python code&lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/7bzat96tt6r9iks/Principal_component_analysis_part_II.Rmd?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download R code (R Markdown)&lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://www.dropbox.com/s/u9gbbviswkfgmsj/pca_part_2_MATLAB_code.pdf?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download MATLAB code&lt;/a&gt;
  &lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;This post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-I: Basic Theory of PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-III: Reproducing results of a published paper on PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions. Rather, in our opinion, it enhances understanding by knowing what happens under the hood when a built-in function is called. In actual applications, readers should always use built functions as they are robust(almost always) and tested for efficiency.&lt;/p&gt;
&lt;p&gt;This post is written in R. Equivalent &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MATLAB code&lt;/a&gt; for the same can be obtained from this &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use French food data form reference [2]. Refer to the paper to know about the original source of the data. We will apply different methods to this data and compare the result. As the dataset is pretty small, one way to load the data into R is to create a dataframe in R using the values in the paper. Another way is to first create a csv file and then read the file into R/MATLAB. We have used the later approach.&lt;/p&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load Data&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Create a dataframe of food data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;class &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Blue_collar&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;White_collar&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Upper_class&amp;#34;&lt;/span&gt;), times &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;children &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;), each &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bread &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;332&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;293&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;372&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;406&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;386&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;438&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;534&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;460&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;385&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;655&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;584&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;515&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vegetables &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;428&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;559&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;767&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;563&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;608&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;843&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;660&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;699&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;789&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;776&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;995&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1097&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fruit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;354&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;388&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;562&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;341&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;396&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;689&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;367&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;484&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;621&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;423&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;548&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;887&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;meat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1437&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1527&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1948&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1507&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1501&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2345&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1620&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1856&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2366&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1848&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2056&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2630&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;poultry &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;526&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;567&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;927&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;544&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;558&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1148&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;638&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;762&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1149&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;759&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;893&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1167&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;milk &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;247&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;239&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;235&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;324&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;319&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;243&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;414&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;400&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;304&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;495&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;518&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;561&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wine &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;427&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;258&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;433&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;407&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;363&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;341&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;407&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;416&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;282&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;486&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;319&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;284&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;food &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; F)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;food
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;          class children bread vegetables fruit meat poultry milk wine
1   Blue_collar        2   332        428   354 1437     526  247  427
2  White_collar        2   293        559   388 1527     567  239  258
3   Upper_class        2   372        767   562 1948     927  235  433
4   Blue_collar        3   406        563   341 1507     544  324  407
5  White_collar        3   386        608   396 1501     558  319  363
6   Upper_class        3   438        843   689 2345    1148  243  341
7   Blue_collar        4   534        660   367 1620     638  414  407
8  White_collar        4   460        699   484 1856     762  400  416
9   Upper_class        4   385        789   621 2366    1149  304  282
10  Blue_collar        5   655        776   423 1848     759  495  486
11 White_collar        5   584        995   548 2056     893  518  319
12  Upper_class        5   515       1097   887 2630    1167  561  284
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Centerd data matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cent_food &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;scale&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;],scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; F)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Scaled data matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;scale_food &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;scale&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;],scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;covariance-pca&#34;&gt;Covariance PCA&lt;/h2&gt;
&lt;h3 id=&#34;using-built-in-function&#34;&gt;Using built-in function&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Using built-in function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pca_food_cov &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;prcomp&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;],scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; F)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores (we have printed only four columns out of seven)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rotation[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;             PC1   PC2   PC3   PC4
bread       0.07 -0.58 -0.40  0.11
vegetables  0.33 -0.41  0.29  0.61
fruit       0.30  0.10  0.34 -0.40
meat        0.75  0.11 -0.07 -0.29
poultry     0.47  0.24 -0.38  0.33
milk        0.09 -0.63  0.23 -0.41
wine       -0.06 -0.14 -0.66 -0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor score (we have printed only four PCs out of seven)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have printed only four columns of loading scores out of seven.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;          PC1     PC2     PC3    PC4
 [1,] -635.05  120.89  -21.14 -68.97
 [2,] -488.56  142.33  132.37  34.91
 [3,]  112.03  139.75  -61.86  44.19
 [4,] -520.01  -12.05    2.85 -13.70
 [5,] -485.94   -1.17   65.75  11.51
 [6,]  588.17  188.44  -71.85  28.56
 [7,] -333.95 -144.54  -34.94  10.07
 [8,]  -57.51  -42.86  -26.26 -46.55
 [9,]  571.32  206.76  -38.45   3.69
[10,]  -39.38 -264.47 -126.43 -12.74
[11,]  296.04 -235.92   58.84  87.43
[12,]  992.83  -97.15  121.13 -78.39
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We have printed only four principal components out of seven.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variances using built-in function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total variance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 312295.4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;comparison-of-variance-before-and-after-transformation&#34;&gt;Comparison of variance before and after transformation&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total variance before transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;])))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 312295.4
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total variance after transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 312295.4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Another important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along variables before transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;])),&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;     bread vegetables      fruit       meat    poultry       milk       wine 
  11480.61   35789.09   27255.45  156618.39   62280.52   13718.75    5152.63 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that calculation of variance is unaffected by centering data matrix. So variance of original data matrix as well as centered data matrix is same. Check it for yourself. Now see how PCA transforms these variance.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along principal compoennts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x)),&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      PC1       PC2       PC3       PC4       PC5       PC6       PC7 
274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81 
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can obtain the same result using built-in fucntion&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;performing-covariance-pca-manually-using-svd&#34;&gt;Performing covariance PCA manually using SVD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;svd_food_cov &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(cent_food)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(svd_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# We have printed only four columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,]  0.07 -0.58 -0.40  0.11
[2,]  0.33 -0.41  0.29  0.61
[3,]  0.30  0.10  0.34 -0.40
[4,]  0.75  0.11 -0.07 -0.29
[5,]  0.47  0.24 -0.38  0.33
[6,]  0.09 -0.63  0.23 -0.41
[7,] -0.06 -0.14 -0.66 -0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;((cent_food &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; svd_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# only 4 columns printed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;         [,1]    [,2]    [,3]   [,4]
 [1,] -635.05  120.89  -21.14 -68.97
 [2,] -488.56  142.33  132.37  34.91
 [3,]  112.03  139.75  -61.86  44.19
 [4,] -520.01  -12.05    2.85 -13.70
 [5,] -485.94   -1.17   65.75  11.51
 [6,]  588.17  188.44  -71.85  28.56
 [7,] -333.95 -144.54  -34.94  10.07
 [8,]  -57.51  -42.86  -26.26 -46.55
 [9,]  571.32  206.76  -38.45   3.69
[10,]  -39.38 -264.47 -126.43 -12.74
[11,]  296.04 -235.92   58.84  87.43
[12,]  992.83  -97.15  121.13 -78.39
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance of principal components&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(svd_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d^2&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Our data matrix contains 12 data points. So to find variance of principal components we have to divide the square of the diagonal matrix by 11. To know the theory behind it, refer &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-I&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;performing-covariance-pca-using-eigen-decomopositionnot-recommended&#34;&gt;Performing covariance PCA using Eigen-decomoposition(Not recommended)&lt;/h3&gt;
&lt;p&gt;This procedure is not recommended because forming a covariance matrix is computationally not efficient for large matrices if data matrix contains smaller entries. So doing eigen analysis on covariance matrix may give erroneous results. However, for our example we can use it to obtain results.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eigen_food_cov &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(cent_food))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,] -0.07  0.58 -0.40 -0.11
[2,] -0.33  0.41  0.29 -0.61
[3,] -0.30 -0.10  0.34  0.40
[4,] -0.75 -0.11 -0.07  0.29
[5,] -0.47 -0.24 -0.38 -0.33
[6,] -0.09  0.63  0.23  0.41
[7,]  0.06  0.14 -0.66  0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;((cent_food &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; eigen_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;         [,1]    [,2]    [,3]   [,4]
 [1,]  635.05 -120.89  -21.14  68.97
 [2,]  488.56 -142.33  132.37 -34.91
 [3,] -112.03 -139.75  -61.86 -44.19
 [4,]  520.01   12.05    2.85  13.70
 [5,]  485.94    1.17   65.75 -11.51
 [6,] -588.17 -188.44  -71.85 -28.56
 [7,]  333.95  144.54  -34.94 -10.07
 [8,]   57.51   42.86  -26.26  46.55
 [9,] -571.32 -206.76  -38.45  -3.69
[10,]   39.38  264.47 -126.43  12.74
[11,] -296.04  235.92   58.84 -87.43
[12,] -992.83   97.15  121.13  78.39
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along principal components&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_cov&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;values,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Instead of using the &amp;lsquo;cov()&amp;rsquo; command to find the covariance matrix manually and perform its eigen analysis.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cov_matrix_manual_food &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;t&lt;/span&gt;(cent_food) &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; cent_food
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eigen_food_new &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(cov_matrix_manual_food)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_new&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,] -0.07  0.58 -0.40  0.11
[2,] -0.33  0.41  0.29  0.61
[3,] -0.30 -0.10  0.34 -0.40
[4,] -0.75 -0.11 -0.07 -0.29
[5,] -0.47 -0.24 -0.38  0.33
[6,] -0.09  0.63  0.23 -0.41
[7,]  0.06  0.14 -0.66 -0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along principal components&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_new&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;values,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are also different ways to find total variance of the data matrix. We will explore some of the options.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total varaiance before transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(cent_food)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 312295.4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that total variance is invariant to translations. So calculating the total variance on raw data will also give the same answer. Check it to convince yourself.&lt;/p&gt;
&lt;h2 id=&#34;correlation-pca&#34;&gt;Correlation PCA&lt;/h2&gt;
&lt;p&gt;When PCA is performed on a scaled data matrix (each variable is centered as well as variance of each variable is one), it is called correlation PCA. Before discussing correlation PCA we will take some time to see different ways in which we can obtain correlation matrix.&lt;/p&gt;
&lt;h3 id=&#34;different-ways-to-obtain-correlation-matrix&#34;&gt;Different ways to obtain correlation matrix.&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Using built-in command&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cor&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]),&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# We have printed only four columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;           bread vegetables fruit  meat
bread       1.00       0.59  0.20  0.32
vegetables  0.59       1.00  0.86  0.88
fruit       0.20       0.86  1.00  0.96
meat        0.32       0.88  0.96  1.00
poultry     0.25       0.83  0.93  0.98
milk        0.86       0.66  0.33  0.37
wine        0.30      -0.36 -0.49 -0.44
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# manually&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;((&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;t&lt;/span&gt;(scale_food) &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; scale_food,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;           bread vegetables fruit  meat
bread       1.00       0.59  0.20  0.32
vegetables  0.59       1.00  0.86  0.88
fruit       0.20       0.86  1.00  0.96
meat        0.32       0.88  0.96  1.00
poultry     0.25       0.83  0.93  0.98
milk        0.86       0.66  0.33  0.37
wine        0.30      -0.36 -0.49 -0.44
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;performing-correlation-pca-using-built-in-function&#34;&gt;Performing correlation PCA using built-in function&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pca_food_cor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;prcomp&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;],scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rotation[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# Printed only four&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;             PC1   PC2   PC3   PC4
bread       0.24 -0.62  0.01 -0.54
vegetables  0.47 -0.10  0.06 -0.02
fruit       0.45  0.21 -0.15  0.55
meat        0.46  0.14 -0.21 -0.05
poultry     0.44  0.20 -0.36 -0.32
milk        0.28 -0.52  0.44  0.45
wine       -0.21 -0.48 -0.78  0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;        PC1   PC2   PC3   PC4
 [1,] -2.86  0.36 -0.40  0.36
 [2,] -1.89  1.79  1.31 -0.16
 [3,] -0.12  0.73 -1.42  0.20
 [4,] -2.04 -0.32  0.11  0.10
 [5,] -1.69  0.16  0.51  0.16
 [6,]  1.69  1.35 -0.99 -0.43
 [7,] -0.93 -1.37  0.28 -0.26
 [8,] -0.25 -0.63 -0.27  0.29
 [9,]  1.60  1.74 -0.10 -0.40
[10,]  0.22 -2.78 -0.57 -0.25
[11,]  1.95 -1.13  0.99 -0.32
[12,]  4.32  0.10  0.57  0.72
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variances along principal componentes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sum of vairances&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 7
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;comparison-of-variance-before-and-after-transformation-1&#34;&gt;Comparison of variance before and after transformation&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total variance before transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(scale_food)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 7
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Total variance after transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Another important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along variables before transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(scale_food)),&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;     bread vegetables      fruit       meat    poultry       milk       wine 
         1          1          1          1          1          1          1 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is obvious as we have scaled the matrix. Now see how PCA transforms these variance.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along principal compoennts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;diag&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cov&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;x)),&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; PC1  PC2  PC3  PC4  PC5  PC6  PC7 
4.33 1.83 0.63 0.13 0.06 0.02 0.00 
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can obtain the same result using built-in fucntion&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(pca_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdev^2,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;performing-correlation-pca-manually-using-svd&#34;&gt;Performing correlation PCA manually using SVD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;svd_food_cor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;svd&lt;/span&gt;(scale_food)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(svd_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,]  0.24 -0.62  0.01 -0.54
[2,]  0.47 -0.10  0.06 -0.02
[3,]  0.45  0.21 -0.15  0.55
[4,]  0.46  0.14 -0.21 -0.05
[5,]  0.44  0.20 -0.36 -0.32
[6,]  0.28 -0.52  0.44  0.45
[7,] -0.21 -0.48 -0.78  0.31
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;((scale_food &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; svd_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;v)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;       [,1]  [,2]  [,3]  [,4]
 [1,] -2.86  0.36 -0.40  0.36
 [2,] -1.89  1.79  1.31 -0.16
 [3,] -0.12  0.73 -1.42  0.20
 [4,] -2.04 -0.32  0.11  0.10
 [5,] -1.69  0.16  0.51  0.16
 [6,]  1.69  1.35 -0.99 -0.43
 [7,] -0.93 -1.37  0.28 -0.26
 [8,] -0.25 -0.63 -0.27  0.29
 [9,]  1.60  1.74 -0.10 -0.40
[10,]  0.22 -2.78 -0.57 -0.25
[11,]  1.95 -1.13  0.99 -0.32
[12,]  4.32  0.10  0.57  0.72
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variance along each principcal component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(svd_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d^2&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sum of variances&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;(svd_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;d^2&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again we have to divide by 11 to get eigenvalues of correlation matrix. Check the formulation of correlation matrix using scaled data matrix to convince yourself.&lt;/p&gt;
&lt;h3 id=&#34;using-eigen-decomposition-not-recommended&#34;&gt;Using eigen-decomposition (Not Recommended)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eigen_food_cor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;eigen&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;cor&lt;/span&gt;(food[,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loading scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    0    1    0   -1    0    1    0
[2,]    0    0    0    0    1    0    0
[3,]    0    0    0    1    0    1    0
[4,]    0    0    0    0    0    0   -1
[5,]    0    0    0    0    0    0    1
[6,]    0    1    0    0    0    0    0
[7,]    0    0   -1    0    0    0    0
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Factor scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;((scale_food &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; eigen_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;vectors)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;       [,1]  [,2]  [,3]  [,4]
 [1,]  2.86 -0.36 -0.40  0.36
 [2,]  1.89 -1.79  1.31 -0.16
 [3,]  0.12 -0.73 -1.42  0.20
 [4,]  2.04  0.32  0.11  0.10
 [5,]  1.69 -0.16  0.51  0.16
 [6,] -1.69 -1.35 -0.99 -0.43
 [7,]  0.93  1.37  0.28 -0.26
 [8,]  0.25  0.63 -0.27  0.29
 [9,] -1.60 -1.74 -0.10 -0.40
[10,] -0.22  2.78 -0.57 -0.25
[11,] -1.95  1.13  0.99 -0.32
[12,] -4.32 -0.10  0.57  0.72
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Variances along each principal component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(eigen_food_cor&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;values,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I hope this post would help clear some of the confusions that a beginner might have while encountering PCA for the first time. Please send me a note if you find any errors.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.&lt;/li&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part I</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/</guid>
      <description>&lt;p&gt;In this post, we will discuss about Principal Component Analysis (PCA),
one of the most popular dimensionality reduction techniques used in
machine learning. Applications of PCA and its variants are ubiquitous.
Thus, a through understanding of PCA is considered essential to start
one&amp;rsquo;s journey into machine learning. In this and subsequent posts, we
will first briefly discuss relevant theory of PCA. Then we will
implement PCA from scratch without using any built-in function. This
will give us an idea as to what happens under the hood when a built-in
function is called in any software environment. Simultaneously, we will
also show how to use built-in commands to obtain results. Finally, we
will reproduce the results of a popular paper on PCA. Including all this
in a single post will make it very very long. Therefore, the post has
been divided into three parts. Readers totally familiar with PCA should
read none and leave this page immediately to save their precious time.
Other readers, who have a passing knowledge of PCA and want to see
different implementations, should pick and choose material from
different parts as per their need. Absolute beginners should start with
Part-I and work their way through gradually. Beginners are also
encouraged to explore the references at the end of this post for further
information. Here is the outline of different parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-I: Basic Theory of
PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II: PCA Implementation with and without using built-in
functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-III: Reproducing results of a published paper on
PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt;,
Python, R, and MATLAB code are available to reproduce all the results.
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-III&lt;/a&gt;
contains both R and Python code to reproduce results of the paper. In
this post, we will discuss the theory behind PCA in brief.&lt;/p&gt;
&lt;h1 id=&#34;principal-component-analysis&#34;&gt;Principal Component Analysis&lt;/h1&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory:&lt;/h2&gt;
&lt;p&gt;Given a data matrix, we apply PCA to transform it in a way such that the
transformed data reveals maximum information. So we have to first get
the data on which we want to perform PCA. The usual convention in
storing data is to place variables as columns and different observations
as rows (Data frames in R follow this convention by default). For
example, let&amp;rsquo;s suppose we are collecting data about daily weather for a
year. Our variables of interest may include maximum temperature in a
day, minimum temperature, humidity, max. wind speed, etc. Everyday we
collect observations for each of these variables. In vector form, our
data point for one day will contain number of observations equal to the
number of variables under study and this becomes one row of our data
matrix. Assuming that we are observing 10 variables everyday, our data
matrix for one year (assuming it&amp;rsquo;s not a leap year) will contain 365
rows and 10 columns. Once data matrix is obtained, further analysis is
done on this data matrix to obtain important hidden information
regarding the data. We will use notations from matrix theory to simplify
our analysis.&lt;/p&gt;
&lt;p&gt;Let $\textbf{X}$ be the data matrix of size $n\times p$, where $n$ is
the number of data points and $p$ is the number of variables. We can
assume without any loss of generality that data is centered, meaning its
column means are zero. This only shifts the data towards the origin
without changing their relative orientation. So if originally not
centered, it is first centered before doing PCA. From now onward we will
assume that data matrix is always centered.&lt;/p&gt;
&lt;p&gt;Variance of a variable (a column) in $\textbf{X}$ is equal to sum of
squares of entries (because the column is centered) of that column
divided by (n - 1) (to make it unbiased). So sum of variance of all
variables is $\frac{1}{n - 1}$ times sum of squares of all elements of
the matrix . Readers who are familiar with matrix norms would instantly
recognize that total variance is $\frac{1}{n - 1}$ times the square of
&lt;strong&gt;Frobenius norm&lt;/strong&gt; of $\textbf{X}$. Frobenius norm is nothing but square
root of sum of squares of all elements of a matrix.

$$ \|\textbf{X}\|_{F} = (\sum_{i,j}{x_{ij}^2})^{\frac{1}{2}}=\sqrt{trace(\textbf{X}^T\textbf{X}})=\sqrt{trace(\textbf{X}\textbf{X}^T})$$

&lt;/p&gt;
&lt;p&gt;Using this definition, total variance before transformation =

$$\begin{aligned}\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=\frac{1}{n-1}trace(\textbf{X}^T\textbf{X})
&amp;= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) \\
&amp;= \frac{1}{n-1}\|\textbf{X}\|_{F}^2\end{aligned}$$

&lt;/p&gt;
&lt;p&gt;Where, trace of a matrix is the sum of its diagonal entries and
 $\|\textbf{X}\|_{F}^2$ 
 is the square of &lt;strong&gt;Frobenius norm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The aim of PCA is to transform the data in such a way that along first
principal direction, variance of transformed data is maximum. It
subsequently finds second principal direction orthogonal to the first
one in such a way that it explains maximum of the remaining variance
among all possible direction in the orthogonal subspace.&lt;/p&gt;
&lt;p&gt;In matrix form the transformation can be written as

$$\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}$$


Where $\textbf{Y}$ is the transformed data matrix. The columns of
$\textbf{Y}$ are called principal components and $\textbf{P}$ is usually
called loading matrix. Our aim is to find matrix $\textbf{P}$. Once we
find $\textbf{P}$ we can then find $\textbf{Y}$ just by a matrix
multiplication. We will show in the next section that matrix
$\textbf{P}$ is the eigenvector matrix of the covariance matrix. Before
that, let&amp;rsquo;s first define the covariance matrix.&lt;/p&gt;
&lt;p&gt;Given a data matrix $\textbf{X}$(centered), its covariance matrix
$(\textbf{S})$ is defined as
$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}$$
Now we will show how to compute the loading vectors (columns of $\textbf{P}$) and consequently the principal components (columns of $\textbf{Y}$) from the given centered data matrix $\textbf{X}$.&lt;/p&gt;
&lt;h3 id=&#34;sketch-of-the-proof&#34;&gt;Sketch of the Proof&lt;/h3&gt;
&lt;p&gt;We call it a sketch because we will not be giving the full proof.
Rather, we will give the proof only for the first principal component
and then give a commentary as to how it can be extended for other
principal components.&lt;/p&gt;
&lt;p&gt;The first principal component is the result obtained by transforming
original data matrix $\textbf{X}_{n\times p}$
 in such a way that
variance of data along first principal component is the highest. The
transformation is a linear transformation that is obtained by taking
linear combination of the columns of $\textbf{X}_{n\times p}$
. The
coefficients of the linear combination are called loading scores
corresponding to original variables of $\textbf{X}_{n\times p}$
.&lt;/p&gt;
&lt;p&gt;Assuming $\boldsymbol{\alpha}_{p\times 1} = [\alpha_1, \alpha_2, \ldots, \alpha_p]^T$
,
where $\alpha_1$, $\alpha_2$, $\ldots$ , $\alpha_p$ are scalars, to be the
loading vector (we don&amp;rsquo;t know, as of now, from where to get
$\alpha_{p \times 1}$. We will find that out shortly.), first principal
component is obtained by the the product
$\textbf{X}_{n\times p} \boldsymbol{\alpha}_{p\times 1}$
. This product can be written
as

$$
\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1} = \alpha_1 \textbf{X}_{[:,1]} +\alpha_2 \textbf{X}_{[:,2]} + ...  + \alpha_p \textbf{X}_{[:,p]} 
$$

&lt;/p&gt;
&lt;p&gt;Where, $\textbf{X}_{[:,1]}$
 is the first column of
$\textbf{X}_{n\times p}$
. Similarly for other columns. The above
equation makes it clear as to why first principal component is a linear
combination of variables of original data matrix. In the original data
matrix, each column corresponds to a variable.&lt;/p&gt;
&lt;p&gt;Variance of first principal component is given by
$ \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$
 (As the columns are already
centered. We have also ignored the factor $(\frac{1}{n-1})$ as it is
just a scaling factor.). Now our goal is to find an  $\boldsymbol{\alpha}_{p\times 1}$ 

that maximizes $\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$
. As
$\boldsymbol{\alpha}_{p\times 1}$
 is arbitrary, we can choose its entries in such a
way that variance increases as much as we please. So to get any
meaningful solution, we have to apply some constraints on
$\boldsymbol{\alpha}_{p\times 1}$
. The conventional condition is
$\|\boldsymbol{\alpha}_{p\times 1}\|^2 = 1$
. The optimization problem becomes
$$ maximize \ \ \   \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$$

$$s.t. \|\boldsymbol{\alpha}\|^2 = 1$$
 Using Lagrange multipliers, this problem can
be written as

$$maximize \ \ \  \mathcal{L}(\boldsymbol{\alpha}, \lambda)=\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha} + \lambda (1 - \boldsymbol{\alpha}^T\boldsymbol{\alpha})$$


Taking gradient of $\mathcal{L}(\boldsymbol{\alpha}, \lambda)$
 with respect to
$\boldsymbol{\alpha}$
 we get, $\textbf{X}^T\textbf{X}\boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}$
. So
$\boldsymbol{\alpha}$
 is the eigenvector of $\textbf{X}^T\textbf{X}$
. It turns out
that for first principal component, $\boldsymbol{\alpha}$
 is the eigenvector
corresponding to the largest eigenvalue.&lt;/p&gt;
&lt;p&gt;Loading vector for second principal component is computed with the added condition that second loading vector is orthogonal to the first one. With little bit of more work it can be shown that loading vectors for successive principal components are obtained from eigenvectors corresponding to eigenvalues in decreasing order. More details can be found in reference [1].&lt;/p&gt;
&lt;p&gt;Now, it is straightforward to first form the covariance matrix and by
placing its eigenvectors as columns, we can find matrix $\textbf{P}$ and
consequently the principal components. The eigenvectors are arranged in
such a way that first column is the eigenvector corresponding to largest
eigenvector, second column (second eigenvector) corresponds to second
largest eigenvalue and so on. Here we have assumed that we will always
be able to find all the $p$ orthogonal eigenvectors. In fact, we will
always be able to find $p$ orthogonal eigenvectors as the matrix is
symmetric. It can also be shown that the transformed matrix $\textbf{Y}$
is centered and more remarkably, total variance of columns of
$\textbf{Y}$ is same as total variance of columns of $\textbf{X}$. We
will prove these two propositions as the proofs are short.&lt;/p&gt;
&lt;h3 id=&#34;properties-of-pca-transformation&#34;&gt;Properties of PCA Transformation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Principal components are centered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let $\textbf{1}$ be a column vector of all ones of size
$(n\times 1)$. To prove that columns of $\textbf{Y}$ are centered,
just premultiply it by $\textbf{1}^T$ (this finds column sum for
each column). So
$$\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}$$ But
columns of $\textbf{X}$ are already centered, so
$\textbf{1}^T\textbf{X}=\textbf{0}$. Thus
$\textbf{1}^T \textbf{Y}= \textbf{0}$. Hence columns of $\textbf{Y}$
are centered.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sum of variance of principal components is equal to sum of variance
of variables before transformation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: To prove that total variance of $\textbf{Y}$ also remains
same, observe that

    $$\begin{aligned} \mbox{total covariance of} \ \  \textbf{Y} &amp;=
    trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y}) \\ &amp;=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P}) \\ &amp;=\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X}) \\ 
    &amp;= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\end{aligned}$$
    

The previous equation uses the fact that trace is
commutative(i.e.$trace(\textbf{AB})=trace(\textbf{BA})$) and
$\textbf{P}$ is orthogonal (i.e.
$\textbf{P}\textbf{P}^T=\textbf{I}$).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Principal components are orthogonal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: To prove the above claim, it is sufficient to show that
the matrix $\textbf{Y}^T\textbf{Y}$ is diagonal. Remember that columns of $\textbf{Y}$ are principal components. So if we can somehow show $\textbf{Y}^T\textbf{Y}$ to be diagonal, it would automatically mean that principal components are orthogonal.
We know, $\textbf{Y} = \textbf{X}\textbf{P}$. So $\textbf{Y}^T\textbf{Y} = \textbf{P}^T\textbf{X}^T\textbf{X}\textbf{P}$. From sketch of the proof, we know that $\textbf{P}$ is orthogonal as we have required successive loading vectors to be orthogonal to previous ones. We also know that $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$. So from &lt;a href=&#34;https://mathworld.wolfram.com/EigenDecompositionTheorem.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eigen Decomposition Theorem&lt;/a&gt;, it follows that $\textbf{P}^T(\textbf{X}^T\textbf{X})\textbf{P}$ is diagonal as $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$ and $\textbf{P}$ is orthogonal (so $\textbf{P}^{-1} = \textbf{P}^T$).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;link-between-total-variance-and-eigenvalues&#34;&gt;Link between total variance and eigenvalues&lt;/h3&gt;
&lt;p&gt;Total variance is sum of eigenvalues of covariance matrix
$(\textbf{S})$. This follows from the fact that &lt;span style=&#34;color: hotpink&#34;&gt;&lt;em&gt;trace of a matrix is sum of its eigenvalues&lt;/em&gt;&lt;/span&gt;. Total variance of original data matrix is $\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) =trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) = trace(\textbf{S})$. We will show these calculations using a publicly available dataset in
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;variations-of-pca&#34;&gt;Variations of PCA&lt;/h3&gt;
&lt;p&gt;Sometimes our data matrix contains variables that are measured in
different units. So we might have to scale the centered matrix to reduce
the effect of variables with large variation. So depending on the matrix
on which PCA is performed, it is divided into two types.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Covariance PCA (Data matrix is centered but &lt;strong&gt;not&lt;/strong&gt; scaled)&lt;/li&gt;
&lt;li&gt;Correlation PCA (Data matrix is centered and scaled)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of these two types can be found in
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part-II&lt;/a&gt;.
Please note that the above two variations are just two among many
variations. There are &lt;strong&gt;Sparse PCA&lt;/strong&gt;, &lt;strong&gt;Kernel PCA&lt;/strong&gt;, &lt;strong&gt;Robust PCA&lt;/strong&gt;,
&lt;strong&gt;Non-negative PCA&lt;/strong&gt; and many others. We have mentioned the two that are
most widely used.&lt;/p&gt;
&lt;h3 id=&#34;some-common-terminologies-associated-with-pca&#34;&gt;Some common terminologies associated with PCA&lt;/h3&gt;
&lt;p&gt;In literature, there is no standard terminology for different terms in
PCA. Different people use different (often contradictory) terminology
thus confusing newcomers. Therefore, it is better to stick to one set of
terminologies and notations and use those consistently. We will stick to
the terminology used in reference [2].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Factor scores&lt;/strong&gt; corresponding to a principal component: Values of
that column of $\textbf{Y}$ that corresponds to the desired
principal component.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Loading score&lt;/strong&gt;: Values corresponding to a column of $\textbf{P}$.
For example,loading scores of variables corresponding to first
principal component are the values of the first column of
$\textbf{P}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inertia&lt;/strong&gt;: Square of Frobenius norm of the matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-actually-are-principal-components-computed&#34;&gt;How actually are principal components computed?&lt;/h3&gt;
&lt;p&gt;The previously stated method of finding eigenvectors of covariance
matrix is not computationally efficient. In practice, singular value
decomposition (SVD) is used to compute the matrix $\textbf{P}$. SVD
theorem tells that any real matrix $\textbf{X}$ can be decomposed into
three matrices such that $$ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T$$
Where, $\textbf{X}$ is of size $n\times p$. $\textbf{U}$ and
$\textbf{V}$ are orthogonal matrices of size $n\times n$ and $p\times p$
respectively. $\Sigma$ is a diagonal matrix of size $n\times p$.&lt;/p&gt;
&lt;p&gt;Given the SVD decomposition of a matrix $\textbf{X}$,
$$\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T$$&lt;/p&gt;
&lt;p&gt;This is the eigen-decomposition of $\textbf{X}^T\textbf{X}$. So
$\textbf{V}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$. For
PCA we need eigenvector matrix of covariance matrix. So converting the
equation into convenient form, we get
$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T$$
Thus eigenvalues of S are diagonal entries of $(\frac{1}{n-1}\Sigma^2)$.
As SVD is computationally efficient, all built-in functions use SVD to
compute the loading matrix and then use the loading matrix to find
principal components.&lt;/p&gt;
&lt;p&gt;In the interest of keeping the post at a reasonable length, we will stop
our exposition of theory here. Whatever we have discussed is only a
fraction of everything. Entire books have been written on PCA.
Interested readers who want to pursue this further can refer the
references of this post as a starting point. Readers are encouraged to
bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;Last modified: May 5, 2021&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New
York,2002.&lt;/li&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis.
Wiley interdisciplinary reviews: computational statistics, 2(4),
433-459.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
