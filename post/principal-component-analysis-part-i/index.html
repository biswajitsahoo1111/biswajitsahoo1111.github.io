<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: March 17, 2024 --><html lang="en-us" >


<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Biswajit Sahoo" />





  

<meta name="description" content="In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. Applications of PCA and its variants are ubiquitous." />



<link rel="alternate" hreflang="en-us" href="http://localhost:1313/post/principal-component-analysis-part-i/" />
<link rel="canonical" href="http://localhost:1313/post/principal-component-analysis-part-i/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hubfdc04a733bd2d8af21820718fcf71ca_131045_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hubfdc04a733bd2d8af21820718fcf71ca_131045_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="http://localhost:1313/media/icon_hubfdc04a733bd2d8af21820718fcf71ca_131045_512x512_fill_lanczos_center_3.png" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="Biswajit Sahoo" />
<meta property="og:url" content="http://localhost:1313/post/principal-component-analysis-part-i/" />
<meta property="og:title" content="Principal Component Analysis - Part I | Biswajit Sahoo" />
<meta property="og:description" content="In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. Applications of PCA and its variants are ubiquitous." /><meta property="og:image" content="http://localhost:1313/media/icon_hubfdc04a733bd2d8af21820718fcf71ca_131045_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2019-02-03T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2019-02-03T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/post/principal-component-analysis-part-i/"
  },
  "headline": "Principal Component Analysis - Part I",
  
  "datePublished": "2019-02-03T00:00:00Z",
  "dateModified": "2019-02-03T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Biswajit Sahoo"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Biswajit Sahoo",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/media/icon_hubfdc04a733bd2d8af21820718fcf71ca_131045_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. Applications of PCA and its variants are ubiquitous."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Principal Component Analysis - Part I | Biswajit Sahoo</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="de04929f8a175d3344ff47f5edee699f" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Biswajit Sahoo</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Biswajit Sahoo</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#blog"><span>Blog</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/personal"><span>Personal</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>Principal Component Analysis - Part I</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 3, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/blog/">Blog</a>, <a href="/category/pca/">PCA</a>, <a href="/category/machine-learning/">Machine Learning</a>, <a href="/category/r/">R</a>, <a href="/category/python/">Python</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In this post, we will discuss about Principal Component Analysis (PCA),
one of the most popular dimensionality reduction techniques used in
machine learning. Applications of PCA and its variants are ubiquitous.
Thus, a through understanding of PCA is considered essential to start
one&rsquo;s journey into machine learning. In this and subsequent posts, we
will first briefly discuss relevant theory of PCA. Then we will
implement PCA from scratch without using any built-in function. This
will give us an idea as to what happens under the hood when a built-in
function is called in any software environment. Simultaneously, we will
also show how to use built-in commands to obtain results. Finally, we
will reproduce the results of a popular paper on PCA. Including all this
in a single post will make it very very long. Therefore, the post has
been divided into three parts. Readers totally familiar with PCA should
read none and leave this page immediately to save their precious time.
Other readers, who have a passing knowledge of PCA and want to see
different implementations, should pick and choose material from
different parts as per their need. Absolute beginners should start with
Part-I and work their way through gradually. Beginners are also
encouraged to explore the references at the end of this post for further
information. Here is the outline of different parts:</p>
<ul>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/" target="_blank" rel="noopener">Part-I: Basic Theory of
PCA</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II: PCA Implementation with and without using built-in
functions</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener">Part-III: Reproducing results of a published paper on
PCA</a></li>
</ul>
<p>For
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>,
Python, R, and MATLAB code are available to reproduce all the results.
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener">Part-III</a>
contains both R and Python code to reproduce results of the paper. In
this post, we will discuss the theory behind PCA in brief.</p>
<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<h2 id="theory">Theory:</h2>
<p>Given a data matrix, we apply PCA to transform it in a way such that the
transformed data reveals maximum information. So we have to first get
the data on which we want to perform PCA. The usual convention in
storing data is to place variables as columns and different observations
as rows (Data frames in R follow this convention by default). For
example, let&rsquo;s suppose we are collecting data about daily weather for a
year. Our variables of interest may include maximum temperature in a
day, minimum temperature, humidity, max. wind speed, etc. Everyday we
collect observations for each of these variables. In vector form, our
data point for one day will contain number of observations equal to the
number of variables under study and this becomes one row of our data
matrix. Assuming that we are observing 10 variables everyday, our data
matrix for one year (assuming it&rsquo;s not a leap year) will contain 365
rows and 10 columns. Once data matrix is obtained, further analysis is
done on this data matrix to obtain important hidden information
regarding the data. We will use notations from matrix theory to simplify
our analysis.</p>
<p>Let $\textbf{X}$ be the data matrix of size $n\times p$, where $n$ is
the number of data points and $p$ is the number of variables. We can
assume without any loss of generality that data is centered, meaning its
column means are zero. This only shifts the data towards the origin
without changing their relative orientation. So if originally not
centered, it is first centered before doing PCA. From now onward we will
assume that data matrix is always centered.</p>
<p>Variance of a variable (a column) in $\textbf{X}$ is equal to sum of
squares of entries (because the column is centered) of that column
divided by (n - 1) (to make it unbiased). So sum of variance of all
variables is $\frac{1}{n - 1}$ times sum of squares of all elements of
the matrix . Readers who are familiar with matrix norms would instantly
recognize that total variance is $\frac{1}{n - 1}$ times the square of
<strong>Frobenius norm</strong> of $\textbf{X}$. Frobenius norm is nothing but square
root of sum of squares of all elements of a matrix.



$$ \|\textbf{X}\|_{F} = (\sum_{i,j}{x_{ij}^2})^{\frac{1}{2}}=\sqrt{trace(\textbf{X}^T\textbf{X}})=\sqrt{trace(\textbf{X}\textbf{X}^T})$$
</p>
<p>Using this definition, total variance before transformation =



$$\begin{aligned}\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=\frac{1}{n-1}trace(\textbf{X}^T\textbf{X})
&= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) \\
&= \frac{1}{n-1}\|\textbf{X}\|_{F}^2\end{aligned}$$
</p>
<p>Where, trace of a matrix is the sum of its diagonal entries and


 $\|\textbf{X}\|_{F}^2$  is the square of <strong>Frobenius norm</strong>.</p>
<p>The aim of PCA is to transform the data in such a way that along first
principal direction, variance of transformed data is maximum. It
subsequently finds second principal direction orthogonal to the first
one in such a way that it explains maximum of the remaining variance
among all possible direction in the orthogonal subspace.</p>
<p>In matrix form the transformation can be written as



$$\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}$$

Where $\textbf{Y}$ is the transformed data matrix. The columns of
$\textbf{Y}$ are called principal components and $\textbf{P}$ is usually
called loading matrix. Our aim is to find matrix $\textbf{P}$. Once we
find $\textbf{P}$ we can then find $\textbf{Y}$ just by a matrix
multiplication. We will show in the next section that matrix
$\textbf{P}$ is the eigenvector matrix of the covariance matrix. Before
that, let&rsquo;s first define the covariance matrix.</p>
<p>Given a data matrix $\textbf{X}$(centered), its covariance matrix
$(\textbf{S})$ is defined as
$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}$$
Now we will show how to compute the loading vectors (columns of $\textbf{P}$) and consequently the principal components (columns of $\textbf{Y}$) from the given centered data matrix $\textbf{X}$.</p>
<h3 id="sketch-of-the-proof">Sketch of the Proof</h3>
<p>We call it a sketch because we will not be giving the full proof.
Rather, we will give the proof only for the first principal component
and then give a commentary as to how it can be extended for other
principal components.</p>
<p>The first principal component is the result obtained by transforming
original data matrix 

$\textbf{X}_{n\times p}$ in such a way that
variance of data along first principal component is the highest. The
transformation is a linear transformation that is obtained by taking
linear combination of the columns of 

$\textbf{X}_{n\times p}$. The
coefficients of the linear combination are called loading scores
corresponding to original variables of 

$\textbf{X}_{n\times p}$.</p>
<p>Assuming 

$\boldsymbol{\alpha}_{p\times 1} = [\alpha_1, \alpha_2, \ldots, \alpha_p]^T$,
where $\alpha_1$, $\alpha_2$, $\ldots$ , $\alpha_p$ are scalars, to be the
loading vector (we don&rsquo;t know, as of now, from where to get
$\alpha_{p \times 1}$. We will find that out shortly.), first principal
component is obtained by the the product


$\textbf{X}_{n\times p} \boldsymbol{\alpha}_{p\times 1}$. This product can be written
as



$$
\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1} = \alpha_1 \textbf{X}_{[:,1]} +\alpha_2 \textbf{X}_{[:,2]} + ...  + \alpha_p \textbf{X}_{[:,p]} 
$$
</p>
<p>Where, 

$\textbf{X}_{[:,1]}$ is the first column of


$\textbf{X}_{n\times p}$. Similarly for other columns. The above
equation makes it clear as to why first principal component is a linear
combination of variables of original data matrix. In the original data
matrix, each column corresponds to a variable.</p>
<p>Variance of first principal component is given by


$ \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$ (As the columns are already
centered. We have also ignored the factor $(\frac{1}{n-1})$ as it is
just a scaling factor.). Now our goal is to find an 

 $\boldsymbol{\alpha}_{p\times 1}$ 
that maximizes 

$\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$. As


$\boldsymbol{\alpha}_{p\times 1}$ is arbitrary, we can choose its entries in such a
way that variance increases as much as we please. So to get any
meaningful solution, we have to apply some constraints on


$\boldsymbol{\alpha}_{p\times 1}$. The conventional condition is


$\|\boldsymbol{\alpha}_{p\times 1}\|^2 = 1$. The optimization problem becomes


$$ maximize \ \ \   \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$$


$$s.t. \|\boldsymbol{\alpha}\|^2 = 1$$ Using Lagrange multipliers, this problem can
be written as



$$maximize \ \ \  \mathcal{L}(\boldsymbol{\alpha}, \lambda)=\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha} + \lambda (1 - \boldsymbol{\alpha}^T\boldsymbol{\alpha})$$

Taking gradient of 

$\mathcal{L}(\boldsymbol{\alpha}, \lambda)$ with respect to


$\boldsymbol{\alpha}$ we get, 

$\textbf{X}^T\textbf{X}\boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}$. So


$\boldsymbol{\alpha}$ is the eigenvector of 

$\textbf{X}^T\textbf{X}$. It turns out
that for first principal component, 

$\boldsymbol{\alpha}$ is the eigenvector
corresponding to the largest eigenvalue.</p>
<p>Loading vector for second principal component is computed with the added condition that second loading vector is orthogonal to the first one. With little bit of more work it can be shown that loading vectors for successive principal components are obtained from eigenvectors corresponding to eigenvalues in decreasing order. More details can be found in reference [1].</p>
<p>Now, it is straightforward to first form the covariance matrix and by
placing its eigenvectors as columns, we can find matrix $\textbf{P}$ and
consequently the principal components. The eigenvectors are arranged in
such a way that first column is the eigenvector corresponding to largest
eigenvector, second column (second eigenvector) corresponds to second
largest eigenvalue and so on. Here we have assumed that we will always
be able to find all the $p$ orthogonal eigenvectors. In fact, we will
always be able to find $p$ orthogonal eigenvectors as the matrix is
symmetric. It can also be shown that the transformed matrix $\textbf{Y}$
is centered and more remarkably, total variance of columns of
$\textbf{Y}$ is same as total variance of columns of $\textbf{X}$. We
will prove these two propositions as the proofs are short.</p>
<h3 id="properties-of-pca-transformation">Properties of PCA Transformation</h3>
<ol>
<li>
<p>Principal components are centered.</p>
<p><strong>Proof</strong>: Let $\textbf{1}$ be a column vector of all ones of size
$(n\times 1)$. To prove that columns of $\textbf{Y}$ are centered,
just premultiply it by $\textbf{1}^T$ (this finds column sum for
each column). So
$$\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}$$ But
columns of $\textbf{X}$ are already centered, so
$\textbf{1}^T\textbf{X}=\textbf{0}$. Thus
$\textbf{1}^T \textbf{Y}= \textbf{0}$. Hence columns of $\textbf{Y}$
are centered.</p>
</li>
<li>
<p>Sum of variance of principal components is equal to sum of variance
of variables before transformation.</p>
<p><strong>Proof</strong>: To prove that total variance of $\textbf{Y}$ also remains
same, observe that



    $$\begin{aligned} \mbox{total covariance of} \ \  \textbf{Y} &=
    trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y}) \\ &=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P}) \\ &=\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X}) \\ 
    &= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\end{aligned}$$
    
The previous equation uses the fact that trace is
commutative(i.e.$trace(\textbf{AB})=trace(\textbf{BA})$) and
$\textbf{P}$ is orthogonal (i.e.
$\textbf{P}\textbf{P}^T=\textbf{I}$).</p>
</li>
<li>
<p>Principal components are orthogonal.</p>
<p><strong>Proof</strong>: To prove the above claim, it is sufficient to show that
the matrix $\textbf{Y}^T\textbf{Y}$ is diagonal. Remember that columns of $\textbf{Y}$ are principal components. So if we can somehow show $\textbf{Y}^T\textbf{Y}$ to be diagonal, it would automatically mean that principal components are orthogonal.
We know, $\textbf{Y} = \textbf{X}\textbf{P}$. So $\textbf{Y}^T\textbf{Y} = \textbf{P}^T\textbf{X}^T\textbf{X}\textbf{P}$. From sketch of the proof, we know that $\textbf{P}$ is orthogonal as we have required successive loading vectors to be orthogonal to previous ones. We also know that $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$. So from <a href="https://mathworld.wolfram.com/EigenDecompositionTheorem.html" target="_blank" rel="noopener">Eigen Decomposition Theorem</a>, it follows that $\textbf{P}^T(\textbf{X}^T\textbf{X})\textbf{P}$ is diagonal as $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$ and $\textbf{P}$ is orthogonal (so $\textbf{P}^{-1} = \textbf{P}^T$).</p>
</li>
</ol>
<h3 id="link-between-total-variance-and-eigenvalues">Link between total variance and eigenvalues</h3>
<p>Total variance is sum of eigenvalues of covariance matrix
$(\textbf{S})$. This follows from the fact that <span style="color: hotpink"><em>trace of a matrix is sum of its eigenvalues</em></span>. Total variance of original data matrix is $\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) =trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) = trace(\textbf{S})$. We will show these calculations using a publicly available dataset in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>.</p>
<h3 id="variations-of-pca">Variations of PCA</h3>
<p>Sometimes our data matrix contains variables that are measured in
different units. So we might have to scale the centered matrix to reduce
the effect of variables with large variation. So depending on the matrix
on which PCA is performed, it is divided into two types.</p>
<ul>
<li>Covariance PCA (Data matrix is centered but <strong>not</strong> scaled)</li>
<li>Correlation PCA (Data matrix is centered and scaled)</li>
</ul>
<p>Examples of these two types can be found in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>.
Please note that the above two variations are just two among many
variations. There are <strong>Sparse PCA</strong>, <strong>Kernel PCA</strong>, <strong>Robust PCA</strong>,
<strong>Non-negative PCA</strong> and many others. We have mentioned the two that are
most widely used.</p>
<h3 id="some-common-terminologies-associated-with-pca">Some common terminologies associated with PCA</h3>
<p>In literature, there is no standard terminology for different terms in
PCA. Different people use different (often contradictory) terminology
thus confusing newcomers. Therefore, it is better to stick to one set of
terminologies and notations and use those consistently. We will stick to
the terminology used in reference [2].</p>
<ul>
<li>
<p><strong>Factor scores</strong> corresponding to a principal component: Values of
that column of $\textbf{Y}$ that corresponds to the desired
principal component.</p>
</li>
<li>
<p><strong>Loading score</strong>: Values corresponding to a column of $\textbf{P}$.
For example,loading scores of variables corresponding to first
principal component are the values of the first column of
$\textbf{P}$.</p>
</li>
<li>
<p><strong>Inertia</strong>: Square of Frobenius norm of the matrix.</p>
</li>
</ul>
<h3 id="how-actually-are-principal-components-computed">How actually are principal components computed?</h3>
<p>The previously stated method of finding eigenvectors of covariance
matrix is not computationally efficient. In practice, singular value
decomposition (SVD) is used to compute the matrix $\textbf{P}$. SVD
theorem tells that any real matrix $\textbf{X}$ can be decomposed into
three matrices such that $$ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T$$
Where, $\textbf{X}$ is of size $n\times p$. $\textbf{U}$ and
$\textbf{V}$ are orthogonal matrices of size $n\times n$ and $p\times p$
respectively. $\Sigma$ is a diagonal matrix of size $n\times p$.</p>
<p>Given the SVD decomposition of a matrix $\textbf{X}$,
$$\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T$$</p>
<p>This is the eigen-decomposition of $\textbf{X}^T\textbf{X}$. So
$\textbf{V}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$. For
PCA we need eigenvector matrix of covariance matrix. So converting the
equation into convenient form, we get
$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T$$
Thus eigenvalues of S are diagonal entries of $(\frac{1}{n-1}\Sigma^2)$.
As SVD is computationally efficient, all built-in functions use SVD to
compute the loading matrix and then use the loading matrix to find
principal components.</p>
<p>In the interest of keeping the post at a reasonable length, we will stop
our exposition of theory here. Whatever we have discussed is only a
fraction of everything. Entire books have been written on PCA.
Interested readers who want to pursue this further can refer the
references of this post as a starting point. Readers are encouraged to
bring any errors or omissions to my notice.</p>
<p>Last modified: May 5, 2021</p>
<h2 id="references">References</h2>
<ol>
<li>I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New
York,2002.</li>
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis.
Wiley interdisciplinary reviews: computational statistics, 2(4),
433-459.</li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/pca/">PCA</a>
  
  <a class="badge badge-light" href="/tag/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tag/r/">R</a>
  
  <a class="badge badge-light" href="/tag/python/">Python</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;text=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;t=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Principal%20Component%20Analysis%20-%20Part%20I&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;title=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I%20http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;title=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="http://localhost:1313/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu3e225c89d6328e00dc57563a3fd76514_183341_270x270_fill_q75_lanczos_center.jpg" alt="Biswajit Sahoo"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="http://localhost:1313/">Biswajit Sahoo</a></h5>
      <h6 class="card-subtitle">Machine Learning Engineer</h6>
      <p class="card-text">My research interests include machine learning, deep learning, signal processing and data-driven machinery condition monitoring.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/biswajitsahoo1111" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/biswajitsahoo1111/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.in/citations?hl=en&amp;user=zu2CSBMAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>















  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/principal-component-analysis-part-iii/">Principal Component Analysis - Part III</a></li>
      
      <li><a href="/post/principal-component-analysis-part-ii/">Principal Component Analysis - Part II</a></li>
      
      <li><a href="/post/making-github-traffic-type-plots/">Making Github Traffic Type Plots</a></li>
      
      <li><a href="/post/reading-multiple-files-in-tensorflow-2-using-sequence/">Reading multiple files in Tensorflow 2 using Sequence</a></li>
      
      <li><a href="/post/reading-multiple-csv-files-in-pytorch/">Reading multiple csv files in PyTorch</a></li>
      
    </ul>
  </div>
  




    </div>
    <div class="col-12 col-lg-10ish article-style">
    <script src="https://utteranc.es/client.js"
        repo="biswajitsahoo1111/biswajitsahoo1111.github.io"
        issue-term="title"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
    </div>

  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

    











  
    
    
    
    
    













  
  <p class="powered-by copyright-license-text">
    © 2024 Biswajit Sahoo. Powered by <a href="https://hugoblox.com/" target="_blank" rel="noopener">Hugo Blox Builder</a>.
  </p>
  




  
    <p class="powered-by">
      
    </p>
  </footer>
    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.js" type="module"></script>


















</body>
</html>
