<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Biswajit Sahoo">

  
  
  
    
  
  <meta name="description" content="In this post, we will discuss about Principal Component Analysis (PCA),one of the most popular dimensionality reduction techniques used inmachine learning. Applications of PCA and its variants are ubiquitous.Thus, a through understanding of PCA is considered essential to startone’s journey into machine learning. In this and subsequent posts, wewill first briefly discuss relevant theory of PCA. Then we willimplement PCA from scratch without using any built-in function.">

  
  <link rel="alternate" hreflang="en-us" href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153208457-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-153208457-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huffc8dce834b4c0aec33122a3663a38ac_177939_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huffc8dce834b4c0aec33122a3663a38ac_177939_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@biswajitsahoo11">
  <meta property="twitter:creator" content="@biswajitsahoo11">
  
  <meta property="og:site_name" content="Biswajit Sahoo">
  <meta property="og:url" content="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">
  <meta property="og:title" content="Principal Component Analysis - Part I | Biswajit Sahoo">
  <meta property="og:description" content="In this post, we will discuss about Principal Component Analysis (PCA),one of the most popular dimensionality reduction techniques used inmachine learning. Applications of PCA and its variants are ubiquitous.Thus, a through understanding of PCA is considered essential to startone’s journey into machine learning. In this and subsequent posts, wewill first briefly discuss relevant theory of PCA. Then we willimplement PCA from scratch without using any built-in function."><meta property="og:image" content="https://biswajitsahoo1111.github.io/img/icon.png">
  <meta property="twitter:image" content="https://biswajitsahoo1111.github.io/img/icon.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-02-03T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-02-03T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/"
  },
  "headline": "Principal Component Analysis - Part I",
  
  "datePublished": "2019-02-03T00:00:00Z",
  "dateModified": "2019-02-03T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Biswajit Sahoo"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Biswajit Sahoo",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://biswajitsahoo1111.github.io/"
    }
  },
  "description": "In this post, we will discuss about Principal Component Analysis (PCA),\rone of the most popular dimensionality reduction techniques used in\rmachine learning. Applications of PCA and its variants are ubiquitous.\rThus, a through understanding of PCA is considered essential to start\rone’s journey into machine learning. In this and subsequent posts, we\rwill first briefly discuss relevant theory of PCA. Then we will\rimplement PCA from scratch without using any built-in function."
}
</script>

  

  


  


  





  <title>Principal Component Analysis - Part I | Biswajit Sahoo</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Biswajit Sahoo</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Biswajit Sahoo</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#blog"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Principal Component Analysis - Part I</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 3, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/blog/">Blog</a>, <a href="/categories/pca/">PCA</a>, <a href="/categories/machine-learning/">Machine Learning</a>, <a href="/categories/r/">R</a>, <a href="/categories/python/">Python</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this post, we will discuss about Principal Component Analysis (PCA),
one of the most popular dimensionality reduction techniques used in
machine learning. Applications of PCA and its variants are ubiquitous.
Thus, a through understanding of PCA is considered essential to start
one’s journey into machine learning. In this and subsequent posts, we
will first briefly discuss relevant theory of PCA. Then we will
implement PCA from scratch without using any built-in function. This
will give us an idea as to what happens under the hood when a built-in
function is called in any software environment. Simultaneously, we will
also show how to use built-in commands to obtain results. Finally, we
will reproduce the results of a popular paper on PCA. Including all this
in a single post will make it very very long. Therefore, the post has
been divided into three parts. Readers totally familiar with PCA should
read none and leave this page immediately to save their precious time.
Other readers, who have a passing knowledge of PCA and want to see
different implementations, should pick and choose material from
different parts as per their need. Absolute beginners should start with
Part-I and work their way through gradually. Beginners are also
encouraged to explore the references at the end of this post for further
information. Here is the outline of different parts:</p>
<ul>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">Part-I: Basic Theory of
PCA</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II: PCA Implementation with and without using built-in
functions</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/">Part-III: Reproducing results of a published paper on
PCA</a></li>
</ul>
<p>For
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>,
Python, R, and MATLAB code are available to reproduce all the results.
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/">Part-III</a>
contains both R and Python code to reproduce results of the paper. In
this post, we will discuss the theory behind PCA in brief.</p>
<div id="principal-component-analysis" class="section level1">
<h1>Principal Component Analysis</h1>
<div id="theory" class="section level2">
<h2>Theory:</h2>
<p>Given a data matrix, we apply PCA to transform it in a way such that the
transformed data reveals maximum information. So we have to first get
the data on which we want to perform PCA. The usual convention in
storing data is to place variables as columns and different observations
as rows (Data frames in R follow this convention by default). For
example, let’s suppose we are collecting data about daily weather for a
year. Our variables of interest may include maximum temperature in a
day, minimum temperature, humidity, max. wind speed, etc. Everyday we
collect observations for each of these variables. In vector form, our
data point for one day will contain number of observations equal to the
number of variables under study and this becomes one row of our data
matrix. Assuming that we are observing 10 variables everyday, our data
matrix for one year (assuming it’s not a leap year) will contain 365
rows and 10 columns. Once data matrix is obtained, further analysis is
done on this data matrix to obtain important hidden information
regarding the data. We will use notations from matrix theory to simplify
our analysis.</p>
<p>Let <span class="math inline">\(\textbf{X}\)</span> be the data matrix of size <span class="math inline">\(n\times p\)</span>, where <span class="math inline">\(n\)</span> is
the number of data points and <span class="math inline">\(p\)</span> is the number of variables. We can
assume without any loss of generality that data is centered, meaning its
column means are zero. This only shifts the data towards the origin
without changing their relative orientation. So if originally not
centered, it is first centered before doing PCA. From now onward we will
assume that data matrix is always centered.</p>
<p>Variance of a variable (a column) in <span class="math inline">\(\textbf{X}\)</span> is equal to sum of
squares of entries (because the column is centered) of that column
divided by (n - 1) (to make it unbiased). So sum of variance of all
variables is <span class="math inline">\(\frac{1}{n - 1}\)</span> times sum of squares of all elements of
the matrix . Readers who are familiar with matrix norms would instantly
recognize that total variance is <span class="math inline">\(\frac{1}{n - 1}\)</span> times the square of
<strong>Frobenius norm</strong> of <span class="math inline">\(\textbf{X}\)</span>. Frobenius norm is nothing but square
root of sum of squares of all elements of a matrix.
<span class="math display">\[ \|\textbf{X}\|_{F} = (\sum_{i,j}{x_{ij}^2})^{\frac{1}{2}}=\sqrt{trace(\textbf{X}^T\textbf{X}})=\sqrt{trace(\textbf{X}\textbf{X}^T})\]</span></p>
<p>Using this definition, total variance before transformation =
<span class="math display">\[\begin{aligned}\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) 
&amp;= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) \\ 
&amp;= \frac{1}{n-1}\|\textbf{X}\|_{F}^2\end{aligned}\]</span></p>
<p>Where, trace of a matrix is the sum of its diagonal entries and
<span class="math inline">\(\|\textbf{X}\|_{F}^2\)</span> is the square of <strong>Frobenius norm</strong>.</p>
<p>The aim of PCA is to transform the data in such a way that along first
principal direction, variance of transformed data is maximum. It
subsequently finds second principal direction orthogonal to the first
one in such a way that it explains maximum of the remaining variance
among all possible direction in the orthogonal subspace.</p>
<p>In matrix form the transformation can be written as
<span class="math display">\[\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}\]</span>
Where <span class="math inline">\(\textbf{Y}\)</span> is the transformed data matrix. The columns of
<span class="math inline">\(\textbf{Y}\)</span> are called principal components and <span class="math inline">\(\textbf{P}\)</span> is usually
called loading matrix. Our aim is to find matrix <span class="math inline">\(\textbf{P}\)</span>. Once we
find <span class="math inline">\(\textbf{P}\)</span> we can then find <span class="math inline">\(\textbf{Y}\)</span> just by a matrix
multiplication. We will show in the next section that matrix
<span class="math inline">\(\textbf{P}\)</span> is the eigenvector matrix of the covariance matrix. Before
that, let’s first define the covariance matrix.</p>
<p>Given a data matrix <span class="math inline">\(\textbf{X}\)</span>(centered), its covariance matrix
<span class="math inline">\((\textbf{S})\)</span> is defined as
<span class="math display">\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}\]</span>
Now we will show how to compute the loading vectors (columns of <span class="math inline">\(\textbf{P}\)</span>) and consequently the principal components (columns of <span class="math inline">\(\textbf{Y}\)</span>) from the given centered data matrix <span class="math inline">\(\textbf{X}\)</span>.</p>
<div id="sketch-of-the-proof" class="section level3">
<h3>Sketch of the Proof</h3>
<p>We call it a sketch because we will not be giving the full proof.
Rather, we will give the proof only for the first principal component
and then give a commentary as to how it can be extended for other
principal components.</p>
<p>The first principal component is the result obtained by transforming
original data matrix <span class="math inline">\(\textbf{X}_{n\times p}\)</span> in such a way that
variance of data along first principal component is the highest. The
transformation is a linear transformation that is obtained by taking
linear combination of the columns of <span class="math inline">\(\textbf{X}_{n\times p}\)</span>. The
coefficients of the linear combination are called loading scores
corresponding to original variables of <span class="math inline">\(\textbf{X}_{n\times p}\)</span>.</p>
<p>Assuming <span class="math inline">\(\boldsymbol{\alpha}_{p\times 1} = [\alpha_1, \alpha_2, ..., \alpha_p]^T\)</span>,
where <span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, … , <span class="math inline">\(\alpha_p\)</span> are scalars, to be the
loading vector (we don’t know, as of now, from where to get
<span class="math inline">\(\alpha_{p \times 1}\)</span>. We will find that out shortly.), first principal
component is obtained by the the product
<span class="math inline">\(\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1}\)</span>. This product can be written
as</p>
<p><span class="math display">\[
\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1} = \alpha_1 \textbf{X}_{[:,1]} +\alpha_2 \textbf{X}_{[:,2]} + ...  + \alpha_p \textbf{X}_{[:,p]} 
\]</span></p>
<p>Where, <span class="math inline">\(\textbf{X}_{[:,1]}\)</span> is the first column of
<span class="math inline">\(\textbf{X}_{n\times p}\)</span>. Similarly for other columns. The above
equation makes it clear as to why first principal component is a linear
combination of variables of original data matrix. In the original data
matrix, each column corresponds to a variable.</p>
<p>Variance of first principal component is given by
<span class="math inline">\(\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\)</span> (As the columns are already
centered. We have also ignored the factor <span class="math inline">\((\frac{1}{n-1})\)</span> as it is
just a scaling factor.). Now our goal is to find an <span class="math inline">\(\boldsymbol{\alpha}_{p\times 1}\)</span>
that maximizes <span class="math inline">\(\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\)</span>. As
<span class="math inline">\(\boldsymbol{\alpha}_{p\times 1}\)</span> is arbitrary, we can choose its entries in such a
way that variance increases as much as we please. So to get any
meaningful solution, we have to apply some constraints on
<span class="math inline">\(\boldsymbol{\alpha}_{p\times 1}\)</span>. The conventional condition is
<span class="math inline">\(\|\boldsymbol{\alpha}_{p\times 1}\|^2 = 1\)</span>. The optimization problem becomes
<span class="math display">\[ maximize \ \ \   \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\]</span>
<span class="math display">\[s.t. \|\boldsymbol{\alpha}\|^2 = 1\]</span> Using Lagrange multipliers, this problem can
be written as
<span class="math display">\[maximize \ \ \  \mathcal{L}(\boldsymbol{\alpha}, \lambda)=\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha} + \lambda (1 - \boldsymbol{\alpha}^T\boldsymbol{\alpha})\]</span>
Taking gradient of <span class="math inline">\(\mathcal{L}(\boldsymbol{\alpha}, \lambda)\)</span> with respect to
<span class="math inline">\(\boldsymbol{\alpha}\)</span> we get, <span class="math inline">\(\textbf{X}^T\textbf{X}\boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}\)</span>. So
<span class="math inline">\(\boldsymbol{\alpha}\)</span> is the eigenvector of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. It turns out
that for first principal component, <span class="math inline">\(\boldsymbol{\alpha}\)</span> is the eigenvector
corresponding to the largest eigenvalue.</p>
<p>Loading vector for second principal component is computed with the added condition that second loading vector is orthogonal to the first one. With little bit of more work it can be shown that loading vectors for successive principal components are obtained from eigenvectors corresponding to eigenvalues in decreasing order. More details can be found in reference [1].</p>
<p>Now, it is straightforward to first form the covariance matrix and by
placing its eigenvectors as columns, we can find matrix <span class="math inline">\(\textbf{P}\)</span> and
consequently the principal components. The eigenvectors are arranged in
such a way that first column is the eigenvector corresponding to largest
eigenvector, second column (second eigenvector) corresponds to second
largest eigenvalue and so on. Here we have assumed that we will always
be able to find all the <span class="math inline">\(p\)</span> orthogonal eigenvectors. In fact, we will
always be able to find <span class="math inline">\(p\)</span> orthogonal eigenvectors as the matrix is
symmetric. It can also be shown that the transformed matrix <span class="math inline">\(\textbf{Y}\)</span>
is centered and more remarkably, total variance of columns of
<span class="math inline">\(\textbf{Y}\)</span> is same as total variance of columns of <span class="math inline">\(\textbf{X}\)</span>. We
will prove these two propositions as the proofs are short.</p>
</div>
<div id="properties-of-pca-transformation" class="section level3">
<h3>Properties of PCA Transformation</h3>
<ol style="list-style-type: decimal">
<li><p>Principal components are centered.</p>
<p><strong>Proof</strong>: Let <span class="math inline">\(\textbf{1}\)</span> be a column vector of all ones of size
<span class="math inline">\((n\times 1)\)</span>. To prove that columns of <span class="math inline">\(\textbf{Y}\)</span> are centered,
just premultiply it by <span class="math inline">\(\textbf{1}^T\)</span> (this finds column sum for
each column). So
<span class="math display">\[\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}\]</span> But
columns of <span class="math inline">\(\textbf{X}\)</span> are already centered, so
<span class="math inline">\(\textbf{1}^T\textbf{X}=\textbf{0}\)</span>. Thus
<span class="math inline">\(\textbf{1}^T \textbf{Y}= \textbf{0}\)</span>. Hence columns of <span class="math inline">\(\textbf{Y}\)</span>
are centered.</p></li>
<li><p>Sum of variance of principal components is equal to sum of variance
of variables before transformation.</p>
<p><strong>Proof</strong>: To prove that total variance of <span class="math inline">\(\textbf{Y}\)</span> also remains
same, observe that</p>
<p><span class="math display">\[\begin{aligned} \mbox{total covariance of} \ \  \textbf{Y} &amp;=
trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y}) \\ &amp;=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P}) \\ &amp;=\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X}) \\ 
&amp;= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\end{aligned}\]</span>
The previous equation uses the fact that trace is
commutative(i.e.<span class="math inline">\(trace(\textbf{AB})=trace(\textbf{BA})\)</span>) and
<span class="math inline">\(\textbf{P}\)</span> is orthogonal (i.e.
<span class="math inline">\(\textbf{P}\textbf{P}^T=\textbf{I}\)</span>).</p></li>
<li><p>Principal components are orthogonal.</p>
<p><strong>Proof</strong>: To prove the above claim, it is sufficient to show that
the matrix <span class="math inline">\(\textbf{Y}^T\textbf{Y}\)</span> is diagonal. Remember that columns of <span class="math inline">\(\textbf{Y}\)</span> are principal components. So if we can somehow show <span class="math inline">\(\textbf{Y}^T\textbf{Y}\)</span> to be diagonal, it would automatically mean that principal components are orthogonal.
We know, <span class="math inline">\(\textbf{Y} = \textbf{X}\textbf{P}\)</span>. So <span class="math inline">\(\textbf{Y}^T\textbf{Y} = \textbf{P}^T\textbf{X}^T\textbf{X}\textbf{P}\)</span>. From sketch of the proof, we know that <span class="math inline">\(\textbf{P}\)</span> is orthogonal as we have required successive loading vectors to be orthogonal to previous ones. We also know that <span class="math inline">\(\textbf{P}\)</span> is the eigenvector matrix of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. So from <a href="https://mathworld.wolfram.com/EigenDecompositionTheorem.html">Eigen Decomposition Theorem</a>, it follows that <span class="math inline">\(\textbf{P}^T(\textbf{X}^T\textbf{X})\textbf{P}\)</span> is diagonal as <span class="math inline">\(\textbf{P}\)</span> is the eigenvector matrix of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span> and <span class="math inline">\(\textbf{P}\)</span> is orthogonal (so <span class="math inline">\(\textbf{P}^{-1} = \textbf{P}^T\)</span>).</p></li>
</ol>
</div>
<div id="link-between-total-variance-and-eigenvalues" class="section level3">
<h3>Link between total variance and eigenvalues</h3>
<p>Total variance is sum of eigenvalues of covariance matrix
<span class="math inline">\((\textbf{S})\)</span>. This follows from the fact that <span style="color: hotpink"><em>trace of a matrix is sum of its eigenvalues</em></span>. Total variance of original data matrix is <span class="math inline">\(\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) =trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) = trace(\textbf{S})\)</span>. We will show these calculations using a publicly available dataset in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>.</p>
</div>
<div id="variations-of-pca" class="section level3">
<h3>Variations of PCA</h3>
<p>Sometimes our data matrix contains variables that are measured in
different units. So we might have to scale the centered matrix to reduce
the effect of variables with large variation. So depending on the matrix
on which PCA is performed, it is divided into two types.</p>
<ul>
<li>Covariance PCA (Data matrix is centered but <strong>not</strong> scaled)</li>
<li>Correlation PCA (Data matrix is centered and scaled)</li>
</ul>
<p>Examples of these two types can be found in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>.
Please note that the above two variations are just two among many
variations. There are <strong>Sparse PCA</strong>, <strong>Kernel PCA</strong>, <strong>Robust PCA</strong>,
<strong>Non-negative PCA</strong> and many others. We have mentioned the two that are
most widely used.</p>
</div>
<div id="some-common-terminologies-associated-with-pca" class="section level3">
<h3>Some common terminologies associated with PCA</h3>
<p>In literature, there is no standard terminology for different terms in
PCA. Different people use different (often contradictory) terminology
thus confusing newcomers. Therefore, it is better to stick to one set of
terminologies and notations and use those consistently. We will stick to
the terminology used in reference [2].</p>
<ul>
<li><p><strong>Factor scores</strong> corresponding to a principal component: Values of
that column of <span class="math inline">\(\textbf{Y}\)</span> that corresponds to the desired
principal component.</p></li>
<li><p><strong>Loading score</strong>: Values corresponding to a column of <span class="math inline">\(\textbf{P}\)</span>.
For example,loading scores of variables corresponding to first
principal component are the values of the first column of
<span class="math inline">\(\textbf{P}\)</span>.</p></li>
<li><p><strong>Inertia</strong>: Square of Frobenius norm of the matrix.</p></li>
</ul>
</div>
<div id="how-actually-are-principal-components-computed" class="section level3">
<h3>How actually are principal components computed?</h3>
<p>The previously stated method of finding eigenvectors of covariance
matrix is not computationally efficient. In practice, singular value
decomposition (SVD) is used to compute the matrix <span class="math inline">\(\textbf{P}\)</span>. SVD
theorem tells that any real matrix <span class="math inline">\(\textbf{X}\)</span> can be decomposed into
three matrices such that <span class="math display">\[ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T\]</span>
Where, <span class="math inline">\(\textbf{X}\)</span> is of size <span class="math inline">\(n\times p\)</span>. <span class="math inline">\(\textbf{U}\)</span> and
<span class="math inline">\(\textbf{V}\)</span> are orthogonal matrices of size <span class="math inline">\(n\times n\)</span> and <span class="math inline">\(p\times p\)</span>
respectively. <span class="math inline">\(\Sigma\)</span> is a diagonal matrix of size <span class="math inline">\(n\times p\)</span>.</p>
<p>Given the SVD decomposition of a matrix <span class="math inline">\(\textbf{X}\)</span>,
<span class="math display">\[\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T\]</span></p>
<p>This is the eigen-decomposition of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. So
<span class="math inline">\(\textbf{V}\)</span> is the eigenvector matrix of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. For
PCA we need eigenvector matrix of covariance matrix. So converting the
equation into convenient form, we get
<span class="math display">\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T\]</span>
Thus eigenvalues of S are diagonal entries of <span class="math inline">\((\frac{1}{n-1}\Sigma^2)\)</span>.
As SVD is computationally efficient, all built-in functions use SVD to
compute the loading matrix and then use the loading matrix to find
principal components.</p>
<p>In the interest of keeping the post at a reasonable length, we will stop
our exposition of theory here. Whatever we have discussed is only a
fraction of everything. Entire books have been written on PCA.
Interested readers who want to pursue this further can refer the
references of this post as a starting point. Readers are encouraged to
bring any errors or omissions to my notice.</p>
<p>Last modified: May 5, 2021</p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ol style="list-style-type: decimal">
<li>I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New
York,2002.</li>
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis.
Wiley interdisciplinary reviews: computational statistics, 2(4),
433-459.</li>
</ol>
</div>
</div>

    </div>

    



<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/pca/">PCA</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&amp;text=Principal%20Component%20Analysis%20-%20Part%20I" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&amp;t=Principal%20Component%20Analysis%20-%20Part%20I" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Principal%20Component%20Analysis%20-%20Part%20I&amp;body=https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&amp;title=Principal%20Component%20Analysis%20-%20Part%20I" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Principal%20Component%20Analysis%20-%20Part%20I%20https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&amp;title=Principal%20Component%20Analysis%20-%20Part%20I" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu43a4d92e507f8fd6150aa45a41e27c0c_293040_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://biswajitsahoo1111.github.io/">Biswajit Sahoo</a></h5>
      <h6 class="card-subtitle">PhD Student</h6>
      <p class="card-text">My research interests include machine learning, deep learning, signal processing and data-driven machinery condition monitoring.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/biswajitsahoo1111" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/biswajitsahoo1111/" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/biswajitsahoo11" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.in/citations?hl=en&amp;user=zu2CSBMAAAAJ" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@biswajitsahoo1111" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/rul_codes_open/">Data-Driven Remaining Useful Life (RUL) Prediction</a></li>
      
      <li><a href="/project/cbm_codes_open/">Data-Driven Machinery Fault Diagnosis</a></li>
      
    </ul>
  </div>
  


    </div>
    <div class="col-12 col-lg-10ish article-style">
    <script src="https://utteranc.es/client.js"
        repo="biswajitsahoo1111/biswajitsahoo1111.github.io"
        issue-term="title"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
    </div>

  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/latex.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/bash.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.9a374e5cab8073b17a30ac262e5eb3bc.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2021 Biswajit Sahoo &middot; 

    Powered by <a href="https://www.r-project.org/" rel="noopener"><i class="fab fa-r-project"></i> </a><a href="https://github.com/rstudio/blogdown" rel="noopener">blogdown</a> package and the
    <a href="https://sourcethemes.com/academic/" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
