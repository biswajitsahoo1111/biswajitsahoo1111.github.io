<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: December 28, 2024 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Biswajit Sahoo, Ph.D." />

  
  
  
    
  
  <meta name="description" content="An introduction to the theory of Principal Components." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/post/principal-component-analysis-part-i/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/emerald.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu15771692248986819267.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu13363495885888113771.png" />

  <link rel="canonical" href="http://localhost:1313/post/principal-component-analysis-part-i/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Biswajit Sahoo, Ph.D." />
  <meta property="og:url" content="http://localhost:1313/post/principal-component-analysis-part-i/" />
  <meta property="og:title" content="Principal Component Analysis - Part I | Biswajit Sahoo, Ph.D." />
  <meta property="og:description" content="An introduction to the theory of Principal Components." /><meta property="og:image" content="http://localhost:1313/media/icon_hu14255546723713444625.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_hu14255546723713444625.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2019-02-03T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2019-02-03T00:00:00&#43;00:00">
  

  



  


  <title>Principal Component Analysis - Part I | Biswajit Sahoo, Ph.D.</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  







<link type="text/css" rel="stylesheet" href="/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>










  
  
  <link type="text/css" rel="stylesheet" href="/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr&#43;QR0SQDNfgglgtcM=" />
  
  
  <script defer src="/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js" integrity="sha256-3ISyluw&#43;iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script>
  
  
  
  
  <script defer src="/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js" integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  






  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Biswajit Sahoo, Ph.D.">
        Biswajit Sahoo, Ph.D.
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ml-auto mr-6">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >Home</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#blog"
        >Blog</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#projects"
        >Projects</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#experience"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#publications"
        >Publications</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/personal/"
        >Personal</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#contact"
        >Contact</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body  my-10">
      





<div class="mx-auto flex max-w-screen-xl">
  



<aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:hidden xl:block">
  
  <div class="px-4 pt-4 lg:hidden">
    
    
  </div>
  <div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]">
    <ul class="flex flex-col gap-1 lg:hidden">
      
      
        <li class="open"><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/"
    
  >Blog
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/compute-element-stiffness-matrix-symbolically/"
    
  >Compute element stiffness matrix symbolically
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/making-github-traffic-type-plots/"
    
  >Making Github Traffic Type Plots
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/indexdslices-in-tensorflow/"
    
  >IndexedSlices in Tensorflow
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/"
    
  >Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/reading-multiple-files-in-tensorflow-2-using-sequence/"
    
  >Reading multiple files in Tensorflow 2 using Sequence
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/reading-multiple-csv-files-in-pytorch/"
    
  >Reading multiple csv files in PyTorch
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/efficiently-reading-multiple-files-in-tensorflow-2/"
    
  >Efficiently reading multiple files in Tensorflow 2
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/doing-linear-algebra-using-tensorflow-2/"
    
  >Doing Linear Algebra using Tensorflow 2
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/reading-multiple-files-in-tensorflow-2/"
    
  >Reading multiple files in Tensorflow 2
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/using-python-generators/"
    
  >Using Python Generators
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/fault-diagnosis-of-machines/"
    
  >Fault Diagnosis of Machines
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/revisiting-systems-of-linear-equations/"
    
  >Revisiting Systems of Linear Equations
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/principal-component-analysis-part-iii/"
    
  >Principal Component Analysis - Part III
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/principal-component-analysis-part-ii/"
    
  >Principal Component Analysis - Part II
    </a>
              
            </li><li class="flex flex-col open"><a
    class="hb-sidebar-custom-link
      sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900"
    href="/post/principal-component-analysis-part-i/"
    
  >Principal Component Analysis - Part I
    </a>
  
    <ul class="hb-sidebar-mobile-toc"><li>
              <a
                href="#theory"
                class="hb-docs-link"
              >Theory:</a>
            </li>
          <li>
              <a
                href="#references"
                class="hb-docs-link"
              >References</a>
            </li>
          </ul>
  
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/"
    
  >Publications
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/data_driven_features_fault_diagnosis/"
    
  >Multiclass bearing fault classification using features learned by a deep neural network
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/ml_regression_optimization/"
    
  >Machine Learning, Regression, and Optimization
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/spca_fault_diagnosis/"
    
  >Feature Subset Selection Using Sparse Principal Component Analysis and Multiclass Fault Classification Using Selected Features
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/"
    
  >Projects
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/rul_codes_open/"
    
  >Data-Driven Remaining Useful Life (RUL) Prediction
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/cbm_codes_open/"
    
  >Data-Driven Machinery Fault Diagnosis
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/personal/"
    
  >
    </a></li>
    </ul>

    <div class="max-xl:hidden h-0 w-64 shrink-0"></div></div>

</aside>
  

<nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents">
  











  <div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#theory">Theory:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#sketch-of-the-proof">Sketch of the Proof</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#properties-of-pca-transformation">Properties of PCA Transformation</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#link-between-total-variance-and-eigenvalues">Link between total variance and eigenvalues</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#variations-of-pca">Variations of PCA</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#some-common-terminologies-associated-with-pca">Some common terminologies associated with PCA</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#how-actually-are-principal-components-computed">How actually are principal components computed?</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#references">References</a>
      </li></ul>

  
  
    
    
  



    












  </div>
  </nav>


  <article class="w-full break-words flex min-h-[calc(100vh-var(--navbar-height))] min-w-0 justify-center pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]">
    <main class="w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12">

      

      <h1 class="mt-2 text-4xl font-bold tracking-tight text-slate-900 dark:text-slate-100">Principal Component Analysis - Part I</h1>

      <div class="mt-4 mb-16">
      <div class="text-gray-500 dark:text-gray-300 text-sm flex items-center flex-wrap gap-y-2"><span class="mr-1">Feb 3, 2019</span>
        

        
        <span class="mx-1">·</span>
        <span class="mx-1">
          10 min read
        </span>
        
        </div>

        <div class="mt-3">
          





        </div>
      </div>



      
      
      

      
      

      <div class="prose prose-slate lg:prose-xl dark:prose-invert">
        <p>In this post, we will discuss about Principal Component Analysis (PCA),
one of the most popular dimensionality reduction techniques used in
machine learning. Applications of PCA and its variants are ubiquitous.
Thus, a through understanding of PCA is considered essential to start
one&rsquo;s journey into machine learning. In this and subsequent posts, we
will first briefly discuss relevant theory of PCA. Then we will
implement PCA from scratch without using any built-in function. This
will give us an idea as to what happens under the hood when a built-in
function is called in any software environment. Simultaneously, we will
also show how to use built-in commands to obtain results. Finally, we
will reproduce the results of a popular paper on PCA. Including all this
in a single post will make it very very long. Therefore, the post has
been divided into three parts. Readers totally familiar with PCA should
read none and leave this page immediately to save their precious time.
Other readers, who have a passing knowledge of PCA and want to see
different implementations, should pick and choose material from
different parts as per their need. Absolute beginners should start with
Part-I and work their way through gradually. Beginners are also
encouraged to explore the references at the end of this post for further
information. Here is the outline of different parts:</p>
<ul>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/" target="_blank" rel="noopener">Part-I: Basic Theory of
PCA</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II: PCA Implementation with and without using built-in
functions</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener">Part-III: Reproducing results of a published paper on
PCA</a></li>
</ul>
<p>For
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>,
Python, R, and MATLAB code are available to reproduce all the results.
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener">Part-III</a>
contains both R and Python code to reproduce results of the paper. In
this post, we will discuss the theory behind PCA in brief.</p>
<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<h2 id="theory">Theory:</h2>
<p>Given a data matrix, we apply PCA to transform it in a way such that the
transformed data reveals maximum information. So we have to first get
the data on which we want to perform PCA. The usual convention in
storing data is to place variables as columns and different observations
as rows (Data frames in R follow this convention by default). For
example, let&rsquo;s suppose we are collecting data about daily weather for a
year. Our variables of interest may include maximum temperature in a
day, minimum temperature, humidity, max. wind speed, etc. Everyday we
collect observations for each of these variables. In vector form, our
data point for one day will contain number of observations equal to the
number of variables under study and this becomes one row of our data
matrix. Assuming that we are observing 10 variables everyday, our data
matrix for one year (assuming it&rsquo;s not a leap year) will contain 365
rows and 10 columns. Once data matrix is obtained, further analysis is
done on this data matrix to obtain important hidden information
regarding the data. We will use notations from matrix theory to simplify
our analysis.</p>
<p>Let $\textbf{X}$ be the data matrix of size $n\times p$, where $n$ is
the number of data points and $p$ is the number of variables. We can
assume without any loss of generality that data is centered, meaning its
column means are zero. This only shifts the data towards the origin
without changing their relative orientation. So if originally not
centered, it is first centered before doing PCA. From now onward we will
assume that data matrix is always centered.</p>
<p>Variance of a variable (a column) in $\textbf{X}$ is equal to sum of
squares of entries (because the column is centered) of that column
divided by (n - 1) (to make it unbiased). So sum of variance of all
variables is $\frac{1}{n - 1}$ times sum of squares of all elements of
the matrix . Readers who are familiar with matrix norms would instantly
recognize that total variance is $\frac{1}{n - 1}$ times the square of
<strong>Frobenius norm</strong> of $\textbf{X}$. Frobenius norm is nothing but square
root of sum of squares of all elements of a matrix.

$$ \|\textbf{X}\|_{F} = (\sum_{i,j}{x_{ij}^2})^{\frac{1}{2}}=\sqrt{trace(\textbf{X}^T\textbf{X}})=\sqrt{trace(\textbf{X}\textbf{X}^T})$$

</p>
<p>Using this definition, total variance before transformation =

$$\begin{aligned}\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=\frac{1}{n-1}trace(\textbf{X}^T\textbf{X})
&= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) \\
&= \frac{1}{n-1}\|\textbf{X}\|_{F}^2\end{aligned}$$

</p>
<p>Where, trace of a matrix is the sum of its diagonal entries and
 $\|\textbf{X}\|_{F}^2$ 
 is the square of <strong>Frobenius norm</strong>.</p>
<p>The aim of PCA is to transform the data in such a way that along first
principal direction, variance of transformed data is maximum. It
subsequently finds second principal direction orthogonal to the first
one in such a way that it explains maximum of the remaining variance
among all possible direction in the orthogonal subspace.</p>
<p>In matrix form the transformation can be written as

$$\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}$$


Where $\textbf{Y}$ is the transformed data matrix. The columns of
$\textbf{Y}$ are called principal components and $\textbf{P}$ is usually
called loading matrix. Our aim is to find matrix $\textbf{P}$. Once we
find $\textbf{P}$ we can then find $\textbf{Y}$ just by a matrix
multiplication. We will show in the next section that matrix
$\textbf{P}$ is the eigenvector matrix of the covariance matrix. Before
that, let&rsquo;s first define the covariance matrix.</p>
$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}$$<p>
Now we will show how to compute the loading vectors (columns of $\textbf{P}$) and consequently the principal components (columns of $\textbf{Y}$) from the given centered data matrix $\textbf{X}$.</p>
<h3 id="sketch-of-the-proof">Sketch of the Proof</h3>
<p>We call it a sketch because we will not be giving the full proof.
Rather, we will give the proof only for the first principal component
and then give a commentary as to how it can be extended for other
principal components.</p>
<p>The first principal component is the result obtained by transforming
original data matrix $\textbf{X}_{n\times p}$
 in such a way that
variance of data along first principal component is the highest. The
transformation is a linear transformation that is obtained by taking
linear combination of the columns of $\textbf{X}_{n\times p}$
. The
coefficients of the linear combination are called loading scores
corresponding to original variables of $\textbf{X}_{n\times p}$
.</p>
<p>Assuming $\boldsymbol{\alpha}_{p\times 1} = [\alpha_1, \alpha_2, \ldots, \alpha_p]^T$
,
where $\alpha_1$, $\alpha_2$, $\ldots$ , $\alpha_p$ are scalars, to be the
loading vector (we don&rsquo;t know, as of now, from where to get
$\alpha_{p \times 1}$. We will find that out shortly.), first principal
component is obtained by the the product
$\textbf{X}_{n\times p} \boldsymbol{\alpha}_{p\times 1}$
. This product can be written
as

$$
\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1} = \alpha_1 \textbf{X}_{[:,1]} +\alpha_2 \textbf{X}_{[:,2]} + ...  + \alpha_p \textbf{X}_{[:,p]} 
$$

</p>
<p>Where, $\textbf{X}_{[:,1]}$
 is the first column of
$\textbf{X}_{n\times p}$
. Similarly for other columns. The above
equation makes it clear as to why first principal component is a linear
combination of variables of original data matrix. In the original data
matrix, each column corresponds to a variable.</p>
<p>Variance of first principal component is given by
$ \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$
 (As the columns are already
centered. We have also ignored the factor $(\frac{1}{n-1})$ as it is
just a scaling factor.). Now our goal is to find an  $\boldsymbol{\alpha}_{p\times 1}$ 

that maximizes $\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$
. As
$\boldsymbol{\alpha}_{p\times 1}$
 is arbitrary, we can choose its entries in such a
way that variance increases as much as we please. So to get any
meaningful solution, we have to apply some constraints on
$\boldsymbol{\alpha}_{p\times 1}$
. The conventional condition is
$\|\boldsymbol{\alpha}_{p\times 1}\|^2 = 1$
. The optimization problem becomes
$$ maximize \ \ \   \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}$$

$$s.t. \|\boldsymbol{\alpha}\|^2 = 1$$
 Using Lagrange multipliers, this problem can
be written as

$$maximize \ \ \  \mathcal{L}(\boldsymbol{\alpha}, \lambda)=\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha} + \lambda (1 - \boldsymbol{\alpha}^T\boldsymbol{\alpha})$$


Taking gradient of $\mathcal{L}(\boldsymbol{\alpha}, \lambda)$
 with respect to
$\boldsymbol{\alpha}$
 we get, $\textbf{X}^T\textbf{X}\boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}$
. So
$\boldsymbol{\alpha}$
 is the eigenvector of $\textbf{X}^T\textbf{X}$
. It turns out
that for first principal component, $\boldsymbol{\alpha}$
 is the eigenvector
corresponding to the largest eigenvalue.</p>
<p>Loading vector for second principal component is computed with the added condition that second loading vector is orthogonal to the first one. With little bit of more work it can be shown that loading vectors for successive principal components are obtained from eigenvectors corresponding to eigenvalues in decreasing order. More details can be found in reference [1].</p>
<p>Now, it is straightforward to first form the covariance matrix and by
placing its eigenvectors as columns, we can find matrix $\textbf{P}$ and
consequently the principal components. The eigenvectors are arranged in
such a way that first column is the eigenvector corresponding to largest
eigenvector, second column (second eigenvector) corresponds to second
largest eigenvalue and so on. Here we have assumed that we will always
be able to find all the $p$ orthogonal eigenvectors. In fact, we will
always be able to find $p$ orthogonal eigenvectors as the matrix is
symmetric. It can also be shown that the transformed matrix $\textbf{Y}$
is centered and more remarkably, total variance of columns of
$\textbf{Y}$ is same as total variance of columns of $\textbf{X}$. We
will prove these two propositions as the proofs are short.</p>
<h3 id="properties-of-pca-transformation">Properties of PCA Transformation</h3>
<ol>
<li>
<p>Principal components are centered.</p>
$$\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}$$<p> But
columns of $\textbf{X}$ are already centered, so
$\textbf{1}^T\textbf{X}=\textbf{0}$. Thus
$\textbf{1}^T \textbf{Y}= \textbf{0}$. Hence columns of $\textbf{Y}$
are centered.</p>
</li>
<li>
<p>Sum of variance of principal components is equal to sum of variance
of variables before transformation.</p>
<p><strong>Proof</strong>: To prove that total variance of $\textbf{Y}$ also remains
same, observe that

    $$\begin{aligned} \mbox{total covariance of} \ \  \textbf{Y} &=
    trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y}) \\ &=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P}) \\ &=\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X}) \\ 
    &= trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\end{aligned}$$
    

The previous equation uses the fact that trace is
commutative(i.e.$trace(\textbf{AB})=trace(\textbf{BA})$) and
$\textbf{P}$ is orthogonal (i.e.
$\textbf{P}\textbf{P}^T=\textbf{I}$).</p>
</li>
<li>
<p>Principal components are orthogonal.</p>
<p><strong>Proof</strong>: To prove the above claim, it is sufficient to show that
the matrix $\textbf{Y}^T\textbf{Y}$ is diagonal. Remember that columns of $\textbf{Y}$ are principal components. So if we can somehow show $\textbf{Y}^T\textbf{Y}$ to be diagonal, it would automatically mean that principal components are orthogonal.
We know, $\textbf{Y} = \textbf{X}\textbf{P}$. So $\textbf{Y}^T\textbf{Y} = \textbf{P}^T\textbf{X}^T\textbf{X}\textbf{P}$. From sketch of the proof, we know that $\textbf{P}$ is orthogonal as we have required successive loading vectors to be orthogonal to previous ones. We also know that $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$. So from <a href="https://mathworld.wolfram.com/EigenDecompositionTheorem.html" target="_blank" rel="noopener">Eigen Decomposition Theorem</a>, it follows that $\textbf{P}^T(\textbf{X}^T\textbf{X})\textbf{P}$ is diagonal as $\textbf{P}$ is the eigenvector matrix of $\textbf{X}^T\textbf{X}$ and $\textbf{P}$ is orthogonal (so $\textbf{P}^{-1} = \textbf{P}^T$).</p>
</li>
</ol>
<h3 id="link-between-total-variance-and-eigenvalues">Link between total variance and eigenvalues</h3>
<p>Total variance is sum of eigenvalues of covariance matrix
$(\textbf{S})$. This follows from the fact that <span style="color: hotpink"><em>trace of a matrix is sum of its eigenvalues</em></span>. Total variance of original data matrix is $\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) =trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) = trace(\textbf{S})$. We will show these calculations using a publicly available dataset in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>.</p>
<h3 id="variations-of-pca">Variations of PCA</h3>
<p>Sometimes our data matrix contains variables that are measured in
different units. So we might have to scale the centered matrix to reduce
the effect of variables with large variation. So depending on the matrix
on which PCA is performed, it is divided into two types.</p>
<ul>
<li>Covariance PCA (Data matrix is centered but <strong>not</strong> scaled)</li>
<li>Correlation PCA (Data matrix is centered and scaled)</li>
</ul>
<p>Examples of these two types can be found in
<a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/" target="_blank" rel="noopener">Part-II</a>.
Please note that the above two variations are just two among many
variations. There are <strong>Sparse PCA</strong>, <strong>Kernel PCA</strong>, <strong>Robust PCA</strong>,
<strong>Non-negative PCA</strong> and many others. We have mentioned the two that are
most widely used.</p>
<h3 id="some-common-terminologies-associated-with-pca">Some common terminologies associated with PCA</h3>
<p>In literature, there is no standard terminology for different terms in
PCA. Different people use different (often contradictory) terminology
thus confusing newcomers. Therefore, it is better to stick to one set of
terminologies and notations and use those consistently. We will stick to
the terminology used in reference [2].</p>
<ul>
<li>
<p><strong>Factor scores</strong> corresponding to a principal component: Values of
that column of $\textbf{Y}$ that corresponds to the desired
principal component.</p>
</li>
<li>
<p><strong>Loading score</strong>: Values corresponding to a column of $\textbf{P}$.
For example,loading scores of variables corresponding to first
principal component are the values of the first column of
$\textbf{P}$.</p>
</li>
<li>
<p><strong>Inertia</strong>: Square of Frobenius norm of the matrix.</p>
</li>
</ul>
<h3 id="how-actually-are-principal-components-computed">How actually are principal components computed?</h3>
$$ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T$$<p>
Where, $\textbf{X}$ is of size $n\times p$. $\textbf{U}$ and
$\textbf{V}$ are orthogonal matrices of size $n\times n$ and $p\times p$
respectively. $\Sigma$ is a diagonal matrix of size $n\times p$.</p>
$$\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T$$$$\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T$$<p>
Thus eigenvalues of S are diagonal entries of $(\frac{1}{n-1}\Sigma^2)$.
As SVD is computationally efficient, all built-in functions use SVD to
compute the loading matrix and then use the loading matrix to find
principal components.</p>
<p>In the interest of keeping the post at a reasonable length, we will stop
our exposition of theory here. Whatever we have discussed is only a
fraction of everything. Entire books have been written on PCA.
Interested readers who want to pursue this further can refer the
references of this post as a starting point. Readers are encouraged to
bring any errors or omissions to my notice.</p>
<p>Last modified: May 5, 2021</p>
<h2 id="references">References</h2>
<ol>
<li>I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New
York,2002.</li>
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis.
Wiley interdisciplinary reviews: computational statistics, 2(4),
433-459.</li>
</ol>

      </div>

      
  <time class="mt-12 mb-8 block text-xs text-gray-500 ltr:text-right rtl:text-left dark:text-gray-400" datetime="2019-02-03T00:00:00.000Z">
    <span>Last updated on</span>
    Feb 3, 2019</time>

      <div class="container mx-auto prose prose-slate lg:prose-xl dark:prose-invert mt-5">
        
        <div class="max-w-prose print:hidden">
  
  

  

<div class="flex justify-center">
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/pca/">PCA</a>
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/r/">R</a>
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/python/">Python</a>
  
</div>


  
<section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;text=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I"
    title="X"
    aria-label="X"
    id="share-link-x"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;t=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I"
    title="Facebook"
    aria-label="Facebook"
    id="share-link-facebook"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="currentColor" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55 0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95z"/></svg>
  </a>
  

  
  
  
  
  
  
    
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="mailto:?subject=Principal%20Component%20Analysis%20-%20Part%20I&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F"
    title="Email"
    aria-label="Email"
    id="share-link-email"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5 0 1 1-9 0a4.5 4.5 0 0 1 9 0Zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 1 0-2.636 6.364M16.5 12V8.25"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F&amp;title=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I"
    title="LinkedIn"
    aria-label="LinkedIn"
    id="share-link-linkedin"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="whatsapp://send?text=Principal&#43;Component&#43;Analysis&#43;-&#43;Part&#43;I%20http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fprincipal-component-analysis-part-i%2F"
    title="WhatsApp"
    aria-label="WhatsApp"
    id="share-link-whatsapp"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256" fill="currentColor"><path d="m187.58 144.84-32-16a8 8 0 0 0-8 .5l-14.69 9.8a40.55 40.55 0 0 1-16-16l9.8-14.69a8 8 0 0 0 .5-8l-16-32A8 8 0 0 0 104 64a40 40 0 0 0-40 40 88.1 88.1 0 0 0 88 88 40 40 0 0 0 40-40 8 8 0 0 0-4.42-7.16ZM152 176a72.08 72.08 0 0 1-72-72 24 24 0 0 1 19.29-23.54l11.48 23L101 118a8 8 0 0 0-.73 7.51 56.47 56.47 0 0 0 30.15 30.15A8 8 0 0 0 138 155l14.61-9.74 23 11.48A24 24 0 0 1 152 176ZM128 24a104 104 0 0 0-91.82 152.88l-11.35 34.05a16 16 0 0 0 20.24 20.24l34.05-11.35A104 104 0 1 0 128 24Zm0 192a87.87 87.87 0 0 1-44.06-11.81 8 8 0 0 0-6.54-.67L40 216l12.47-37.4a8 8 0 0 0-.66-6.54A88 88 0 1 1 128 216Z"/></svg>
  </a>
  
</section>


  








  
  



  
  
  
    
  
  
  

<div class="flex pt-12 pb-4">
  
  
  <img
    class="mr-4 h-24 w-24 rounded-full"
    width="96"
    height="96"
    alt="Biswajit Sahoo, Ph.D."
  src="/author/biswajit-sahoo-ph.d./avatar_hu13253178770287002488.jpg"
  loading="lazy"
  />
  
  <div class="place-self-center">
    <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
      Authors
    </div>
    <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
      <a href="http://localhost:1313/" class="no-underline">
      Biswajit Sahoo, Ph.D.
      </a>
    </div>

    
    <div class="text-sm font-bold text-neutral-700 dark:text-neutral-300">
    Machine Learning Engineer
    </div>
    


    
    <div class="text-sm text-neutral-700 dark:text-neutral-300">My research interests include machine learning, deep learning, signal processing and data-driven machinery condition monitoring.</div>
    

    <div class="text-2xl sm:text-lg pt-1">

      
<div class="flex flex-wrap text-neutral-500 dark:text-neutral-300">
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="/#contact"
      
      aria-label="Envelope"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://github.com/biswajitsahoo1111"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/Github"
    ><svg style="height: 1em;" fill="currentColor" viewBox="3 3 18 18"><path d="M12 3C7.0275 3 3 7.12937 3 12.2276C3 16.3109 5.57625 19.7597 9.15374 20.9824C9.60374 21.0631 9.77249 20.7863 9.77249 20.5441C9.77249 20.3249 9.76125 19.5982 9.76125 18.8254C7.5 19.2522 6.915 18.2602 6.735 17.7412C6.63375 17.4759 6.19499 16.6569 5.8125 16.4378C5.4975 16.2647 5.0475 15.838 5.80124 15.8264C6.51 15.8149 7.01625 16.4954 7.18499 16.7723C7.99499 18.1679 9.28875 17.7758 9.80625 17.5335C9.885 16.9337 10.1212 16.53 10.38 16.2993C8.3775 16.0687 6.285 15.2728 6.285 11.7432C6.285 10.7397 6.63375 9.9092 7.20749 9.26326C7.1175 9.03257 6.8025 8.08674 7.2975 6.81794C7.2975 6.81794 8.05125 6.57571 9.77249 7.76377C10.4925 7.55615 11.2575 7.45234 12.0225 7.45234C12.7875 7.45234 13.5525 7.55615 14.2725 7.76377C15.9937 6.56418 16.7475 6.81794 16.7475 6.81794C17.2424 8.08674 16.9275 9.03257 16.8375 9.26326C17.4113 9.9092 17.76 10.7281 17.76 11.7432C17.76 15.2843 15.6563 16.0687 13.6537 16.2993C13.98 16.5877 14.2613 17.1414 14.2613 18.0065C14.2613 19.2407 14.25 20.2326 14.25 20.5441C14.25 20.7863 14.4188 21.0746 14.8688 20.9824C16.6554 20.364 18.2079 19.1866 19.3078 17.6162C20.4077 16.0457 20.9995 14.1611 21 12.2276C21 7.12937 16.9725 3 12 3Z"></path></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://www.linkedin.com/in/biswajitsahoo1111/"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/Linkedin"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://scholar.google.co.in/citations?hl=en&amp;user=zu2CSBMAAAAJ"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Academicons/Google-Scholar"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M343.759 106.662V79.43L363.524 64h-213.89L20.476 176.274h85.656a82.339 82.339 0 0 0-.219 6.225c0 20.845 7.22 38.087 21.672 51.861c14.453 13.797 32.252 20.648 53.327 20.648c4.923 0 9.75-.368 14.438-1.024c-2.907 6.5-4.374 12.523-4.374 18.142c0 9.875 4.499 20.43 13.467 31.642c-39.234 2.67-68.061 9.732-86.437 21.163c-10.531 6.5-19 14.704-25.39 24.531c-6.391 9.9-9.578 20.515-9.578 31.962c0 9.648 2.062 18.336 6.219 26.062c4.156 7.726 9.578 14.07 16.312 18.984c6.718 4.968 14.469 9.101 23.219 12.469c8.734 3.344 17.406 5.718 26.061 7.062A167.052 167.052 0 0 0 180.555 448c13.469 0 26.953-1.734 40.547-5.187c13.562-3.485 26.28-8.642 38.171-15.493c11.86-6.805 21.515-16.086 28.922-27.718c7.39-11.68 11.094-24.805 11.094-39.336c0-11.016-2.25-21.039-6.75-30.14c-4.468-9.073-9.938-16.542-16.452-22.345c-6.501-5.813-13-11.155-19.516-15.968c-6.5-4.845-12-9.75-16.468-14.813c-4.485-5.046-6.735-10.054-6.735-14.984c0-4.921 1.734-9.672 5.216-14.265c3.455-4.61 7.674-9.048 12.61-13.306c4.937-4.25 9.875-8.968 14.796-14.133c4.922-5.147 9.141-11.827 12.61-20.008c3.485-8.18 5.203-17.445 5.203-27.757c0-13.453-2.547-24.46-7.547-33.314c-.594-1.022-1.218-1.803-1.875-3.022l56.907-46.672v17.119c-7.393.93-6.624 5.345-6.624 10.635V245.96c0 5.958 4.875 10.834 10.834 10.834h3.989c5.958 0 10.833-4.875 10.833-10.834V117.293c0-5.277.778-9.688-6.561-10.63zm-107.36 222.48c1.14.75 3.704 2.78 7.718 6.038c4.05 3.243 6.797 5.695 8.266 7.414a443.553 443.553 0 0 1 6.376 7.547c2.813 3.375 4.718 6.304 5.718 8.734c1 2.477 2.016 5.461 3.047 8.946a38.27 38.27 0 0 1 1.485 10.562c0 17.048-6.564 29.68-19.656 37.859c-13.125 8.18-28.767 12.274-46.938 12.274c-9.187 0-18.203-1.093-27.063-3.196c-8.843-2.116-17.311-5.336-25.39-9.601c-8.078-4.258-14.577-10.204-19.5-17.797c-4.938-7.64-7.407-16.415-7.407-26.25c0-10.32 2.797-19.29 8.422-26.906c5.594-7.625 12.938-13.391 22.032-17.315c9.063-3.946 18.25-6.742 27.562-8.398a157.865 157.865 0 0 1 28.438-2.555c4.47 0 7.936.25 10.405.696c.455.219 3.032 2.07 7.735 5.563c4.704 3.462 7.625 5.595 8.75 6.384zm-3.359-100.579c-7.406 8.86-17.734 13.288-30.953 13.288c-11.86 0-22.298-4.764-31.266-14.312c-9-9.523-15.422-20.328-19.344-32.43c-3.937-12.109-5.906-23.984-5.906-35.648c0-13.694 3.596-25.352 10.781-34.976c7.187-9.65 17.5-14.485 30.938-14.485c11.875 0 22.374 5.038 31.437 15.157c9.094 10.085 15.61 21.413 19.517 33.968c3.922 12.54 5.873 24.53 5.873 35.984c0 13.446-3.702 24.61-11.076 33.454z"/></svg></a>
  
</div>



    </div>
  </div>
</div>







  
  
    
    
    
      
      
    
<div class="pt-1 no-prose w-full">
  <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
  <div class="flex flex-col md:flex-row flex-nowrap justify-between gap-5 pt-2">
    <div class="">
      
        <a class="group flex no-underline" href="/post/principal-component-analysis-part-ii/">
          <span
            class="mt-[-0.3rem] me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
          ><span class="ltr:inline rtl:hidden">&larr;</span></span>
          <span class="flex flex-col">
            <span
              class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
            >Principal Component Analysis - Part II</span>
            <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
              
                Feb 4, 2019
              
            </span>
          </span>
        </a>
      
    </div>
    <div class="">
      
    </div>
  </div>
</div>



  

  
  


  


  
  
  

  

  
  <section id="comments">
    
  
  <script src="https://giscus.app/client.js"
          data-repo="biswajitsahoo1111/biswajitsahoo1111.github.io"
          data-repo-id="MDEwOlJlcG9zaXRvcnkxNzE5MzExMzM="
          data-category="General"
          data-category-id="DIC_kwDOCj91_c4CljPc"
          data-mapping="title"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="top"
          data-theme="preferred_color_scheme"
          data-lang="en"
          data-loading="lazy"
          crossorigin="anonymous"
          async>
  </script>


  </section>
  


</div>

      </div>

    </main>
  </article>
</div>

    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

    











  
    
    
    
    
    














  
  <p class="powered-by text-center">
    © 2024 Biswajit Sahoo. Powered by <a href="https://hugoblox.com/" target="_blank" rel="noopener">Hugo Blox Builder</a>.
  </p>
  




  
    <p class="powered-by text-center">
      
    </p>
  
  </footer>
    </div>

    
    











  </body>
</html>
