<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.54.0" />
  <meta name="author" content="Biswajit Sahoo">

  
  
  
  
    
  
  <meta name="description" content="In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function.">

  
  <link rel="alternate" hreflang="en-us" href="/post/principal-component-analysis-part-i/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-153208457-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Biswajit Sahoo">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Biswajit Sahoo">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/principal-component-analysis-part-i/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@biswajitsahoo11">
  <meta property="twitter:creator" content="@biswajitsahoo11">
  
  <meta property="og:site_name" content="Biswajit Sahoo">
  <meta property="og:url" content="/post/principal-component-analysis-part-i/">
  <meta property="og:title" content="Principal Component Analysis - Part I | Biswajit Sahoo">
  <meta property="og:description" content="In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-02-03T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-02-03T00:00:00&#43;00:00">
  

  

  

  <title>Principal Component Analysis - Part I | Biswajit Sahoo</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Biswajit Sahoo</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#blog">
            
            <span>Blog</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Principal Component Analysis - Part I</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Biswajit Sahoo">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-02-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2019-02-03 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Feb 3, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Biswajit Sahoo">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/principal-component-analysis-part-i/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/blog/">Blog</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Principal%20Component%20Analysis%20-%20Part%20I&amp;url=%2fpost%2fprincipal-component-analysis-part-i%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fprincipal-component-analysis-part-i%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fprincipal-component-analysis-part-i%2f&amp;title=Principal%20Component%20Analysis%20-%20Part%20I"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fprincipal-component-analysis-part-i%2f&amp;title=Principal%20Component%20Analysis%20-%20Part%20I"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Principal%20Component%20Analysis%20-%20Part%20I&amp;body=%2fpost%2fprincipal-component-analysis-part-i%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      


<p>In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function. This will give us an idea as to what happens under the hood when a built-in function is called in any software environment. Simultaneously, we will also show how to use built-in commands to obtain results. Finally, we will reproduce the results of a popular paper on PCA. Including all this in a single post will make it very very long. Therefore, the post has been divided into three parts. Readers totally familiar with PCA should read none and leave this page immediately to save their precious time. Other readers, who have a passing knowledge of PCA and want to see different implementations, should pick and choose material from different parts as per their need. Absolute beginners should start with Part-I and work their way through gradually. Beginners are also encouraged to explore the references at the end of this post for further information. Here is the outline of different parts:</p>
<ul>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">Part-I: Basic Theory of PCA</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II: PCA Implementation with and without using built-in functions</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/">Part-III: Reproducing results of a published paper on PCA</a></li>
</ul>
<p>For <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>, both MATLAB and R codes are available to reproduce all the results. <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/">Part-III</a> contains both R and Python codes to reproduce results of the paper. In this post, we will discuss only the theory behind PCA.</p>
<div id="principal-component-analysis" class="section level1">
<h1>Principal Component Analysis</h1>
<div id="theory" class="section level2">
<h2>Theory:</h2>
<p>Given a data matrix, we apply PCA to transform it in a way such that the transformed data reveals maximum information. So we have to first get the data on which we want to perform PCA. The usual convention in storing data is to place variables as columns and different observations as rows (Data frames in R follow this convention by default). For example, let’s suppose we are collecting data about daily weather for a year. Our variables of interest may include maximum temperature in a day, minimum temperature, humidity, max. wind speed, etc. For every day, we have to collect observations for each of these variables. In vector form, our data point for one day will contain number of observations equal to the number of variables under study and this becomes one row of our data matrix. Assuming that we are observing 10 variables everyday, our data matrix for one year (assuming it’s not a leap year) will contain 365 rows and 10 columns. Once data matrix is obtained, further analysis is done on this data matrix to obtain important hidden information regarding the data. We will use notations from matrix theory to simplify our analysis.</p>
<p>Let <span class="math inline">\(\textbf{X}\)</span> be the data matrix of size <span class="math inline">\(n\times p\)</span>, where is the number of data points and is the number of variables. We can assume without any loss of generality that is centered, meaning its column means are zero. This only shifts the data towards the origin without changing their relative orientation. So if originally is not centered, it is first centered before doing PCA. From now onward we will assume that is always centered.</p>
<p>Variance of a variable (a column)in <span class="math inline">\(\textbf{X}\)</span> is equal to sum of squares of entries (because the column is centered) of that column divided by (n - 1) to make it unbiased. So sum of variance of all variables is <span class="math inline">\(\frac{1}{n - 1}\)</span> times sum of squares of all elements of the matrix . Readers who are familiar with matrix norms would instantly recognize that total variance is <span class="math inline">\(\frac{1}{n - 1}\)</span> times the square of <strong>Frobenius norm</strong> of <span class="math inline">\(\textbf{X}\)</span>.</p>
<p>Total variance before transformation =
<span class="math display">\[\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})=\frac{1}{n-1}\|\textbf{X}\|_{F}^2\]</span>
Where trace of a matrix is sum of its diagonal entries and <span class="math inline">\(\|\textbf{X}\|_{F}^2\)</span> is the square of <strong>Frobenius norm</strong>.</p>
<p>The aim of PCA is to transform the data in such a way that along first principal direction, variance of transformed data is maximum. It subsequently finds second principal direction orthogonal to the first one in such a way that it explains maximum of the remaining variance among all possible direction in the orthogonal subspace.</p>
<p>In matrix form the transformation can be written as
<span class="math display">\[\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}\]</span>
Where <span class="math inline">\(\textbf{Y}\)</span> is the transformed data matrix. The columns of <span class="math inline">\(\textbf{Y}\)</span> are called principal components and <span class="math inline">\(\textbf{P}\)</span> is usually called loading matrix. Our aim is to find matrix <span class="math inline">\(\textbf{P}\)</span>. Once we find <span class="math inline">\(\textbf{P}\)</span> we can then find <span class="math inline">\(\textbf{Y}\)</span> just by a matrix multiplication. Though we will not go into to proof here, it can be easily proved (see references), that matrix <span class="math inline">\(\textbf{P}\)</span> is the eigenvector matrix of the covariance matrix. Let’s first define covariance matrix.</p>
<p>Given a data matrix <span class="math inline">\(\textbf{X}\)</span>(centered), its covariance matrix <span class="math inline">\((\textbf{S})\)</span> is defined as
<span class="math display">\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}\]</span>
As principal directions are orthogonal, we will also require <span class="math inline">\(\textbf{P}\)</span> to be an orthogonal matrix.</p>
<p>Now, it is straightforward to form the covariance matrix and by placing its eigenvectors as columns, we can find matrix <span class="math inline">\(\textbf{P}\)</span> and consequently the principal components. The eigenvectors are arranged in such a way that first column is the eigenvector corresponding to largest eigenvector, second column (second eigenvector) corresponds to second largest eigenvalue and so on. Here we have assumed that we will always be able to find all the <span class="math inline">\(p\)</span> orthogonal eigenvectors. In fact, we will always be able to find <span class="math inline">\(p\)</span> orthogonal eigenvectors as the matrix is symmetric. It can also be shown that the transformed matrix <span class="math inline">\(\textbf{Y}\)</span> is centered and more remarkably, total variance of columns of <span class="math inline">\(\textbf{Y}\)</span> is same as total variance of columns of <span class="math inline">\(\textbf{X}\)</span>. We will prove these two propositions as the proof are short.</p>
<p>Let <span class="math inline">\(\textbf{1}\)</span> be a column vector of all ones of size <span class="math inline">\((n\times 1)\)</span>. To prove that columns of <span class="math inline">\(\textbf{Y}\)</span> are centered, just premultiply it by <span class="math inline">\(\textbf{1}^T\)</span> (this finds column sum for each column). So
<span class="math display">\[\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}\]</span>
But columns of <span class="math inline">\(\textbf{X}\)</span> are already centered, so <span class="math inline">\(\textbf{1}^T\textbf{X}=\textbf{0}\)</span>. Thus <span class="math inline">\(\textbf{1}^T \textbf{Y}= \textbf{0}\)</span>. Hence columns of <span class="math inline">\(\textbf{Y}\)</span> are centered.</p>
<p>To prove that total variance of <span class="math inline">\(\textbf{Y}\)</span> also remains same, observe that</p>
<p>total covariance of <span class="math inline">\(\textbf{Y}\)</span> =
<span class="math display">\[trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y})=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P})=\\\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X})=trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\]</span>
The previous equation uses the fact that trace is commutative(i.e.<span class="math inline">\(trace(\textbf{AB})=trace(\textbf{BA})\)</span>) and <span class="math inline">\(\textbf{P}\)</span> is orthogonal (i.e. <span class="math inline">\(\textbf{P}\textbf{P}^T=\textbf{I}\)</span>).</p>
<div id="link-between-total-variance-and-eigenvalues" class="section level3">
<h3>Link between total variance and eigenvalues</h3>
<p>Total variance is sum of eigenvalues of covariance matrix <span class="math inline">\((\textbf{S})\)</span>. We will further discuss this point in <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>.</p>
</div>
<div id="variations-in-pca" class="section level3">
<h3>Variations in PCA</h3>
<p>Sometimes our data matrix contains variables that are measured in different units. So we might have to scale the centered matrix to reduce the effect of variables with large variation. So depending on the matrix on which PCA is performed, it is divided into two types.</p>
<ul>
<li>Covariance PCA (Data matrix is centered but not scaled)</li>
<li>Correlation PCA (Data matrix is centered and scaled)</li>
</ul>
<p>Examples of these two types can be found in <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>.</p>
</div>
<div id="some-common-terminology-associated-with-pca" class="section level3">
<h3>Some common terminology associated with PCA</h3>
<ul>
<li><p><strong>Factor scores</strong> corresponding to a principal component: Values of that column of <span class="math inline">\(\textbf{Y}\)</span> that corresponds to the desired principal component.</p></li>
<li><p><strong>Loading score</strong>: Values corresponding to a column of <span class="math inline">\(\textbf{P}\)</span>. For example,loading scores of variables corresponding to first principal component are the values of the first column of <span class="math inline">\(\textbf{P}\)</span>.</p></li>
<li><p><strong>Inertia</strong>: Square of Frobenius norm of the matrix.</p></li>
</ul>
</div>
<div id="how-actually-are-principal-components-computed" class="section level3">
<h3>How actually are principal components computed</h3>
<p>The previously stated method of finding eigenvectors of covariance matrix is not computationally efficient. In practice, singular value decomposition (SVD) is used to find the matrix <span class="math inline">\(\textbf{P}\)</span>. SVD theorem tells that any real matrix <span class="math inline">\(\textbf{X}\)</span> can be decomposed into three matrices such that
<span class="math display">\[ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T\]</span>
Where, <span class="math inline">\(\textbf{X}\)</span> is of size <span class="math inline">\(n\times p\)</span>. <span class="math inline">\(\textbf{U}\)</span> and <span class="math inline">\(\textbf{V}\)</span> are orthogonal matrices of size <span class="math inline">\(n\times n\)</span> and <span class="math inline">\(p\times p\)</span> respectively. <span class="math inline">\(\Sigma\)</span> is a diagonal matrix of size <span class="math inline">\(n\times p\)</span>.</p>
<p>Given the SVD decomposition of a matrix <span class="math inline">\(\textbf{X}\)</span>,
<span class="math display">\[\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T\]</span>
This is the eigen-decomposition of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. So <span class="math inline">\(\textbf{V}\)</span> is the eigenvector matrix of <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span>. For PCA we need eigenvector matrix of covariance matrix. So converting the equation into convenient form, we get
<span class="math display">\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T\]</span>
Thus eigenvalues of S are diagonal entries of <span class="math inline">\((\frac{1}{n-1}\Sigma^2)\)</span>. As SVD is computationally efficient, all built-in functions use SVD to find the loading matrix and then use it to find principal components.</p>
<p>In the interest of keeping the post at a reasonable length, we will stop our exposition of theory here. Whatever we have discussed is only a fraction of everything. Entire books have been written on PCA. Interested readers who want to pursue further can refer to the references given here and later to the references given in the references. Please send me a note if you find any errors.</p>
<p><a href="https://github.com/biswajitsahoo1111/PCA/blob/master/pca_interpretation_part_1.Rmd">R Markdown file for this post</a></p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ol style="list-style-type: decimal">
<li>I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.</li>
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.</li>
</ol>
</div>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="/tags/pca/">PCA</a>
  
  <a class="label label-default" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="label label-default" href="/tags/r/">R</a>
  
</div>




    
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "biswajitsahoo1111" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2020 Biswajit Sahoo &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//biswajitsahoo1111.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/search.json";
      const i18n = {
        'placeholder': "Search...",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    

  </body>
</html>

