<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Biswajit Sahoo">

  
  
  
    
  
  <meta name="description" content="(Python codes for this post can be found here)In this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).">

  
  <link rel="alternate" hreflang="en-us" href="/post/principal-component-analysis-part-iii/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153208457-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-153208457-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huffc8dce834b4c0aec33122a3663a38ac_177939_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huffc8dce834b4c0aec33122a3663a38ac_177939_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/principal-component-analysis-part-iii/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="BISWAJIT SAHOO">
  <meta property="og:url" content="/post/principal-component-analysis-part-iii/">
  <meta property="og:title" content="Principal Component Analysis - Part III | BISWAJIT SAHOO">
  <meta property="og:description" content="(Python codes for this post can be found here)In this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised)."><meta property="og:image" content="/img/icon.png">
  <meta property="twitter:image" content="/img/icon.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-02-05T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-02-05T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/principal-component-analysis-part-iii/"
  },
  "headline": "Principal Component Analysis - Part III",
  
  "datePublished": "2019-02-05T00:00:00Z",
  "dateModified": "2019-02-05T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Biswajit Sahoo"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "BISWAJIT SAHOO",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "(Python codes for this post can be found here)\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised)."
}
</script>

  

  


  


  





  <title>Principal Component Analysis - Part III | BISWAJIT SAHOO</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">BISWAJIT SAHOO</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">BISWAJIT SAHOO</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#blog"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Principal Component Analysis - Part III</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 5, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    16 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/principal-component-analysis-part-iii/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/blog/">Blog</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      


<center>
(<a href="https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/PCA_Abdi_in_python.ipynb">Python codes for this post can be found here</a>)
</center>
<p>In this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘<a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wics.101">Principal component analysis</a>’ and is authored by <a href="https://personal.utdallas.edu/~herve/">Herve Abdi</a> and <a href="https://ljwilliams.github.io/">Lynne J. Williams</a>. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).</p>
<p>This post is Part-III of a three part series on PCA. Other parts of the series can be found at the links below.</p>
<ul>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">Part-I: Basic Theory of PCA</a></li>
<li><a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II: PCA Implementation with and without using built-in functions</a></li>
</ul>
<p>This post contains code snippets in R. Equivalent <a href="https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf">MATLAB codes</a> can be written using commands of <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>. For figures, the reader has to write his/her own code in MATLAB.</p>
<div id="structure-of-the-paper" class="section level3">
<h3>Structure of the paper</h3>
<p>Along with basic theory, the paper contains three examples on PCA, one example on correspondence analysis, and one example on multiple factor analysis. We will only focus on PCA examples in this post.</p>
<p>To run following R codes seamlessly, readers have to load following packages. If these packages have not been installed previously, use <code>install.packages("package_name")</code> to install those.</p>
<pre class="r"><code>library(ggplot2)
library(ggrepel)</code></pre>
</div>
<div id="how-to-get-data" class="section level3">
<h3>How to get data</h3>
<p>Data for the examples have been taken from the paper [1]. The datasets are pretty small. So one way to read the data is to create a dataframe itself in R using the values given in paper. Otherwise, the values can first be stored in a csv file and then read into R. To make this post self-sufficient, we will adopt the former approach.</p>
<p><strong>Note</strong>: Throughout this article, additional comments have been made beside code segments. It would be a good idea to read those commented lines along with the codes.</p>
<pre class="r"><code># Table 1
# Create a dataframe
Words = c(&quot;Bag&quot;, &quot;Across&quot;, &quot;On&quot;, &quot;Insane&quot;, &quot;By&quot;, &quot;Monastery&quot;, &quot;Relief&quot;, &quot;Slope&quot;, &quot;Scoundrel&quot;, &quot;With&quot;, &quot;Neither&quot;, &quot;Pretentious&quot;, &quot;Solid&quot;, &quot;This&quot;, &quot;For&quot;, &quot;Therefore&quot;, &quot;Generality&quot;, &quot;Arise&quot;, &quot;Blot&quot;, &quot;Infectious&quot;)
Word_length = c(3, 6, 2, 6, 2, 9, 6, 5, 9, 4, 7, 11, 5, 4, 3, 9, 10, 5, 4, 10)
Lines_in_dict = c(14, 7, 11, 9, 9, 4, 8, 11, 5, 8, 2, 4, 12, 9, 8, 1, 4, 13, 15, 6)
words = data.frame(Words, Word_length, Lines_in_dict, stringsAsFactors = F)
words</code></pre>
<pre><code>         Words Word_length Lines_in_dict
1          Bag           3            14
2       Across           6             7
3           On           2            11
4       Insane           6             9
5           By           2             9
6    Monastery           9             4
7       Relief           6             8
8        Slope           5            11
9    Scoundrel           9             5
10        With           4             8
11     Neither           7             2
12 Pretentious          11             4
13       Solid           5            12
14        This           4             9
15         For           3             8
16   Therefore           9             1
17  Generality          10             4
18       Arise           5            13
19        Blot           4            15
20  Infectious          10             6</code></pre>
<pre class="r"><code>(words_centered = scale(words[,2:3],scale = F)) # Centering after reemoving the first column</code></pre>
<pre><code>      Word_length Lines_in_dict
 [1,]          -3             6
 [2,]           0            -1
 [3,]          -4             3
 [4,]           0             1
 [5,]          -4             1
 [6,]           3            -4
 [7,]           0             0
 [8,]          -1             3
 [9,]           3            -3
[10,]          -2             0
[11,]           1            -6
[12,]           5            -4
[13,]          -1             4
[14,]          -2             1
[15,]          -3             0
[16,]           3            -7
[17,]           4            -4
[18,]          -1             5
[19,]          -2             7
[20,]           4            -2
attr(,&quot;scaled:center&quot;)
  Word_length Lines_in_dict 
            6             8 </code></pre>
</div>
<div id="covariance-pca" class="section level3">
<h3>Covariance PCA</h3>
<p>Covariance PCA uses centered data matrix. But data matrix is not scaled. <code>prcomp()</code> centers data by default.</p>
<pre class="r"><code>pca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA
factor_scores_words = pca_words_cov$x
round(factor_scores_words,2)</code></pre>
<pre><code>        PC1   PC2
 [1,] -6.67  0.69
 [2,]  0.84 -0.54
 [3,] -4.68 -1.76
 [4,] -0.84  0.54
 [5,] -2.99 -2.84
 [6,]  4.99  0.38
 [7,]  0.00  0.00
 [8,] -3.07  0.77
 [9,]  4.14  0.92
[10,] -1.07 -1.69
[11,]  5.60 -2.38
[12,]  6.06  2.07
[13,] -3.91  1.30
[14,] -1.92 -1.15
[15,] -1.61 -2.53
[16,]  7.52 -1.23
[17,]  5.52  1.23
[18,] -4.76  1.84
[19,] -6.98  2.07
[20,]  3.83  2.30</code></pre>
<p>Observer that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as principal directions are orthogonal.</p>
</div>
<div id="principal-directions-are-orthogonal" class="section level3">
<h3>Principal directions are orthogonal</h3>
<pre class="r"><code>#  It can also be checked that both the principal components are orthogonal.
sum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal</code></pre>
<pre><code>[1] -4.773959e-15</code></pre>
</div>
<div id="contribution-of-each-factor" class="section level3">
<h3>Contribution of each factor</h3>
<p>It is defined as square of factor score divided by sum of squares of factor scores in that column.</p>
<pre class="r"><code>round(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)</code></pre>
<pre><code> [1] 11.36  0.18  5.58  0.18  2.28  6.34  0.00  2.40  4.38  0.29  8.00  9.37
[13]  3.90  0.94  0.66 14.41  7.78  5.77 12.43  3.75</code></pre>
<pre class="r"><code>round(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)</code></pre>
<pre><code> [1]  0.92  0.55  5.98  0.55 15.49  0.28  0.00  1.13  1.63  5.48 10.87  8.25
[13]  3.27  2.55 12.32  2.90  2.90  6.52  8.25 10.18</code></pre>
<p>The calculations in above two lines can be done in a single line</p>
<pre class="r"><code>round(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)</code></pre>
<pre><code>        PC1   PC2
 [1,] 11.36  0.92
 [2,]  0.18  0.55
 [3,]  5.58  5.98
 [4,]  0.18  0.55
 [5,]  2.28 15.49
 [6,]  6.34  0.28
 [7,]  0.00  0.00
 [8,]  2.40  1.13
 [9,]  4.38  1.63
[10,]  0.29  5.48
[11,]  8.00 10.87
[12,]  9.37  8.25
[13,]  3.90  3.27
[14,]  0.94  2.55
[15,]  0.66 12.32
[16,] 14.41  2.90
[17,]  7.78  2.90
[18,]  5.77  6.52
[19,] 12.43  8.25
[20,]  3.75 10.18</code></pre>
</div>
<div id="squared-distance-to-center-of-gravity" class="section level3">
<h3>Squared distance to center of gravity</h3>
<pre class="r"><code>(dist = rowSums(factor_scores_words^2))</code></pre>
<pre><code> [1] 45  1 25  1 17 25  0 10 18  4 37 41 17  5  9 58 32 26 53 20</code></pre>
</div>
<div id="squared-cosine-of-observations" class="section level3">
<h3>Squared cosine of observations</h3>
<pre class="r"><code>(sq_cos = round(factor_scores_words^2/rowSums(factor_scores_words^2)*100))</code></pre>
<pre><code>      PC1 PC2
 [1,]  99   1
 [2,]  71  29
 [3,]  88  12
 [4,]  71  29
 [5,]  53  47
 [6,]  99   1
 [7,] NaN NaN
 [8,]  94   6
 [9,]  95   5
[10,]  29  71
[11,]  85  15
[12,]  90  10
[13,]  90  10
[14,]  74  26
[15,]  29  71
[16,]  97   3
[17,]  95   5
[18,]  87  13
[19,]  92   8
[20,]  74  26</code></pre>
<p>Nan’s are produced because of division by zero.</p>
<pre class="r"><code># Figue 1
p = ggplot(words,aes(x = Lines_in_dict,y = Word_length,label = Words))+
  geom_point()+ geom_text_repel()+ 
  geom_hline(yintercept = 6)+geom_vline(xintercept = 8)+
  labs(x = &quot;Lines in dictionary&quot;,y = &quot;Word length&quot;)
print(p)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code># Show directions of PCs
# Note that intercept argument in geom_abline considers the line to be at the origin. In our case the data are mean shifted.
# So we have to adjust the intercept taking new origin into consideration. These adjustments have been made below.
slope1 = pca_words_cov$rotation[1,1]/pca_words_cov$rotation[2,1] # Slope of first PC
slope2 = pca_words_cov$rotation[1,2]/pca_words_cov$rotation[2,2] # Slope of second PC
(new_origin = c(mean(words$Lines_in_dict),mean(words$Word_length)))</code></pre>
<pre><code>[1] 8 6</code></pre>
<pre class="r"><code>intercept1 = 6 - slope1*8
intercept2 = 6 - slope2*8
p+geom_abline(slope = slope1,intercept = intercept1,linetype = &quot;dashed&quot;,size = 1.2,col = &quot;red&quot;)+
  geom_abline(slope = slope2,intercept = intercept2,linetype = &quot;dashed&quot;,size = 1.2,col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-10-2.png" width="672" />
In the above figure red dashed line is the 1st principal component (PC) and blue dashed line is the 2nd PC.</p>
</div>
<div id="rotated-pcs" class="section level3">
<h3>Rotated PCs</h3>
<p>This figure is obtained by plotting factor scores. Note that we will plot negative of the factor scores of 1st PC to make the figure consistent with the paper.</p>
<pre class="r"><code>ggplot(as.data.frame(pca_words_cov$x),aes(-pca_words_cov$x[,1],pca_words_cov$x[,2],label = words$Words))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&quot;Factor score along PC1&quot;)+ylab(&quot;Factor score along PC2&quot;)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="with-supplementary-data" class="section level3">
<h3>With supplementary data</h3>
<p>Given a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix.</p>
</div>
<div id="factor-score-of-the-new-word-sur" class="section level3">
<h3>Factor score of the new word ‘sur’</h3>
<pre class="r"><code>sur = c(3,12) # It has 3 letter and 12 lines of dictionary entry
(sur_centered = sur - colMeans(words[,2:3]))</code></pre>
<pre><code>  Word_length Lines_in_dict 
           -3             4 </code></pre>
<pre class="r"><code>(factor_scores_sur = round(sur_centered %*% pca_words_cov$rotation,2))</code></pre>
<pre><code>       PC1   PC2
[1,] -4.99 -0.38</code></pre>
</div>
<div id="eigenvalues-and-variance" class="section level3">
<h3>Eigenvalues and variance</h3>
<p>See <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a> for details.</p>
</div>
<div id="total-variance-before-transformation" class="section level3">
<h3>Total variance before transformation</h3>
<pre class="r"><code>(total_var_before = round(sum(diag(var(words_centered))),3))</code></pre>
<pre><code>[1] 23.368</code></pre>
</div>
<div id="total-variance-after-transformation" class="section level3">
<h3>Total variance after transformation</h3>
<pre class="r"><code>(total_var_after = round(sum(diag(var(pca_words_cov$x))),3))</code></pre>
<pre><code>[1] 23.368</code></pre>
<p>Correlation between principal components and original variables
(In the paper,this correlation is also termed loading. But we will strictly reserve the loading term to mean loading matrix <span class="math inline">\(\textbf{P}\)</span> (see <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/">Part-I</a>)</p>
<p>The sum of correlation coefficients between variables and principal components is 1. Intuitively, this means that variables are orthogonally projected onto the principal components.</p>
</div>
<div id="correlation-matrix" class="section level3">
<h3>Correlation matrix</h3>
<pre class="r"><code># Correlation between PCs and original variables
(cor(pca_words_cov$x,words_centered))</code></pre>
<pre><code>    Word_length Lines_in_dict
PC1   0.8679026    -0.9741764
PC2   0.4967344     0.2257884</code></pre>
<p><strong>Note</strong> that the answers for correlation coefficients don’t match with that of the paper. Readers who get actual answers as given in paper are encouraged to send me an email using my contact details. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below.</p>
</div>
<div id="squared-correlation" class="section level3">
<h3>Squared correlation</h3>
<pre class="r"><code>(cor(pca_words_cov$x,words_centered)^2)</code></pre>
<pre><code>    Word_length Lines_in_dict
PC1   0.7532549    0.94901961
PC2   0.2467451    0.05098039</code></pre>
<p>Sum of correlation coefficients between variables and principal components is 1.</p>
<pre class="r"><code>colSums((cor(pca_words_cov$x,words_centered)^2))</code></pre>
<pre><code>  Word_length Lines_in_dict 
            1             1 </code></pre>
</div>
<div id="loading-matrix" class="section level3">
<h3>Loading matrix</h3>
<pre class="r"><code>(loading_matrix = pca_words_cov$rotation)</code></pre>
<pre><code>                     PC1       PC2
Word_length    0.5368755 0.8436615
Lines_in_dict -0.8436615 0.5368755</code></pre>
</div>
<div id="correlation-score-for-supplementary-variables" class="section level3">
<h3>Correlation score for supplementary variables</h3>
<pre class="r"><code># Supplementary variable (Table 4)
Frequency = c(8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1)
Num_entries = c(6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2)
supp_data = data.frame(Frequency,Num_entries) # Supplementary data
supp_data</code></pre>
<pre><code>   Frequency Num_entries
1          8           6
2        230           3
3        700          12
4          1           2
5        500           7
6          1           1
7          9           1
8          2           6
9          1           1
10       700           5
11         7           2
12         1           1
13         4           5
14       500           9
15       900           7
16         3           1
17         1           1
18        10           4
19         1           4
20         1           2</code></pre>
</div>
<div id="centered-supplementary-data" class="section level3">
<h3>Centered supplementary data</h3>
<pre class="r"><code>supp_data_cent = scale(supp_data,scale = F) # Centered supplementary data</code></pre>
</div>
<div id="correlation-score-for-supplementary-data" class="section level3">
<h3>Correlation score for supplementary data</h3>
<pre class="r"><code>(corr_score_supp = round(cor(pca_words_cov$x,supp_data),4))</code></pre>
<pre><code>    Frequency Num_entries
PC1   -0.3012     -0.6999
PC2   -0.7218     -0.4493</code></pre>
<p>Note that correlation score doesn’t depend on whether supplementary data is centered or not.</p>
<pre class="r"><code>(round(cor(pca_words_cov$x,supp_data_cent),4))</code></pre>
<pre><code>    Frequency Num_entries
PC1   -0.3012     -0.6999
PC2   -0.7218     -0.4493</code></pre>
</div>
<div id="squared-correlation-1" class="section level3">
<h3>Squared correlation</h3>
<pre class="r"><code>(round(cor(pca_words_cov$x,supp_data_cent)^2,4))</code></pre>
<pre><code>    Frequency Num_entries
PC1    0.0907      0.4899
PC2    0.5210      0.2019</code></pre>
</div>
<div id="column-sums-of-squared-correlation-for-support-data" class="section level3">
<h3>Column sums of squared correlation for support data</h3>
<pre class="r"><code>(round(colSums(cor(pca_words_cov$x,supp_data_cent)^2),4))</code></pre>
<pre><code>  Frequency Num_entries 
     0.6118      0.6918 </code></pre>
</div>
<div id="correlation-circle-plot" class="section level3">
<h3>Correlation circle plot</h3>
<pre class="r"><code># First plot correlation circle
x = seq(0,2*pi,length.out = 300)
circle = ggplot() + geom_path(data = data.frame(a = cos(x),b = sin(x)),
                     aes(cos(x),sin(x)),alpha = 0.3, size = 1.5)+
            geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  annotate(&quot;text&quot;,x = c(1.08,0.05),y = c(0.05,1.08),label = c(&quot;PC1&quot;,&quot;PC2&quot;),angle = c(0,90))+
            xlab(NULL)+ylab(NULL)
# Plotting original variables
cor_score = as.data.frame(cor(words_centered,pca_words_cov$x))
variable_plot_original = circle + geom_point(data = cor_score,  aes(cor_score[,1],cor_score[,2]))+
  geom_text_repel(aes(cor_score[,1],cor_score[,2],
                      label = c(&quot;Length of words&quot;,&quot;Number of lines in Dict.&quot;))) 
print(variable_plot_original)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="plotting-supplementary-variables" class="section level3">
<h3>Plotting supplementary variables</h3>
<pre class="r"><code>variable_plot_original+
  geom_point(data = as.data.frame(corr_score_supp),
             aes(corr_score_supp[,1],corr_score_supp[,2]))+
  geom_text_repel(aes(corr_score_supp[,1],corr_score_supp[,2],
                      label = c(&quot;Frequency&quot;,&quot;Number of entries&quot;))) </code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Observe that our correlation circle plot is flipped about y-axis (i.e., PC2) when compared to the plot given in paper. This is because our first principal component is negative of the one given in paper. So while computing correlation score, this negative principal component results in negative correlation scores. Hence, our plot flips about y-axis.</p>
</div>
<div id="example-2-wine-example" class="section level2">
<h2>Example 2 (Wine example)</h2>
<div id="correlation-pca-with-wine-data" class="section level3">
<h3>Correlation PCA with wine data</h3>
<pre class="r"><code># Table 6
wine_type = c(paste(&quot;wine&quot;, 1:5, sep = &quot;_&quot;))
hedonic = c(14, 10, 8, 2, 6)
for_meat = c(7, 7, 5, 4, 2)
for_dessert = c(8, 6, 5, 7, 4)
price = c(7, 4, 10, 16, 13)
sugar = c(7, 3, 5, 7, 3)
alcohol = c(13, 14, 12, 11, 10)
acidity = c(7, 7, 5, 3, 3)
wine = data.frame(wine_type, hedonic, for_meat, for_dessert, price, sugar, alcohol, acidity, stringsAsFactors = F)
wine</code></pre>
<pre><code>  wine_type hedonic for_meat for_dessert price sugar alcohol acidity
1    wine_1      14        7           8     7     7      13       7
2    wine_2      10        7           6     4     3      14       7
3    wine_3       8        5           5    10     5      12       5
4    wine_4       2        4           7    16     7      11       3
5    wine_5       6        2           4    13     3      10       3</code></pre>
<pre class="r"><code>pca_wine_cor = prcomp(wine[2:8],scale = T)
ggplot(as.data.frame(pca_wine_cor$x),aes(x = pca_wine_cor$x[,1],y =  pca_wine_cor$x[,2], label = paste0(&quot;wine &quot;,1:5)))+
  geom_point()+geom_text_repel()+ geom_vline(xintercept = 0)+ geom_hline(yintercept = 0)+
  xlab(&quot;Factor score along PC1&quot;)+ylab(&quot;Factor score along PC2&quot;)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Again our figure seems upside down than that of the paper. This is a minor discrepancy. Our 2nd eigenvector is negative of the one considered in paper. We can match the plot with that of the paper by just flipping the second principal component but we will not do that here.</p>
</div>
<div id="factor-scores-along-1st-and-2nd-pc" class="section level3">
<h3>Factor scores along 1st and 2nd PC</h3>
<pre class="r"><code># Table 7
(pca_wine_cor$x[,1:2])</code></pre>
<pre><code>            PC1       PC2
[1,] -2.3301649  1.095284
[2,] -2.0842419 -1.223185
[3,]  0.1673228 -0.370258
[4,]  1.7842392  1.712563
[5,]  2.4628448 -1.214405</code></pre>
</div>
<div id="contribution-of-each-observation-to-principal-component" class="section level3">
<h3>Contribution of each observation to principal component</h3>
<pre class="r"><code>round(pca_wine_cor$x[,1:2]^2/matrix(rep(colSums(pca_wine_cor$x[,1:2]^2),nrow(wine)),ncol = 2,byrow = T)*100,2)</code></pre>
<pre><code>       PC1   PC2
[1,] 28.50 16.57
[2,] 22.80 20.66
[3,]  0.15  1.89
[4,] 16.71 40.51
[5,] 31.84 20.37</code></pre>
</div>
<div id="squared-cosine-of-observations-of-first-pc" class="section level3">
<h3>Squared cosine of observations of first PC</h3>
<pre class="r"><code>(sq_cos = round(pca_wine_cor$x[,1:2]^2/rowSums(pca_wine_cor$x^2)*100))</code></pre>
<pre><code>     PC1 PC2
[1,]  77  17
[2,]  69  24
[3,]   7  34
[4,]  50  46
[5,]  78  19</code></pre>
</div>
<div id="loading-scores-corresponding-to-first-two-principal-components" class="section level3">
<h3>Loading scores corresponding to first two principal components</h3>
<pre class="r"><code>(round(pca_wine_cor$rotation[,1:2],2))</code></pre>
<pre><code>              PC1   PC2
hedonic     -0.40 -0.11
for_meat    -0.45  0.11
for_dessert -0.26  0.59
price        0.42  0.31
sugar       -0.05  0.72
alcohol     -0.44 -0.06
acidity     -0.45 -0.09</code></pre>
</div>
<div id="correlation-score-variables-with-first-two-principal-components" class="section level3">
<h3>Correlation score variables with first two principal components</h3>
<pre class="r"><code>(corr_score_wine = round(cor(pca_wine_cor$x,wine[,2:8])[1:2,],2))</code></pre>
<pre><code>    hedonic for_meat for_dessert price sugar alcohol acidity
PC1   -0.87    -0.97       -0.58  0.91 -0.11   -0.96   -0.99
PC2   -0.15     0.15        0.79  0.42  0.97   -0.07   -0.12</code></pre>
</div>
<div id="correlation-circle-for-wine-data" class="section level3">
<h3>Correlation circle for wine data</h3>
<pre class="r"><code># Figure 6
corr_score_wine = t(corr_score_wine)
circle + 
  geom_point(data = as.data.frame(corr_score_wine),
             aes(corr_score_wine[,1],corr_score_wine[,2]))+
  geom_text_repel(aes(corr_score_wine[,1],corr_score_wine[,2],
                      label = c(&quot;Hedonic&quot;,&quot;For Meat&quot;,&quot;For dessert&quot;,&quot;Price&quot;,&quot;Sugar&quot;,&quot;Alcohol&quot;,&quot;Acidity&quot;)))</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="varimax-rotation" class="section level3">
<h3>Varimax rotation</h3>
<p>Rotation is applied to loading matrix such that after rotation principal components are interpretable. By interpretable, we mean, some of the loading scores will have higher values and some other loading scores will have lower values. So it can be said that the variables whose loading scores have higher value, contribute significantly towards principal components as compared to other variables with lesser loading scores. Though rotation works in certain cases, it must be remembered that it is no magic wand for principal component interpretability. One of the popular rotations is Varimax rotation. R has a built-in command to perform varimax rotation.</p>
<p>Varimax rotation can be performed on the whole loading matrix or on a few components only. In the paper, varimax has been applied to first two principal components.</p>
</div>
<div id="loading-scores-of-first-two-principal-components" class="section level3">
<h3>Loading scores of first two principal components</h3>
<pre class="r"><code>(round(pca_wine_cor$rotation[,1:2],2))</code></pre>
<pre><code>              PC1   PC2
hedonic     -0.40 -0.11
for_meat    -0.45  0.11
for_dessert -0.26  0.59
price        0.42  0.31
sugar       -0.05  0.72
alcohol     -0.44 -0.06
acidity     -0.45 -0.09</code></pre>
</div>
<div id="varimax-applied-to-first-two-principal-components" class="section level3">
<h3>Varimax applied to first two principal components</h3>
<pre class="r"><code>rotated_loading_scores = varimax(pca_wine_cor$rotation[,1:2])</code></pre>
</div>
<div id="loading-scores-after-rotation" class="section level3">
<h3>Loading scores after rotation</h3>
<pre class="r"><code># Table 10
(round(rotated_loading_scores$loadings[,1:2],2))</code></pre>
<pre><code>              PC1   PC2
hedonic     -0.41 -0.02
for_meat    -0.41  0.21
for_dessert -0.12  0.63
price        0.48  0.21
sugar        0.12  0.72
alcohol     -0.44  0.05
acidity     -0.46  0.02</code></pre>
<p>The same result can also be obtained by multiplying the original loading matrix by the rotation matrix obtained from varimax.</p>
<pre class="r"><code>(round(pca_wine_cor$rotation[,1:2] %*% rotated_loading_scores$rotmat,2))</code></pre>
<pre><code>             [,1]  [,2]
hedonic     -0.41 -0.02
for_meat    -0.41  0.21
for_dessert -0.12  0.63
price        0.48  0.21
sugar        0.12  0.72
alcohol     -0.44  0.05
acidity     -0.46  0.02</code></pre>
</div>
<div id="plot-of-loading-scores-before-rotation" class="section level3">
<h3>Plot of loading scores before rotation</h3>
<pre class="r"><code>#Figure 7
ggplot(as.data.frame(pca_wine_cor$rotation[,1:2]),aes(x = pca_wine_cor$rotation[,1],y = pca_wine_cor$rotation[,2],
                                                      label = c(&quot;Hedonic&quot;,&quot;For Meat&quot;,&quot;For dessert&quot;,&quot;Price&quot;,&quot;Sugar&quot;,&quot;Alcohol&quot;,&quot;Acidity&quot;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&quot;Loading score along PC1&quot;)+ylab(&quot;Loading score along PC2&quot;)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
<div id="plot-of-loading-scores-after-rotation" class="section level3">
<h3>Plot of loading scores after rotation</h3>
<pre class="r"><code>ggplot(as.data.frame(rotated_loading_scores$loadings[,1:2]),
                     aes(x = rotated_loading_scores$loadings[,1],
                         y = rotated_loading_scores$loadings[,2],
                         label = c(&quot;Hedonic&quot;,&quot;For Meat&quot;,&quot;For dessert&quot;,&quot;Price&quot;,&quot;Sugar&quot;,&quot;Alcohol&quot;,&quot;Acidity&quot;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
    xlab(&quot;Loading score along PC1 after rotation&quot;)+
    ylab(&quot;Loading score along PC2 after rotation&quot;)</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="example-3" class="section level2">
<h2>Example 3</h2>
<div id="french-food-example-covariance-pca-example" class="section level3">
<h3>French food example (Covariance PCA example)</h3>
<pre class="r"><code># Table 11 
class = rep(c(&quot;Blue_collar&quot;, &quot;White_collar&quot;, &quot;Upper_class&quot;), times = 4)
children = rep(c(2,3,4,5), each = 3)
bread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)
vegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)
fruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)
meat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)
poultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)
milk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)
wine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)
food = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)
food</code></pre>
<pre><code>          class children bread vegetables fruit meat poultry milk wine
1   Blue_collar        2   332        428   354 1437     526  247  427
2  White_collar        2   293        559   388 1527     567  239  258
3   Upper_class        2   372        767   562 1948     927  235  433
4   Blue_collar        3   406        563   341 1507     544  324  407
5  White_collar        3   386        608   396 1501     558  319  363
6   Upper_class        3   438        843   689 2345    1148  243  341
7   Blue_collar        4   534        660   367 1620     638  414  407
8  White_collar        4   460        699   484 1856     762  400  416
9   Upper_class        4   385        789   621 2366    1149  304  282
10  Blue_collar        5   655        776   423 1848     759  495  486
11 White_collar        5   584        995   548 2056     893  518  319
12  Upper_class        5   515       1097   887 2630    1167  561  284</code></pre>
<pre class="r"><code>pca_food_cov = prcomp(food[,3:9],scale = F)</code></pre>
</div>
<div id="factor-scores" class="section level3">
<h3>Factor scores</h3>
<pre class="r"><code># Table 12
(factor_scores_food = round(pca_food_cov$x[,1:2],2))</code></pre>
<pre><code>          PC1     PC2
 [1,] -635.05  120.89
 [2,] -488.56  142.33
 [3,]  112.03  139.75
 [4,] -520.01  -12.05
 [5,] -485.94   -1.17
 [6,]  588.17  188.44
 [7,] -333.95 -144.54
 [8,]  -57.51  -42.86
 [9,]  571.32  206.76
[10,]  -39.38 -264.47
[11,]  296.04 -235.92
[12,]  992.83  -97.15</code></pre>
</div>
<div id="contribution-of-each-observation-to-principal-component-1" class="section level3">
<h3>Contribution of each observation to principal component</h3>
<pre class="r"><code>round(pca_food_cov$x[,1:2]^2/matrix(rep(colSums(pca_food_cov$x[,1:2]^2),nrow(food)),ncol = 2,byrow = T)*100,2)</code></pre>
<pre><code>        PC1   PC2
 [1,] 13.34  5.03
 [2,]  7.90  6.97
 [3,]  0.42  6.72
 [4,]  8.94  0.05
 [5,]  7.81  0.00
 [6,] 11.44 12.22
 [7,]  3.69  7.19
 [8,]  0.11  0.63
 [9,] 10.80 14.71
[10,]  0.05 24.07
[11,]  2.90 19.15
[12,] 32.61  3.25</code></pre>
<pre class="r"><code>dist = pca_food_cov$x[,1]^2+pca_food_cov$x[,2]^2</code></pre>
</div>
<div id="squared-cosine-of-observations-1" class="section level3">
<h3>Squared cosine of observations</h3>
<pre class="r"><code>(sq_cos = round(pca_food_cov$x[,1:2]^2/rowSums(pca_food_cov$x^2)*100))</code></pre>
<pre><code>      PC1 PC2
 [1,]  95   3
 [2,]  86   7
 [3,]  26  40
 [4,] 100   0
 [5,]  98   0
 [6,]  89   9
 [7,]  83  15
 [8,]  40  22
 [9,]  86  11
[10,]   2  79
[11,]  57  36
[12,]  97   1</code></pre>
</div>
<div id="squared-loading-scores" class="section level3">
<h3>Squared loading scores</h3>
<pre class="r"><code># Table 13
(round(pca_food_cov$rotation[,1:2]^2,2))</code></pre>
<pre><code>            PC1  PC2
bread      0.01 0.33
vegetables 0.11 0.17
fruit      0.09 0.01
meat       0.57 0.01
poultry    0.22 0.06
milk       0.01 0.40
wine       0.00 0.02</code></pre>
<p><strong>Note</strong> that this table doesn’t match with that of the paper. We will stick to our analysis.</p>
</div>
<div id="correlation-score" class="section level3">
<h3>Correlation score</h3>
<pre class="r"><code>(corr_score_food = round((cor(pca_food_cov$x,food[,3:9])[1:2,]),2))</code></pre>
<pre><code>    bread vegetables fruit meat poultry  milk  wine
PC1  0.36       0.91  0.96 1.00    0.98  0.41 -0.43
PC2 -0.87      -0.35  0.10 0.04    0.16 -0.88 -0.33</code></pre>
</div>
<div id="squared-correlation-score" class="section level3">
<h3>Squared correlation score</h3>
<pre class="r"><code>(round((cor(pca_food_cov$x,food[,3:9])[1:2,])^2,2))</code></pre>
<pre><code>    bread vegetables fruit meat poultry milk wine
PC1  0.13       0.83  0.92    1    0.96 0.17 0.18
PC2  0.76       0.12  0.01    0    0.03 0.77 0.11</code></pre>
</div>
<div id="correlation-circle-for-food-data" class="section level3">
<h3>Correlation circle for food data</h3>
<pre class="r"><code># Figure 9
corr_score_food = t(corr_score_food)
circle + geom_point(data = as.data.frame(corr_score_food), 
                    aes(x = corr_score_food[,1],y = corr_score_food[,2]))+
  geom_text_repel(data = as.data.frame(corr_score_food), 
                  aes(x = corr_score_food[,1],y = corr_score_food[,2],
                      label = c(&quot;Bread&quot;,&quot;Vegetables&quot;,&quot;Fruit&quot;,&quot;Meat&quot;,&quot;Poultry&quot;,&quot;Milk&quot;,&quot;Wine&quot;)))</code></pre>
<p><img src="/post/2019-02-03-principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-47-1.png" width="672" />
Now observe that our correlation circle plot is almost close to that of the papers (though in opposite quadrants. But this is not a problem as we have previously mentioned).</p>
</div>
<div id="eigenvalues" class="section level3">
<h3>Eigenvalues</h3>
<p>Eigenvalues of data covariance matrix is square of singular values of centered data matrix. Hence eigenvalues of data covariance matrix can be obtained as below.</p>
<pre class="r"><code>## Table 14
cent_food = food[,3:9]-matrix(rep(colMeans(food[,3:9]),times = 12),nrow = 12,
                              byrow = T)
svd_food = svd(cent_food)
(Eigenvalues = (svd_food$d)^2)</code></pre>
<pre><code>[1] 3023141.2354  290575.8390   68795.2333   25298.9496   22992.2474
[6]    3722.3214     723.9238</code></pre>
<p><strong>Important Note:</strong> These eigenvalues are not the same as variance of factor scores in principal components. Variance of principal component factor scores can be obtained by dividing the eigenvalues by <span class="math inline">\((n-1)\)</span>, where <span class="math inline">\(n\)</span> is number of data points (in this case <span class="math inline">\(n = 12\)</span>). If this point is still not clear, refer to <a href="https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/">Part-II</a>.</p>
</div>
<div id="percentage-contribution-of-each-principal-component" class="section level3">
<h3>Percentage contribution of each principal component</h3>
<pre class="r"><code>(round(Eigenvalues/sum(Eigenvalues),2))</code></pre>
<pre><code>[1] 0.88 0.08 0.02 0.01 0.01 0.00 0.00</code></pre>
</div>
<div id="cumulative-sum-of-eigenvalues" class="section level3">
<h3>Cumulative sum of eigenvalues</h3>
<pre class="r"><code>(round(cumsum(Eigenvalues),2))</code></pre>
<pre><code>[1] 3023141 3313717 3382512 3407811 3430804 3434526 3435250</code></pre>
</div>
<div id="cumulative-percentage-contribution" class="section level3">
<h3>Cumulative percentage contribution</h3>
<pre class="r"><code>(round(cumsum(Eigenvalues)/sum(Eigenvalues),2))</code></pre>
<pre><code>[1] 0.88 0.96 0.98 0.99 1.00 1.00 1.00</code></pre>
</div>
<div id="ress-refer-to-the-paper-for-a-description" class="section level3">
<h3>RESS (Refer to the paper for a description)</h3>
<pre class="r"><code>RESS = array(rep(0,7))
for (i in 1:7){
  RESS[i] = sum(Eigenvalues)-sum(Eigenvalues[1:i])
}
RESS</code></pre>
<pre><code>[1] 412108.5146 121532.6756  52737.4423  27438.4927   4446.2453    723.9238
[7]      0.0000</code></pre>
</div>
<div id="ratio-of-ress-and-sum-of-eigenvalues" class="section level3">
<h3>Ratio of RESS and sum of eigenvalues</h3>
<pre class="r"><code>round(RESS/sum(Eigenvalues),2)</code></pre>
<pre><code>[1] 0.12 0.04 0.02 0.01 0.00 0.00 0.00</code></pre>
<p>We will not calculate the value of PRESS in this post as it requires us to consider random models. We will not pursue that here.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.6.2 (2019-12-12)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggrepel_0.8.1 ggplot2_3.2.1

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.3       knitr_1.27       magrittr_1.5     tidyselect_0.2.5
 [5] munsell_0.5.0    colorspace_1.4-1 R6_2.4.1         rlang_0.4.2     
 [9] dplyr_0.8.3      stringr_1.4.0    tools_3.6.2      grid_3.6.2      
[13] gtable_0.3.0     xfun_0.12        withr_2.1.2      htmltools_0.4.0 
[17] assertthat_0.2.1 yaml_2.2.0       lazyeval_0.2.2   digest_0.6.23   
[21] tibble_2.1.3     lifecycle_0.1.0  crayon_1.3.4     bookdown_0.17   
[25] farver_2.0.1     purrr_0.3.3      glue_1.3.1       evaluate_0.14   
[29] rmarkdown_2.1    blogdown_0.17.1  labeling_0.3     stringi_1.4.5   
[33] compiler_3.6.2   pillar_1.4.2     scales_1.1.0     pkgconfig_2.0.3 </code></pre>
<p>Though unusually long, I hope, this post will be of help to (courageous) readers who work there way through it till end. Comments regarding any errors or omissions may be sent to <a href="https://biswajitsahoo1111.github.io/">the author’s</a> email.</p>
<p><a href="https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_3.Rmd">R Markdown file for this post</a></p>
</div>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<ol style="list-style-type: decimal">
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.</li>
</ol>
<p>Last updated: 19th January, 2020</p>
</div>

    </div>

    



<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/pca/">PCA</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/principal-component-analysis-part-iii/&amp;text=Principal%20Component%20Analysis%20-%20Part%20III" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/principal-component-analysis-part-iii/&amp;t=Principal%20Component%20Analysis%20-%20Part%20III" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Principal%20Component%20Analysis%20-%20Part%20III&amp;body=/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/principal-component-analysis-part-iii/&amp;title=Principal%20Component%20Analysis%20-%20Part%20III" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Principal%20Component%20Analysis%20-%20Part%20III%20/post/principal-component-analysis-part-iii/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/principal-component-analysis-part-iii/&amp;title=Principal%20Component%20Analysis%20-%20Part%20III" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu43a4d92e507f8fd6150aa45a41e27c0c_293040_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Biswajit Sahoo</a></h5>
      <h6 class="card-subtitle">PhD Student</h6>
      <p class="card-text">My research interests include Machine Learning, Deep Learning, and Machinery Condition Monitoring.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:biswajitsahoo1111@iitkgp.ac.in" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/biswajitsahoo1111" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/biswajitsahoo1111/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/biswajitsahoo11" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@biswajitsahoo1111" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "biswajitsahoo1111" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/principal-component-analysis-part-ii/">Principal Component Analysis - Part II</a></li>
      
      <li><a href="/post/principal-component-analysis-part-i/">Principal Component Analysis - Part I</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    

    
    

    

    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://biswajitsahoo1111.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.c6f9beab6fb83af42b79b98b1b192e24.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © 2020 Biswajit Sahoo &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
