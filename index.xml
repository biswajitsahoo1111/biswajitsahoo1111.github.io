<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Biswajit Sahoo</title>
    <link>https://biswajitsahoo1111.github.io/</link>
      <atom:link href="https://biswajitsahoo1111.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Biswajit Sahoo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Biswajit Sahoo</copyright><lastBuildDate>Thu, 01 Apr 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://biswajitsahoo1111.github.io/img/icon.png</url>
      <title>Biswajit Sahoo</title>
      <link>https://biswajitsahoo1111.github.io/</link>
    </image>
    
    <item>
      <title>IndexedSlices in Tensorflow</title>
      <link>https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Tensorflow_IndexedSlices.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Tensorflow_IndexedSlices.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/paze6mubda9bvv4/Tensorflow_IndexedSlices.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will discuss about &lt;code&gt;IndexedSlices&lt;/code&gt; class of &lt;code&gt;Tensorflow&lt;/code&gt;. We will try to answer the following questions in this blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#q1&#34;&gt;What are &lt;code&gt;IndexedSlices&lt;/code&gt;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q2&#34;&gt;Where do we get it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q3&#34;&gt;How to convert from IndexedSlices to tensors?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id = &#34;q1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;what-are-indexedslices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are &lt;code&gt;IndexedSlices&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;According to &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/IndexedSlices&#34;&gt;&lt;code&gt;Tensorflow&lt;/code&gt; documentation&lt;/a&gt;, &lt;code&gt;IndexedSlices&lt;/code&gt; are sparse representation of a set of tensor slices at a given index. At an high level it appears to be some kind of sparse representation. Let’s try to understand it with examples.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;q2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-do-we-get-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where do we get it?&lt;/h2&gt;
&lt;p&gt;We get &lt;code&gt;IndexedSlices&lt;/code&gt; while taking gradients of an &lt;code&gt;Embedding&lt;/code&gt; layer. Embedding matrices can be huge (depending on vocabulary size). But each batch only contains a small fraction of tokens. So while computing the gradient of loss with respect to embedding layer, in each pass we have to only consider the corresponding token embeedings of the present batch. Naturally a sparse tensor seems to be a better option to record those gradients. &lt;code&gt;Tensorflow&lt;/code&gt; does that using &lt;code&gt;IndexedSlices&lt;/code&gt;. We will show that below using a contrived example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow version: &amp;quot;, tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.models.Sequential([
    # Vocab size: 10, Embedding dimension: 4, Input_shape size: (batch_size, num_words). As usual, batch_size is omitted.
    tf.keras.layers.Embedding(10, 4, input_shape = (5,)),
    tf.keras.layers.Flatten(), 
    tf.keras.layers.Dense(1)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 5, 4)              40        
_________________________________________________________________
flatten (Flatten)            (None, 20)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 21        
=================================================================
Total params: 61
Trainable params: 61
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data = tf.random.uniform(shape = (1, 5), minval = 0, maxval = 10, dtype = tf.int32) # Batch size is 1.
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[6, 1, 1, 4, 8]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.variables  # Is a list of 3 tensors. 1 from Embedding layer and 2 from Dense layer (Kernel and bias)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;tf.Variable &amp;#39;embedding/embeddings:0&amp;#39; shape=(10, 4) dtype=float32, numpy=
 array([[ 4.10897247e-02, -2.48962641e-03,  1.26880072e-02,
          3.39310430e-02],
        [ 3.28579657e-02,  3.90318781e-03,  2.81411521e-02,
          3.09719704e-02],
        [ 1.16247907e-02, -1.41257644e-02, -3.36343870e-02,
         -4.41543460e-02],
        [-4.67238426e-02,  2.42819674e-02, -4.26802635e-02,
         -2.59207971e-02],
        [ 2.28367783e-02, -2.09717881e-02,  1.05572566e-02,
          3.33249308e-02],
        [-3.37148309e-02, -4.61939685e-02, -2.61853095e-02,
         -4.10162285e-03],
        [-3.59787717e-02,  2.78765075e-02, -3.16200405e-02,
          4.54976298e-02],
        [-4.67344411e-02, -1.30221620e-02,  1.52915232e-02,
          2.22466923e-02],
        [-1.03901625e-02,  2.40740217e-02, -1.24427900e-02,
          4.47194651e-03],
        [-3.57637033e-02,  4.28059734e-02, -2.59280205e-05,
          4.09286283e-02]], dtype=float32)&amp;gt;,
 &amp;lt;tf.Variable &amp;#39;dense/kernel:0&amp;#39; shape=(20, 1) dtype=float32, numpy=
 array([[ 0.42870212],
        [ 0.04779923],
        [ 0.4126016 ],
        [-0.13294601],
        [-0.3175783 ],
        [-0.46080017],
        [-0.23412797],
        [ 0.30137837],
        [-0.5197849 ],
        [-0.10935467],
        [ 0.5087845 ],
        [-0.06930307],
        [ 0.10028934],
        [-0.11278141],
        [-0.21269777],
        [-0.0214209 ],
        [ 0.12959635],
        [-0.13330323],
        [-0.23972857],
        [ 0.23718971]], dtype=float32)&amp;gt;,
 &amp;lt;tf.Variable &amp;#39;dense/bias:0&amp;#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)
loss_object = tf.keras.losses.MeanSquaredError()
target = tf.constant([2.5], shape = (1,1))
for _ in range(2):   # Let&amp;#39;s run gradient descent for two batches of the same input data. (It&amp;#39;s a contrived examples)
    with tf.GradientTape() as tape:
        output = model(data) # Output has shape: (batch_size, 1). Here batch_size is 1. So output shape is (1,1)
        loss_value = loss_object(target, output)  # Calculating some random loss.
    grads = tape.gradient(loss_value, model.trainable_variables)

    # Gradient descent step
    optimizer.apply_gradients(zip(grads, model.trainable_variables))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;len(grads)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;grads[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.framework.indexed_slices.IndexedSlices at 0x16c04d4a970&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(grads[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IndexedSlices(indices=tf.Tensor([6 1 1 4 8], shape=(5,), dtype=int32), values=tf.Tensor(
[[-0.9495101  -0.14344962 -0.91739434  0.25398374]
 [ 0.69607455  1.0615798   0.5085497  -0.7338184 ]
 [ 1.1639317   0.24842012 -1.2103697   0.12384857]
 [-0.25895947  0.2856651   0.47968888  0.01028775]
 [-0.28760925  0.28005898  0.56933826 -0.5540699 ]], shape=(5, 4), dtype=float32), dense_shape=tf.Tensor([10  4], shape=(2,), dtype=int32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;code&gt;IndexedSlices&lt;/code&gt; object has 3 main entries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;indices&lt;/li&gt;
&lt;li&gt;values, and&lt;/li&gt;
&lt;li&gt;dense_shape&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id = &#34;q3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-convert-indexedslices-to-tensors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to convert &lt;code&gt;IndexedSlices&lt;/code&gt; to &lt;code&gt;Tensors&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;Before we do the conversion, let’s answer a relevant question: Why do we have to do the conversion from &lt;code&gt;IndexedSlices&lt;/code&gt; to tensors given that &lt;code&gt;Tensorflow&lt;/code&gt; can do a gradient descent step automatically through the &lt;code&gt;IndexedSlices&lt;/code&gt;? In the last section, we could run 2 gradient descent steps without worrying about &lt;code&gt;IndexedSlices&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But the problem occurs if we want to do some processing on gradient values. One such processing is &lt;code&gt;gradient clipping&lt;/code&gt;. In &lt;code&gt;gradient clipping&lt;/code&gt;, if sum of norm of gradients exceed a given value, gradients are rescaled to decrease their magnitude. Therefore, to do any gradient clipping, we have to access the gradient tensors. This is precisely where we would like to convert IndexedSlices to tensors. Having an embedding layer is common in deep learning models and applying gradient clipping to gradient values is also a common practice. We will show two approaches to do the conversion.&lt;/p&gt;
&lt;div id=&#34;easiest-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Easiest approach&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.convert_to_tensor(grads[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(10, 4), dtype=float32, numpy=
array([[ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 1.8600063 ,  1.31      , -0.70182   , -0.60996985],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.25895947,  0.2856651 ,  0.47968888,  0.01028775],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.9495101 , -0.14344962, -0.91739434,  0.25398374],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.28760925,  0.28005898,  0.56933826, -0.5540699 ],
       [ 0.        ,  0.        ,  0.        ,  0.        ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-did-just-happen-in-the-last-step&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What did just happen in the last step?&lt;/h3&gt;
&lt;p&gt;Though the last approach is a single line elegant solution, it hides many things. How actually is the conversion done? The code below shows the steps in which we can manually do the conversion.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_grad = tf.zeros_like(model.variables[0]).numpy()   # Create a dense tensor of all zeros
for i, ind in enumerate(grads[0].indices):
    check_grad[ind] = check_grad[ind] + grads[0].values[i]
check_grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 1.8600063 ,  1.31      , -0.70182   , -0.60996985],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.25895947,  0.2856651 ,  0.47968888,  0.01028775],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.9495101 , -0.14344962, -0.91739434,  0.25398374],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.28760925,  0.28005898,  0.56933826, -0.5540699 ],
       [ 0.        ,  0.        ,  0.        ,  0.        ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This brings us to the end of this blog. I hope this blog has demystified a few things about &lt;code&gt;IndexedSlices&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation for this post&lt;/strong&gt;: While writing TF 2 code for &lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Attention Mechanisms chapter of D2L book&lt;/a&gt;, the author encountered an error involving &lt;code&gt;IndexedSlices&lt;/code&gt;. After spending a good deal of time hopelessly trying to figure out what’s going on, the author finally found that the error was occurring because of an user defined gradient clipping function that didn’t handle &lt;code&gt;IndexedSlices&lt;/code&gt; properly. The model involved embedding layers as it was dealing with machine translation task. Therefore, I thought of writing this blog with the hope that it would be of help to readers who are struggling to figure out what &lt;code&gt;IndexedSlices&lt;/code&gt; are.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book</title>
      <link>https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;
&lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;amp;repo=D2L_Attention_Mechanisms_in_TF&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;
&lt;/iframe&gt;
&lt;/td&gt;
&lt;!-----
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  -----&gt;
&lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
&lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download code (.zip)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;amp;repo=D2L_Attention_Mechanisms_in_TF&amp;amp;type=fork&amp;amp;count=true&amp;amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;
&lt;/iframe&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This post contains &lt;code&gt;Tensorflow 2&lt;/code&gt; code for Attention Mechanisms chapter of &lt;a href=&#34;http://d2l.ai/&#34;&gt;Dive into Deep Learning (D2L)&lt;/a&gt; book. The chapter has 7 sections and code for each section can be found at the following links. We have given only code implementations. For theory, readers should refer the book.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_1_Visualization_of_attention.ipynb&#34;&gt;10.1. Attention Cues&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_2_Attention_based_regression.ipynb&#34;&gt;10.2. Attention Pooling: Nadaraya-Watson Kernel Regression&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_3_Attention_scoring_functions.ipynb&#34;&gt;10.3. Attention Scoring Functions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_4_Bahdanau_attention.ipynb&#34;&gt;10.4. Bahdanau Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_5_Multi-head_attention.ipynb&#34;&gt;10.5. Multi-Head Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_6_Self-attention_and_positional_encoding.ipynb&#34;&gt;10.6. Self-Attention and Positional Encoding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_7_Transformer.ipynb&#34;&gt;10.7. Transformer&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;additional-sections&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional sections:&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/additional_sections/9_7_Sequence_to_Sequence_Learning.ipynb&#34;&gt;9.7. Sequence to Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-run-these-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to run these code:&lt;/h3&gt;
&lt;p&gt;The best way (in our opinion) is to either clone the repo (or download the zipped repo) and then run each notebook from the cloned (or extracted) folder. All the notebooks will run without any issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We claim no originality for the code. Credit goes to the authors of this excellent &lt;a href=&#34;http://d2l.ai/&#34;&gt;book&lt;/a&gt;. However, all errors and omissions are my own and readers are encouraged to bring it to my notice. Finally, no TF code was available (to the best of my knowledge) for &lt;code&gt;Attention Mechanisms&lt;/code&gt; chapter when &lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;this repo&lt;/a&gt; was first made public.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple files in Tensorflow 2 using Sequence</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Reading_mulitple_files_using_tensorflow_sequence.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Reading_mulitple_files_using_tensorflow_sequence.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/weog8ubsu9qcugv/Reading_mulitple_files_using_tensorflow_sequence.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will read multiple csv files using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;Tensroflow Sequence&lt;/a&gt;. In an &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;earlier post&lt;/a&gt; we had demonstrated the procedure for reading multiple csv files using a custom generator. Though generators are convenient for handling chunks of data from a large dataset, they have limited portability and scalability (&lt;a href=&#34;https://www.tensorflow.org/guide/data#consuming_python_generators&#34;&gt;see the caution here&lt;/a&gt;). Therefore, Tensorflow prefers &lt;code&gt;Sequence&lt;/code&gt; over generators.&lt;/p&gt;
&lt;p&gt;Sequence is similar to &lt;code&gt;Tensorflow Dataset&lt;/code&gt; but provides flexibility for batched data preparation in a custom manner. &lt;code&gt;Tensorflow Dataset&lt;/code&gt; provides a wide range of functionalities to handle different data types. But for some specific applications we might have to make some custom modifications for which built-in methods are not available in &lt;code&gt;tensorflow dataset&lt;/code&gt;. In that case, we can use &lt;code&gt;Sequence&lt;/code&gt; to create our own dataset equivalent. In this post, we will show how to use sequence to read multiple csv files. The method we will discuss is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 .csv files. But the method can be easily extended to huge datasets involving thousands of csv files.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. Tensorflow is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#sec_1&#34;&gt;Create 500 “.csv” files and save it in the folder “random_data” in current directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_2&#34;&gt;Write a sequence object that reads data from the folder in chunks and preprocesses it.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_3&#34;&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_4&#34;&gt;Make prediction on new data for which labels are not known.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id=&#34;sec_1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 .csv files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each .csv file will have one column of data with 1024 entries.&lt;/li&gt;
&lt;li&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …,“Fault_1_100.csv”. Similarly for other classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...

[&amp;#39;./random_data\\Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data\Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, create a custom &lt;code&gt;Sequence&lt;/code&gt; object, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-custom-sequence-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a custom Sequence object&lt;/h2&gt;
&lt;p&gt;According to &lt;code&gt;Tensorflow&lt;/code&gt; &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;documentation&lt;/a&gt;, every Sequence must implement following two methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__getitem__&lt;/code&gt; method: Iterates over the dataset chunk by chunk. It must return a complete batch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__len__&lt;/code&gt; method: Returns the length of the Sequence (total number of batches that we can extract from the full dataset).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we create a custom sequence object, we have complete control over the arguments we want to pass it. The following sequence object takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re    # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow version: &amp;quot;, tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class CustomSequence(tf.keras.utils.Sequence):  # It inherits from `tf.keras.utils.Sequence` class
  def __init__(self, filenames, batch_size):  # Two input arguments to the class.
        self.filenames= filenames
        self.batch_size = batch_size

  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))

  def __getitem__(self, idx):  # idx is index that runs from 0 to length of sequence
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size] # Select a chunk of file names
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]

        for file in batch_x:   # In this loop read the files in the chunk that was selected previously
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
            data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
            pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
            for j in range(len(label_classes)):
                if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                    labels.append(j)  
        data = np.asarray(data).reshape(-1,32,32,1)
        labels = np.asarray(labels)
        return data, labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the &lt;code&gt;__getitem__&lt;/code&gt; method change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the &lt;code&gt;__getitem__&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Now we will check whether the dataset works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence = CustomSequence(filenames = files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence.__len__()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;len(list(sequence))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sequence is not an infinite loop. Let’s check again. This is yet another way to check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;counter = 0
for _,_ in sequence:
    counter = counter + 1
print(counter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for num, (data, labels) in enumerate(sequence):
    print(data.shape, labels.shape)
    print(labels)
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;We can pass this sequence directly to &lt;code&gt;model.fit()&lt;/code&gt; to train our deep learning model. Now that sequence works fine, we will use it to train a simple deep learning model. The focus of this post is not on the model itself. So we will use a simplest model. If readers want a different model, they can do so by just replacing our model with theirs.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feed-the-chunks-of-data-to-a-cnn-model-and-train-it-for-several-epochs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Feed the chunks of data to a CNN model and train it for several epochs&lt;/h2&gt;
&lt;p&gt;But before we build the model and train it, we will first move our files to different folders depending on their fault type. We do so as it will be convenient later to create a training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders one each for a given fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data\\Fault_1&amp;#39;,
 &amp;#39;./random_data\\Fault_2&amp;#39;,
 &amp;#39;./random_data\\Fault_3&amp;#39;,
 &amp;#39;./random_data\\Fault_4&amp;#39;,
 &amp;#39;./random_data\\Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1\\Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3\\Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle training files (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create sequences for training, validation, and test set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_sequence = CustomSequence(filenames = train_file_names, batch_size = batch_size)
val_sequence = CustomSequence(filenames = validation_file_names, batch_size = batch_size)
test_sequence = CustomSequence(filenames = test_file_names, batch_size = batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers
model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit the model using sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_sequence, validation_data = val_sequence, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
35/35 [==============================] - 2s 38ms/step - loss: 1.6305 - accuracy: 0.1788 - val_loss: 1.6095 - val_accuracy: 0.1800
Epoch 2/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6086 - accuracy: 0.2212 - val_loss: 1.6093 - val_accuracy: 0.2200
Epoch 3/10
35/35 [==============================] - 1s 16ms/step - loss: 1.6089 - accuracy: 0.2057 - val_loss: 1.6094 - val_accuracy: 0.2200
Epoch 4/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6092 - accuracy: 0.1987 - val_loss: 1.6100 - val_accuracy: 0.2000
Epoch 5/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6105 - accuracy: 0.1173 - val_loss: 1.6095 - val_accuracy: 0.2000
Epoch 6/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6069 - accuracy: 0.2062 - val_loss: 1.6098 - val_accuracy: 0.2000
Epoch 7/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6070 - accuracy: 0.2332 - val_loss: 1.6097 - val_accuracy: 0.2200
Epoch 8/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6067 - accuracy: 0.2255 - val_loss: 1.6100 - val_accuracy: 0.2200
Epoch 9/10
35/35 [==============================] - 1s 19ms/step - loss: 1.6044 - accuracy: 0.2510 - val_loss: 1.6090 - val_accuracy: 0.2400
Epoch 10/10
35/35 [==============================] - 1s 20ms/step - loss: 1.5998 - accuracy: 0.3180 - val_loss: 1.6094 - val_accuracy: 0.2400





&amp;lt;tensorflow.python.keras.callbacks.History at 0x24f86841b80&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_sequence, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 14ms/step - loss: 1.6097 - accuracy: 0.2000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.609706997871399
Test accuracy: 0.20000000298023224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_4&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But oftentimes, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just make prediction on this data by calling &lt;code&gt;model.evaluate()&lt;/code&gt; and then use np.argmax() to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a separate &lt;code&gt;Sequence&lt;/code&gt; to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...

[&amp;#39;./random_data/prediction_set\\file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prediction sequence will be slightly different from our previous custom dataset class. We only need to return data in this case.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class PredictionSequence(tf.keras.utils.Sequence):
  def __init__(self, filenames, batch_size):
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))
  def __getitem__(self, idx):
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) 
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,32,32,1)
        return data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator sequence works or not. First we will check its length.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_seq = PredictionSequence(filenames = prediction_files, batch_size=10)
print(prediction_seq.__len__())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data in prediction_seq:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(5, 32, 32, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;predictions = model.predict(prediction_seq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)

array([[0.1673972 , 0.24178548, 0.21493195, 0.14968236, 0.22620301],
       [0.17025462, 0.23611873, 0.21584883, 0.15583281, 0.22194503],
       [0.16997598, 0.23636353, 0.2160589 , 0.15556051, 0.22204101],
       [0.17808239, 0.23929793, 0.20060928, 0.15217312, 0.22983731],
       [0.16749388, 0.24041025, 0.21612632, 0.15114665, 0.22482291],
       [0.16648369, 0.24155213, 0.21663304, 0.14991023, 0.22542089],
       [0.16956756, 0.24306948, 0.21019061, 0.14832966, 0.2288426 ],
       [0.16461462, 0.23314428, 0.22771336, 0.15880677, 0.21572094],
       [0.16099085, 0.24614859, 0.22101355, 0.14492905, 0.22691788],
       [0.16029194, 0.23579895, 0.2323411 , 0.15559815, 0.21596995],
       [0.15644614, 0.24322459, 0.23156258, 0.14755438, 0.22121225],
       [0.16045825, 0.23727809, 0.23065342, 0.1540593 , 0.2175509 ],
       [0.18453611, 0.2363899 , 0.19367108, 0.1549771 , 0.23042585],
       [0.16539477, 0.23163731, 0.22783667, 0.16050129, 0.21463   ],
       [0.16019677, 0.2387398 , 0.22967993, 0.15250796, 0.21887548],
       [0.164585  , 0.241522  , 0.2197408 , 0.14987943, 0.22427279],
       [0.1423634 , 0.2492775 , 0.25001773, 0.13962528, 0.21871613],
       [0.1685916 , 0.23507524, 0.21948369, 0.15692794, 0.21992151],
       [0.1899035 , 0.23893598, 0.18335396, 0.15139207, 0.23641457],
       [0.16065492, 0.2413149 , 0.2264044 , 0.14987133, 0.22175437],
       [0.17529766, 0.24322936, 0.20103565, 0.14799424, 0.23244311],
       [0.16809338, 0.24051304, 0.2150641 , 0.15104856, 0.22528093],
       [0.17161693, 0.23075785, 0.21867743, 0.16179037, 0.21715748],
       [0.16884513, 0.23852658, 0.21579023, 0.15319362, 0.22364433],
       [0.17458686, 0.2428582 , 0.20250764, 0.14843197, 0.23161522],
       [0.1646341 , 0.2297526 , 0.23079726, 0.16249931, 0.21231675],
       [0.15729222, 0.23233345, 0.24062563, 0.15889668, 0.210852  ],
       [0.14776482, 0.23704691, 0.2526417 , 0.15245639, 0.21009028],
       [0.16680573, 0.23577859, 0.22168827, 0.15609236, 0.21963505],
       [0.17260087, 0.24030833, 0.20811835, 0.15126604, 0.22770639],
       [0.13550763, 0.24409433, 0.26797664, 0.1427981 , 0.20962337],
       [0.15663186, 0.23662733, 0.23769669, 0.15432158, 0.21472257],
       [0.1549199 , 0.23210286, 0.24487424, 0.15878738, 0.20931558],
       [0.1547824 , 0.24484529, 0.23276137, 0.14575545, 0.22185549],
       [0.1587394 , 0.23657197, 0.23419838, 0.15462619, 0.21586415],
       [0.16478752, 0.24151964, 0.21941346, 0.14988995, 0.22438939],
       [0.16816008, 0.23736185, 0.21800458, 0.15443383, 0.22203967],
       [0.16076393, 0.24149327, 0.22604792, 0.14969479, 0.22200012],
       [0.17530249, 0.23095778, 0.21273129, 0.16161412, 0.21939443],
       [0.15688972, 0.2358581 , 0.23799387, 0.15515216, 0.21410616],
       [0.16600947, 0.23382592, 0.22481105, 0.15816282, 0.21719065],
       [0.14932008, 0.24364983, 0.24340245, 0.14621353, 0.21741422],
       [0.17864552, 0.22986974, 0.20857714, 0.16281399, 0.22009356],
       [0.1659024 , 0.22970615, 0.2287755 , 0.1626553 , 0.21296065],
       [0.17181188, 0.24148299, 0.2082077 , 0.15000671, 0.22849073],
       [0.17638905, 0.23389527, 0.20834343, 0.15828574, 0.22308654],
       [0.15240492, 0.24294852, 0.23874305, 0.14735675, 0.21854681],
       [0.18557529, 0.2407352 , 0.18793964, 0.14989276, 0.23585702],
       [0.13773988, 0.2396555 , 0.2682667 , 0.14754502, 0.20679286],
       [0.15072912, 0.24018031, 0.2443891 , 0.1498653 , 0.21483612],
       [0.15102535, 0.24379413, 0.24028388, 0.14632827, 0.2185683 ],
       [0.17316325, 0.23940398, 0.20811678, 0.1522415 , 0.22707452],
       [0.14266515, 0.24046816, 0.25842705, 0.14801402, 0.21042569],
       [0.16398384, 0.23465969, 0.22732665, 0.1571278 , 0.21690205],
       [0.16170275, 0.23673837, 0.22910713, 0.15473619, 0.2177155 ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use np.argmax() command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,
       1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1], dtype=int64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that our data are randomly generated. So we should not be surprised by this result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a custom sequence, trained a model using that sequence, and made predictions on new data. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-this-method-efficient-will-it-work-at-a-reasobable-speed-if-we-have-many-complex-preprocessing-steps-to-do-before-training-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is this method efficient? Will it work at a reasobable speed if we have many complex preprocessing steps to do before training the model?&lt;/h2&gt;
&lt;p&gt;In this blog, we have mentioned nothing about the ways to speed up the data loading process. Tensorflow prefers sequences over generators as sequences are a safer way to do multiprocessing (&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;see this&lt;/a&gt;). Sequences can be passed to &lt;code&gt;model.fit()&lt;/code&gt; along with parameters like &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt; (&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&#34;&gt;see this&lt;/a&gt;). To efficiently load data, we have to choose suitable values for &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt;. The next obvious question is: How do we choose suitable values for the parameters for our particular system architecture? The best approach, that this author can suggest, is to try different values and choose the ones that work best for your system architecture. Even when we are using complex preprocessing steps, suitable choice of above parameters will, hopefully, speedup our training process.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple csv files in PyTorch</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-csv-files-in-pytorch/</link>
      <pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-csv-files-in-pytorch/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/qxz8zmctpopulck/Reading_multiple_csv_files_in_PyTorch.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In many engineering applications data are usually stored in CSV (Comma Separated Values) files. In big data applications, it’s not uncommon to obtain thousands of csv files. As the number of files increases, at some point, we can no longer load the whole dataset into computer’s memory. In deep learning applications it is increasingly common to come across datasets that don’t fit in the computer’s memory. In that case, we have to devise a way so as to be able to read chunks of data at a time so that the model can be trained using the whole dataset.&lt;/p&gt;
&lt;p&gt;There are many ways to achieve this objective. In this post, we will adopt an approach that allows us to read csv files in chunks and preprocess those files in whatever way we like. Then we can pass the processed data to train any deep learning model. Though we will use csv files in this post, the method is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 csv files. But the method can be easily extended to huge datasets involving thousands of csv files.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use PyTorch to run our deep learning model. For efficiency in data loading, we will use PyTorch dataloaders.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#sec_1&#34;&gt;Create 500 “.csv” files and save it in the folder “random_data” in current working directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_2&#34;&gt;Create a custom dataloader.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_3&#34;&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_4&#34;&gt;Make prediction on new data for which labels are not known.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id=&#34;sec_1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 .csv files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each .csv file will have one column of data with 1024 entries.&lt;/li&gt;
&lt;li&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...

[&amp;#39;./random_data/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, create a custom dataloader, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-custom-dataloader&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a custom dataloader&lt;/h2&gt;
&lt;p&gt;We have to first create a &lt;code&gt;Dataset&lt;/code&gt; class. Then we can pass the dataset to the dataloader. Every dataset class must implement the &lt;code&gt;__len__&lt;/code&gt; method that determines the length of the dataset and &lt;code&gt;__getitem__&lt;/code&gt; method that iterates over the dataset item by item. In our case, item would mean the processed version of a chunk of data.&lt;/p&gt;
&lt;p&gt;The following dataset class takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re   
import torch
from torch.utils.data import Dataset
print(&amp;quot;PyTorch Version: &amp;quot;, torch.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;PyTorch Version:  1.7.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class CustomDataset(Dataset):
  def __init__(self, filenames, batch_size):
    # `filenames` is a list of strings the contains all file names.
    # `batch_size` is the determines the number of files that we want to read in a chunk.
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))   # Number of chunks.
  def __getitem__(self, idx): #idx means index of the chunk.
    # In this method, we do all the preprocessing.
    # First read data from files in a chunk. Preprocess it. Extract labels. Then return data and labels.
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]   # This extracts one batch of file names from the list `filenames`.
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
            data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
            pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
            for j in range(len(label_classes)):
                if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                    labels.append(j)  
        data = np.asarray(data).reshape(-1,1,32,32) # Because of Pytorch&amp;#39;s channel first convention
        labels = np.asarray(labels)

        # The following condition is actually needed in Pytorch. Otherwise, for our particular example, the iterator will be an infinite loop.
        # Readers can verify this by removing this condition.
        if idx == self.__len__():  
          raise IndexError

        return data, labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the &lt;code&gt;__getitem__&lt;/code&gt; method change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the &lt;code&gt;__getitem__&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Now we will check whether the dataset works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_dataset = CustomDataset(filenames = files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_dataset.__len__()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i, (data, labels) in enumerate(check_dataset):
  print(data.shape, labels.shape)
  print(labels)
  if i == 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;To train a deep learning model, we need to create a data loader from the dataset. Dataloaders offer multi-worker, multi-processing capabilities without requiring us to right codes for that. So let’s first create a dataloader from the dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from torch.utils.data import DataLoader&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dataloader = DataLoader(check_dataset,batch_size = None, shuffle = True) # Here we select batch size to be None as we have already batched our data in dataset.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether dataloader works on not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i, (data,labels) in enumerate(dataloader):
    print(data.shape, labels.shape)
    print(labels)  # Just to see the labels.
    if i == 3: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that dataloader works, we will use it to train a simple deep learning model. The focus of this post is not on the model itself. So we will use a simplest model. If readers want a different model, they can do so by just replacing our model with theirs.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feed-chunks-of-data-to-a-cnn-model-and-train-it-for-several-epochs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Feed chunks of data to a CNN model and train it for several epochs&lt;/h2&gt;
&lt;p&gt;But before we build the model and train it, we will first move our files to different folders depending on their fault type. We do so as it will be convenient later to create a training, validation, and test set from the data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders one each for a given fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle training files (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create the datasets and dataloaders for training, validation, and test set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = CustomDataset(filenames = train_file_names, batch_size = batch_size)
val_dataset = CustomDataset(filenames = validation_file_names, batch_size = batch_size)
test_dataset = CustomDataset(filenames = test_file_names, batch_size = batch_size)

train_dataloader = DataLoader(train_dataset, batch_size = None, shuffle = True)
val_dataloader = DataLoader(val_dataset, batch_size = None)  # Shuffle is False by default.
test_dataloader = DataLoader(test_dataset, batch_size = None)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model. We will build one of the simplest models. Readers are free to choose a different model of their choice. If &lt;code&gt;torchsummary&lt;/code&gt; is not installed, use &lt;code&gt;pip install torchsummary&lt;/code&gt; to install it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear, ReLU, Softmax
from torchsummary import summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dev = torch.device(&amp;quot;cuda&amp;quot;) if torch.cuda.is_available() else torch.device(&amp;quot;cpu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = Sequential(
        Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3),
        ReLU(),
        MaxPool2d(2),
        Conv2d(16,32,3),
        ReLU(),
        MaxPool2d(2),
        Flatten(),
        Linear(in_features = 1152, out_features=16),
        ReLU(),
        Linear(16, 5),
        Softmax(dim = 1)
)
model.to(dev)
summary(model,input_size = (1,32,32), device = dev.type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 30, 30]             160
              ReLU-2           [-1, 16, 30, 30]               0
         MaxPool2d-3           [-1, 16, 15, 15]               0
            Conv2d-4           [-1, 32, 13, 13]           4,640
              ReLU-5           [-1, 32, 13, 13]               0
         MaxPool2d-6             [-1, 32, 6, 6]               0
           Flatten-7                 [-1, 1152]               0
            Linear-8                   [-1, 16]          18,448
              ReLU-9                   [-1, 16]               0
           Linear-10                    [-1, 5]              85
          Softmax-11                    [-1, 5]               0
================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.35
Params size (MB): 0.09
Estimated Total Size (MB): 0.44
----------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = model.float()   # We will make all model parameters floats.
epochs = 10
for epoch in range(epochs):
    running_loss_train = 0.0
    running_loss_val = 0.0
    correct_train = 0.0
    correct_val = 0.0
    num_labels_train = 0.0
    num_labels_val = 0.0

    # Training loop
    for inputs, labels in train_dataloader:
        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.
        optimizer.zero_grad()
        outputs = model(inputs.float())
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss_train = running_loss_train + loss.item()
        correct_train = correct_train + (torch.argmax(outputs,dim = 1) == labels).float().sum()
        num_labels_train = num_labels_train + len(labels)

    # Validation loop
    for inputs, labels in val_dataloader:
        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.
        outputs = model(inputs.float())
        loss = criterion(outputs, labels)
        running_loss_val = running_loss_val + loss.item()
        correct_val = correct_val + (torch.argmax(outputs, dim = 1) == labels).float().sum()
        num_labels_val = num_labels_val + len(labels)

    train_accuracy = correct_train/num_labels_train
    val_accuracy = correct_val/num_labels_val
    print(&amp;quot;Epoch:{}, Train_loss: {:.4f}, Train_accuracy: {:.4f}, Val_loss: {:.4f}, Val_accuracy: {:.4f}&amp;quot;.\
        format(epoch, running_loss_train/len(train_dataloader), train_accuracy, running_loss_val/len(val_dataloader), val_accuracy))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:0, Train_loss: 1.6112, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:1, Train_loss: 1.6096, Train_accuracy: 0.2000, Val_loss: 1.6097, Val_accuracy: 0.2000
Epoch:2, Train_loss: 1.6095, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:3, Train_loss: 1.6090, Train_accuracy: 0.2000, Val_loss: 1.6097, Val_accuracy: 0.2000
Epoch:4, Train_loss: 1.6084, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:5, Train_loss: 1.6075, Train_accuracy: 0.2000, Val_loss: 1.6099, Val_accuracy: 0.2000
Epoch:6, Train_loss: 1.6084, Train_accuracy: 0.2057, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:7, Train_loss: 1.6045, Train_accuracy: 0.2000, Val_loss: 1.6101, Val_accuracy: 0.2000
Epoch:8, Train_loss: 1.6051, Train_accuracy: 0.2000, Val_loss: 1.6100, Val_accuracy: 0.2000
Epoch:9, Train_loss: 1.6001, Train_accuracy: 0.2057, Val_loss: 1.6126, Val_accuracy: 0.2200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we make any comments on training accuracy and validation accuracy, we should keep in mind that our original dataset contains only random numbers. So it would be better if we don’t interpret the results here.&lt;/p&gt;
&lt;p&gt;Compute test score.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;running_loss_test = 0.0
correct_test = 0.0
num_labels_test = 0.0
for inputs, labels in test_dataloader:
    inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # Pytorch expects categorical targets as LongTensor.
    outputs = model(inputs.float())
    loss = criterion(outputs.to(dev), labels.to(dev))
    running_loss_test = running_loss_test + loss.item()
    correct_test = correct_test + (torch.argmax(outputs, dim = 1) == labels).float().sum()
    num_labels_test = num_labels_test + len(labels)

test_accuracy = correct_test/num_labels_test
print(&amp;quot;Test_loss: {:.4f}, Test_accuracy: {:.4f}&amp;quot;.format(running_loss_test/len(test_dataloader), test_accuracy))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test_loss: 1.6130, Test_accuracy: 0.2100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id=&#34;sec_4&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But oftentimes, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just make prediction on this data and then use &lt;code&gt;np.argmax()&lt;/code&gt; or &lt;code&gt;torch.argmax()&lt;/code&gt; to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a separate dataset to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...

[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prediction dataset will be slightly different from our previous custom dataset class. We only need to return data in this case.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class PredictionDataset(Dataset):
  def __init__(self, filenames, batch_size):
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))
  def __getitem__(self, idx):
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) 
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,1,32,32) 
        
        
        if idx == self.__len__():  
          raise IndexError

        return data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the dataset and dataloader work or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_dataset = PredictionDataset(prediction_files, batch_size = 10)
prediction_dataloader = DataLoader(prediction_dataset,batch_size = None, shuffle = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data in prediction_dataloader:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([5, 1, 32, 32])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make predictions.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;preds = []
for data in prediction_dataloader:
    data = data.to(dev)
    preds.append(model(data.float()))
preds = torch.cat(preds)
preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0.3369, 0.0306, 0.2113, 0.3813, 0.0399],
        [0.3747, 0.0238, 0.2525, 0.3083, 0.0407],
        [0.3462, 0.0353, 0.2434, 0.3220, 0.0531],
        [0.3387, 0.0365, 0.2338, 0.3394, 0.0516],
        [0.3619, 0.0246, 0.2021, 0.3783, 0.0331],
        [0.3302, 0.0431, 0.2429, 0.3209, 0.0629],
        [0.4018, 0.0178, 0.2334, 0.3161, 0.0308],
        [0.3479, 0.0335, 0.2398, 0.3288, 0.0501],
        [0.3279, 0.0465, 0.2430, 0.3162, 0.0665],
        [0.3299, 0.0396, 0.2703, 0.2957, 0.0645],
        [0.3538, 0.0263, 0.2408, 0.3382, 0.0409],
        [0.3306, 0.0415, 0.2074, 0.3691, 0.0514],
        [0.3377, 0.0326, 0.2195, 0.3661, 0.0441],
        [0.3326, 0.0387, 0.2409, 0.3324, 0.0554],
        [0.3321, 0.0376, 0.2649, 0.3054, 0.0600],
        [0.3368, 0.0385, 0.2396, 0.3286, 0.0565],
        [0.3596, 0.0239, 0.2409, 0.3377, 0.0379],
        [0.3905, 0.0235, 0.2188, 0.3307, 0.0365],
        [0.3396, 0.0298, 0.2374, 0.3488, 0.0444],
        [0.3534, 0.0248, 0.2546, 0.3260, 0.0412],
        [0.3356, 0.0336, 0.2101, 0.3772, 0.0435],
        [0.3255, 0.0501, 0.2172, 0.3441, 0.0632],
        [0.3375, 0.0318, 0.2428, 0.3409, 0.0471],
        [0.3309, 0.0345, 0.2925, 0.2799, 0.0621],
        [0.3575, 0.0294, 0.2304, 0.3385, 0.0443],
        [0.3312, 0.0428, 0.2192, 0.3513, 0.0556],
        [0.3382, 0.0355, 0.2282, 0.3489, 0.0493],
        [0.3400, 0.0287, 0.2491, 0.3374, 0.0448],
        [0.3407, 0.0410, 0.2238, 0.3386, 0.0559],
        [0.3529, 0.0316, 0.2259, 0.3444, 0.0452],
        [0.3413, 0.0346, 0.2100, 0.3699, 0.0442],
        [0.3432, 0.0274, 0.2159, 0.3754, 0.0380],
        [0.3319, 0.0403, 0.2334, 0.3386, 0.0559],
        [0.3323, 0.0377, 0.2615, 0.3090, 0.0595],
        [0.3351, 0.0355, 0.2241, 0.3571, 0.0482],
        [0.3420, 0.0367, 0.2103, 0.3636, 0.0474],
        [0.3271, 0.0416, 0.1838, 0.4018, 0.0456],
        [0.3345, 0.0272, 0.1773, 0.4302, 0.0308],
        [0.3489, 0.0246, 0.2447, 0.3428, 0.0389],
        [0.3360, 0.0338, 0.1997, 0.3890, 0.0414],
        [0.3340, 0.0365, 0.2511, 0.3235, 0.0550],
        [0.3622, 0.0207, 0.2225, 0.3632, 0.0314],
        [0.3674, 0.0261, 0.2247, 0.3425, 0.0393],
        [0.3371, 0.0320, 0.2391, 0.3452, 0.0465],
        [0.3620, 0.0271, 0.2215, 0.3501, 0.0393],
        [0.3303, 0.0415, 0.2512, 0.3150, 0.0620],
        [0.3315, 0.0402, 0.2165, 0.3600, 0.0517],
        [0.3358, 0.0389, 0.2567, 0.3086, 0.0600],
        [0.3580, 0.0276, 0.2110, 0.3652, 0.0382],
        [0.3577, 0.0290, 0.2221, 0.3494, 0.0418],
        [0.3450, 0.0297, 0.2698, 0.3044, 0.0512],
        [0.3370, 0.0325, 0.2324, 0.3520, 0.0461],
        [0.3694, 0.0259, 0.2063, 0.3626, 0.0358],
        [0.3308, 0.0414, 0.2087, 0.3674, 0.0517],
        [0.3410, 0.0344, 0.2436, 0.3302, 0.0508]], device=&amp;#39;cuda:0&amp;#39;,
       grad_fn=&amp;lt;CatBackward&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; or &lt;code&gt;torch.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;torch.argmax(preds, dim = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 3, 3, 0,
        0, 3, 3, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 0, 3, 0, 3, 0, 3, 0, 0, 3, 0,
        3, 0, 0, 3, 0, 3, 0], device=&amp;#39;cuda:0&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that our data are randomly generated. So we should not be surprised by this result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a custom dataloader, trained a model using that dataloader, and made predictions on new data. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;Also note that we have not used the multi-worker and multi-processing capabilities of dataloader. To further speedup the dataloading process, readers should take advantage of the multiprocessing capabilities of dataloader. The best way to choose optimum multiprocessing and multi-worker parameters is to try a few ones and see which set of parameters work best for the system under consideration.&lt;/p&gt;
&lt;p&gt;As a final note, please keep in mind that the approach we have discussed in only one of many different ways in which we can read multiple files. I have chosen this approach as it seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope this blog would be of help to reader. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: It is very likely that this blog might not have used some of the best practices of PyTorch. This is because the author has a superficial knowledge of PyTorch and is not aware of its best practices. The author (un)fortunately prefers &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Efficiently reading multiple files in Tensorflow 2</title>
      <link>https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Whether this method is efficient or not is contestable. Efficiency of a data input pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they use this method in their own problems. For the simple (and small) problem considered in this post, we got no perceivable performance improvement. But for one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope this method can be of help to others as well. The system on which we ran this notebook has 44 CPU cores. &lt;code&gt;Tensorflow&lt;/code&gt; version was 2.4.0 and we did not use any GPU. Please note that for some weird reason, the speedup technique doesn’t work in &lt;code&gt;Google Colab&lt;/code&gt;. But it works in GPU enabled personal systems, that I have checked.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Along with the method described in this post, readers should also try using &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34;&gt;Tensorflow Sequence&lt;/a&gt; and see if it improves input pipeline efficiecy. Define all the complex transformations inside &lt;code&gt;__getitem__&lt;/code&gt; method of seqeuence class and then suitably choose &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt; in &lt;code&gt;model.fit()&lt;/code&gt; to improve pipeline efficiency.&lt;/p&gt;
&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/0naikdujqvzosh4/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;This post is a sequel to &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;an older post&lt;/a&gt;. In the previous post, we discussed ways in which we can read multiple files in &lt;code&gt;Tensorflow 2&lt;/code&gt;. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are &lt;code&gt;spectrogram&lt;/code&gt; and normalizing (converting each value to a standard normal value). We have chosen these transformations just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this &lt;a href=&#34;https://www.tensorflow.org/guide/data_performance&#34;&gt;tensorflow guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on &lt;a href=&#34;#speedup&#34;&gt;section 4&lt;/a&gt; which is the main topic of this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#create_files&#34;&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generator&#34;&gt;Write a generator that reads data from the folder in chunks and transforms it.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Build data pipeline and train a CNN model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#speedup&#34;&gt;How to make the code run faster?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictions&#34;&gt;How to make predictions?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id = &#34;create_files&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.
* Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.
* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = np.sort(glob.glob(&amp;quot;./random_data/*&amp;quot;))
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





array([&amp;#39;./random_data/Fault_1_001.csv&amp;#39;, &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_003.csv&amp;#39;, &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_005.csv&amp;#39;, &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_007.csv&amp;#39;, &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_009.csv&amp;#39;, &amp;#39;./random_data/Fault_1_010.csv&amp;#39;],
      dtype=&amp;#39;&amp;lt;U29&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;generator&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;These are the few things that we want our generator to have.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It should run indefinitely, i.e., it is an infinite loop.&lt;/li&gt;
&lt;li&gt;Inside generator loop, read individual files using &lt;code&gt;pandas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Do transformations on data if required.&lt;/li&gt;
&lt;li&gt;Yield the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels. The generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow Version: &amp;quot;, tf.__version__)
import pandas as pd
import re&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow Version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:    # This loop makes the generator an infinite loop
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                #########################################################################################################
                # Apply transformations. Comment this portion if you don&amp;#39;t have to do any.
                # Try to use Tensorflow transformations as much as possible. First compute a spectrogram.
                temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
                # After STFT transformation with given parameters, shape = (31,33)
                temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
                ##########################################################################################################
                # temp = tf.reshape(temp, (32,32,1)) # Uncomment this line if you have not done any transformation.
                data.append(temp)
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
            data = np.asarray(data).reshape(-1,31,33,1) 
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The generator works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;model&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-a-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training a CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;))[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;))[:10] # Showing first 10 files of Falut_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle files
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,31,33,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,31,33,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,31,33,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (31,33,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 29, 31, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 12, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  11
validation_steps =  2
steps =  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 2s 202ms/step - loss: 1.6211 - accuracy: 0.1585 - val_loss: 1.6088 - val_accuracy: 0.1400
Epoch 2/5
11/11 [==============================] - 2s 164ms/step - loss: 1.6080 - accuracy: 0.2110 - val_loss: 1.6097 - val_accuracy: 0.2200
Epoch 3/5
11/11 [==============================] - 2s 164ms/step - loss: 1.6084 - accuracy: 0.1907 - val_loss: 1.6093 - val_accuracy: 0.1200
Epoch 4/5
11/11 [==============================] - 2s 163ms/step - loss: 1.6038 - accuracy: 0.2405 - val_loss: 1.6101 - val_accuracy: 0.1800
Epoch 5/5
11/11 [==============================] - 2s 162ms/step - loss: 1.6025 - accuracy: 0.2750 - val_loss: 1.6101 - val_accuracy: 0.1400





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7f28cc0cfd60&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 0s 101ms/step - loss: 1.6099 - accuracy: 0.1900&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6099034547805786
Test accuracy: 0.1899999976158142&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;speedup&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-the-code-run-faster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make the code run faster?&lt;/h2&gt;
&lt;p&gt;If no transformations are used, just using &lt;code&gt;prefetch&lt;/code&gt; might improve performance. In deep learning usually GPUs are used for training. But all the data processing is done in CPU. In the naive approach, we will first process data in CPU, then send the processed data to GPU and after training finishes, we will prepare another batch of data. This approach is not efficient because GPU has to wait for data to get prepared. But using &lt;code&gt;prefetch&lt;/code&gt;, we prepare and keep ready batches of data while training continues. In this way, waiting time of GPU is minimized.&lt;/p&gt;
&lt;p&gt;When data transformations are used, out aim should always be to use parallel processing capabilities of &lt;code&gt;tensorflow&lt;/code&gt;. We can achieve this using &lt;code&gt;map&lt;/code&gt; function. Inside the &lt;code&gt;map&lt;/code&gt; function, all transformations are defined. Then we can &lt;code&gt;prefetch&lt;/code&gt; batches to further improve performance. The whole pipeline is as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. def transformation_function(...):
    # Define all transormations (STFT, Normalization, etc.)
    
2. def generator(...):
    
       # Read data
    
       # Call transformation_function using tf.data.Dataset.map so that it can parallelize operations.
    
       # Finally yield the processed data

3. Create tf.data.Dataset s.

4. Prefecth datasets.

5. Create model and train it.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use one extra library &lt;code&gt;tensorflow_datasets&lt;/code&gt; that will allow us to switch from &lt;code&gt;tf.dataset&lt;/code&gt; to &lt;code&gt;numpy&lt;/code&gt;. If &lt;code&gt;tensorflow_datasets&lt;/code&gt; is not installed in your system, use &lt;code&gt;pip install tensorflow-datasets&lt;/code&gt; to install it and then run following codes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow_datasets as tfds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_transformation_func(data):
  transformed_data = tf.math.abs(tf.signal.stft(data,frame_length = 64, frame_step = 32, fft_length = 64))
  transformed_data = tf.image.per_image_standardization(tf.reshape(transformed_data, shape = (-1,31,33,1))) # Normalization
  return transformed_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator_new(file_list, batch_size = 4):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size]
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                data.append(tf.reshape(temp.values, shape = (1,1024)))
                pattern = tf.constant(eval(&amp;quot;file[22:29]&amp;quot;))
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
                    
            data = np.asarray(data)
            labels = np.asarray(labels)
            first_dim = data.shape[0]
            # Create tensorflow dataset so that we can use `map` function that can do parallel computation.
            data_ds = tf.data.Dataset.from_tensor_slices(data)
            data_ds = data_ds.batch(batch_size = first_dim).map(data_transformation_func,
                                                                num_parallel_calls = tf.data.experimental.AUTOTUNE)
            # Convert the dataset to a generator and subsequently to numpy array
            data_ds = tfds.as_numpy(data_ds)   # This is where tensorflow-datasets library is used.
            data = np.asarray([data for data in data_ds]).reshape(first_dim,31,33,1)
            
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_045.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_032.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_025.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_013.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_087.csv&amp;#39;,
 &amp;#39;./random_data/Fault_5/Fault_5_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_4/Fault_4_019.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_034.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_044.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[0][22:29]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;Fault_3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 20
dataset_check = tf.data.Dataset.from_generator(tf_data_generator_new,args= [train_file_names, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset_check.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(20, 31, 33, 1)
tf.Tensor([2. 0. 0. 1. 2. 0. 4. 3. 2. 1. 1. 0. 3. 3. 2. 3. 1. 4. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([3. 1. 1. 3. 4. 4. 2. 3. 4. 3. 3. 0. 1. 2. 0. 3. 2. 2. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 3. 0. 2. 2. 4. 3. 0. 4. 1. 0. 0. 2. 0. 0. 1. 0. 3. 2. 1.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([4. 2. 2. 2. 0. 3. 4. 2. 0. 1. 2. 2. 3. 4. 0. 4. 2. 0. 4. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([1. 0. 4. 4. 0. 1. 0. 4. 0. 2. 1. 4. 3. 2. 1. 4. 4. 2. 4. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 2. 0. 1. 3. 2. 2. 2. 1. 3. 3. 4. 0. 1. 4. 1. 3. 2. 1. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 1. 2. 2. 4. 4. 1. 0. 2. 2. 1. 2. 3. 0. 0. 2. 2. 0. 3. 3.], shape=(20,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [train_file_names, batch_size], 
                                                  output_shapes = ((None,31,33,1),(None,)),
                                                  output_types = (tf.float32, tf.float32))

validation_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [validation_file_names, batch_size],
                                                       output_shapes = ((None,31,33,1),(None,)),
                                                       output_types = (tf.float32, tf.float32))

test_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [test_file_names, batch_size],
                                                 output_shapes = ((None,31,33,1),(None,)),
                                                 output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prefetch datasets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_dataset_new = train_dataset_new.prefetch(buffer_size = tf.data.AUTOTUNE)
validation_dataset_new = validation_dataset_new.prefetch(buffer_size = tf.data.AUTOTUNE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset_new, validation_data = validation_dataset_new, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 3s 226ms/step - loss: 1.6027 - accuracy: 0.1989 - val_loss: 1.6112 - val_accuracy: 0.1600
Epoch 2/5
11/11 [==============================] - 2s 214ms/step - loss: 1.5986 - accuracy: 0.2520 - val_loss: 1.6104 - val_accuracy: 0.2400
Epoch 3/5
11/11 [==============================] - 2s 200ms/step - loss: 1.5954 - accuracy: 0.3161 - val_loss: 1.6122 - val_accuracy: 0.1800
Epoch 4/5
11/11 [==============================] - 2s 209ms/step - loss: 1.5892 - accuracy: 0.3650 - val_loss: 1.6101 - val_accuracy: 0.1600
Epoch 5/5
11/11 [==============================] - 2s 196ms/step - loss: 1.5816 - accuracy: 0.2972 - val_loss: 1.6148 - val_accuracy: 0.1600





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7f2888147940&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss_new, test_acc_new = model.evaluate(test_dataset_new, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 1s 139ms/step - loss: 1.6089 - accuracy: 0.2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;predictions&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;In the generator used for prediction, we can also use &lt;code&gt;map&lt;/code&gt; function to parallelize data preprocessing. But in practice, inference is much faster. So we can make fast predictions using naive method also. We show the naive implementation below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)
            temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
            # After STFT transformation with given parameters, shape = (31,33)
            temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
            data.append(temp) 
        data = np.asarray(data).reshape(-1,31,33,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(5, 31, 33, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,31,33,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)


array([[0.13616945, 0.22521223, 0.29032916, 0.11108191, 0.23720728],
       [0.13136739, 0.1767847 , 0.2776762 , 0.11521462, 0.29895717],
       [0.12264969, 0.17929009, 0.2746509 , 0.11005757, 0.3133517 ],
       [0.12557542, 0.14570946, 0.20385396, 0.15842426, 0.3664368 ],
       [0.13804483, 0.13592169, 0.24367407, 0.13454145, 0.347818  ],
       [0.1430458 , 0.18991745, 0.28873193, 0.11874774, 0.25955713],
       [0.13853352, 0.17857482, 0.31380644, 0.10534842, 0.26373684],
       [0.10823276, 0.22618511, 0.32452983, 0.0847131 , 0.25633916],
       [0.1383514 , 0.16129029, 0.25447774, 0.13601685, 0.30986372],
       [0.13583152, 0.1730804 , 0.25602627, 0.12829432, 0.30676743],
       [0.12959503, 0.1772274 , 0.30786148, 0.10242429, 0.28289178],
       [0.13454609, 0.16846487, 0.2601272 , 0.12760776, 0.30925405],
       [0.14779252, 0.14842029, 0.2833778 , 0.11843931, 0.3019701 ],
       [0.11896624, 0.21513633, 0.25535005, 0.11902266, 0.29152465],
       [0.13734229, 0.13935044, 0.2748529 , 0.11947   , 0.3289844 ],
       [0.15610486, 0.20303495, 0.30530566, 0.11581586, 0.21973862],
       [0.13397609, 0.15995616, 0.28893223, 0.11423217, 0.30290335],
       [0.12130069, 0.22576565, 0.2828214 , 0.10909654, 0.26101568],
       [0.15606783, 0.14581656, 0.25918248, 0.1374906 , 0.3014426 ],
       [0.13284522, 0.15171063, 0.23527463, 0.13531938, 0.34485018],
       [0.15039593, 0.18859874, 0.2730181 , 0.13246077, 0.25552657],
       [0.13006356, 0.23040107, 0.31713945, 0.09858881, 0.2238071 ],
       [0.14617579, 0.1553615 , 0.24506982, 0.14371207, 0.30968076],
       [0.15057516, 0.18175651, 0.26442483, 0.13344486, 0.26979864],
       [0.12942658, 0.17502321, 0.27020454, 0.11938909, 0.3059566 ],
       [0.1362109 , 0.18799251, 0.2874499 , 0.11758988, 0.27075684],
       [0.12986937, 0.20687391, 0.28071418, 0.11440149, 0.26814106],
       [0.13393444, 0.1739722 , 0.27028513, 0.12331051, 0.2984978 ],
       [0.12112842, 0.13956769, 0.22920072, 0.12982692, 0.3802763 ],
       [0.11119145, 0.23633307, 0.32426968, 0.08549411, 0.2427116 ],
       [0.13327955, 0.18379854, 0.2872899 , 0.11320265, 0.28242943],
       [0.12073855, 0.20085782, 0.2646106 , 0.11651796, 0.29727504],
       [0.11189438, 0.20137395, 0.27387396, 0.10702953, 0.30582818],
       [0.18017001, 0.16150263, 0.28068233, 0.1368001 , 0.2408449 ],
       [0.10944357, 0.17276171, 0.25993338, 0.10688126, 0.35098007],
       [0.13728923, 0.1559456 , 0.25643092, 0.13189963, 0.31843463],
       [0.15782082, 0.1793215 , 0.28856605, 0.12700985, 0.24728177],
       [0.13353582, 0.20542818, 0.32362464, 0.0972899 , 0.2401215 ],
       [0.1327661 , 0.19204186, 0.29048327, 0.11032179, 0.274387  ],
       [0.15101205, 0.16577183, 0.28014943, 0.12510163, 0.27796513],
       [0.10811005, 0.24937892, 0.2825413 , 0.09674085, 0.26322895],
       [0.12502007, 0.17934126, 0.23135227, 0.1389524 , 0.32533407],
       [0.15938489, 0.12479166, 0.2140554 , 0.16871263, 0.33305547],
       [0.13133633, 0.15853986, 0.2776162 , 0.11680949, 0.31569818],
       [0.13070984, 0.20629251, 0.32593974, 0.09547149, 0.24158639],
       [0.12578759, 0.13497958, 0.2329479 , 0.13350599, 0.37277895],
       [0.11535928, 0.18584532, 0.25516343, 0.11577264, 0.3278593 ],
       [0.12250951, 0.16624808, 0.22112629, 0.14213458, 0.34798154],
       [0.11505162, 0.22170952, 0.29335403, 0.09799453, 0.27189022],
       [0.15128227, 0.18352163, 0.26057395, 0.1377367 , 0.2668855 ],
       [0.10571367, 0.14169416, 0.2291365 , 0.12079947, 0.40265626],
       [0.11637849, 0.2011716 , 0.28354362, 0.10431051, 0.29459572],
       [0.12311503, 0.1520483 , 0.26735488, 0.11320169, 0.34428006],
       [0.13812302, 0.23593263, 0.27522495, 0.12361279, 0.22710668],
       [0.13761182, 0.17356406, 0.23145248, 0.14515112, 0.3122205 ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2,
       4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4,
       2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a final comment, read the &lt;strong&gt;note&lt;/strong&gt; at the beginning of this post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Doing Linear Algebra using Tensorflow 2</title>
      <link>https://biswajitsahoo1111.github.io/post/doing-linear-algebra-using-tensorflow-2/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/doing-linear-algebra-using-tensorflow-2/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Doing_Linear_Algebra_using_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Doing_Linear_Algebra_using_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/vtp81fo71uo9ctn/Doing_Linear_Algebra_using_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will explore ways of doing linear algebra &lt;strong&gt;only&lt;/strong&gt; using &lt;code&gt;tensorflow&lt;/code&gt;. We will only import &lt;code&gt;tensorflow&lt;/code&gt; and nothing else. As we will see, we can do all the common linear algebra operations without using any other library. This post is very long as it covers almost all the functions that are there in the linear algebra library &lt;code&gt;tf.linalg&lt;/code&gt;. But this is not a copy of &lt;code&gt;tensorflow&lt;/code&gt; documentation. Rather, the &lt;code&gt;tensorflow&lt;/code&gt; documentation is a super set of what has been discussed here. This post also assumes that readers have a working knowledge of linear algebra. Most of the times, we will give examples to illustrate a function without going into the underlying theory. Interested readers should use the contents to browse relevant sections of their interest.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basics&#34;&gt;Basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating_tensors&#34;&gt;Creating tensors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating_a_sequence_of_numbers&#34;&gt;Creating a sequence of numbers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slicing&#34;&gt;Slicing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modifying_elements_of_a_matrix&#34;&gt;Modifying elements of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating_a_complex_matrix&#34;&gt;Creating a complex matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_matrix&#34;&gt;Transpose of a matrix&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_real_matrix&#34;&gt;Transpose of a real matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_complex_matrix&#34;&gt;Transpose of a complex matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_common_matrices&#34;&gt;Some common matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#identity_matrix&#34;&gt;Identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagonal_matrix&#34;&gt;Diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tri-diagonal_matrix&#34;&gt;Tri-diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_of_all_zeros_and_ones&#34;&gt;Matrix of all zeros and ones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_matrices&#34;&gt;Random matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#random_uniform_matrix&#34;&gt;Random uniform matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_normal_matrix&#34;&gt;Random normal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#truncated_random_normal_matrix&#34;&gt;Truncated random normal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_poisson_matrix&#34;&gt;Random Poisson matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_gamma_matrix&#34;&gt;Random gamma matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_special_matrices&#34;&gt;Some special matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sparse_matrices&#34;&gt;Sparse matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_multiplication&#34;&gt;Matrix multiplication&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_column_vectors&#34;&gt;Multiplying two column vectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inner_product&#34;&gt;Inner product&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outer_product&#34;&gt;Outer product&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_a_matrix_with_a_vector&#34;&gt;Multiplying a matrix with a vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_matrices&#34;&gt;Multiplying two matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_tri-diagonal_matrices&#34;&gt;Multiplying two tri-diagonal matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_common_operations_on_matrices&#34;&gt;Some common operations on matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trace&#34;&gt;Trace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#determinant&#34;&gt;Determinant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rank&#34;&gt;Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_inverse&#34;&gt;Matrix inverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extract_diagonals_of_a_matrix&#34;&gt;Extract diagonals of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extract_band_part_of_a_matrix&#34;&gt;Extract band part of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_factorizations&#34;&gt;Matrix factorizations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lu&#34;&gt;LU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cholesky&#34;&gt;Cholesky&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qr&#34;&gt;QR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#svd&#34;&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigenvalues_and_eigenvectors&#34;&gt;Eigenvalues and eigenvectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eigen-analysis_of_hermitian_matrices&#34;&gt;Eigen-analysis of Hermitian matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigen-analysis_of_non-Hermitian_matrices&#34;&gt;Eigen-analysis of non-Hermitian matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_dense_linear_systems&#34;&gt;Solving dense linear systems&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using_lu_decomposition&#34;&gt;Using LU decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using_cholesky_decomposition&#34;&gt;Using Cholesky decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_structured_linear_systems&#34;&gt;Solving structured linear systems&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solving_triangular_systems&#34;&gt;Solving triangular systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_tri-diagonal_systems&#34;&gt;Solving tri-diagonal systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_banded_triangular_systems&#34;&gt;Solving banded triangular systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_least_squares_problems&#34;&gt;Solving least squares problems&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ordinary_least_squares&#34;&gt;Ordinary least squares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularized_least_squares&#34;&gt;Regularized least squares&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_specialized_operations&#34;&gt;Some specialized operations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#norm&#34;&gt;Norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalizing_a_tensor&#34;&gt;Normalizing a tensor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#global_norm&#34;&gt;Global norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross_product_of_vectors&#34;&gt;Cross product of vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_square_root&#34;&gt;Matrix square root&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_exponential&#34;&gt;Matrix exponential&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_logarithm&#34;&gt;Matrix logarithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#log-determinant_of_a_matrix&#34;&gt;Log-determinant of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pseudo_inverse_of_a_matrix&#34;&gt;Pseudo inverse of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear_operators&#34;&gt;Linear operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#common_methods_on_linear_operators&#34;&gt;Common methods on linear operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#special_matrices_using_operators&#34;&gt;Special matrices using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#toeplitz_matrix&#34;&gt;Toeplitz matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#circulant_matrix&#34;&gt;Circulant matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block_diagonal_matrix&#34;&gt;Block diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block_lower_triangular_matrix&#34;&gt;Block lower triangular matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#householder_matrix&#34;&gt;Householder matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kronecker_matrix&#34;&gt;Kronecker matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#permutation_matrix&#34;&gt;Permutation matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common_matrices_using_operators&#34;&gt;Common matrices using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#identity_matrix&#34;&gt;Identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled_identity_matrix&#34;&gt;Scaled identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagonal_matrix&#34;&gt;Diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tri-diagonal_matrix&#34;&gt;Tri-diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lower_triangular_matrix&#34;&gt;Lower triangular matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_of_zeros&#34;&gt;Matrix of zeros&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_operations_using_operators&#34;&gt;Matrix operations using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#low-rank_update&#34;&gt;Low-rank update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operator_inversion&#34;&gt;Operator inversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operator_composition&#34;&gt;Operator composition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing we have to keep in mind is that while accessing a function, we have to always append the function by &lt;code&gt;tf.linalg&lt;/code&gt;. It is possible to remove the &lt;code&gt;tf&lt;/code&gt; part by importing the &lt;code&gt;linalg&lt;/code&gt; library from &lt;code&gt;tensorflow&lt;/code&gt;. But even then we have to append every function by &lt;code&gt;linalg&lt;/code&gt;. In this post, we will always use &lt;code&gt;tf.linalg&lt;/code&gt; followed by function name. This amounts to little more typing. But we will do this to remind ourselves that we are using &lt;code&gt;linalg&lt;/code&gt; library of &lt;code&gt;tensorflow&lt;/code&gt;. This might seem little awkward to seasoned users of &lt;code&gt;MATLAB&lt;/code&gt; or &lt;code&gt;Julia&lt;/code&gt; where you just need to type the function name to use it without having to write the library name all the time. Except that, linear algebra in &lt;code&gt;tensorflow&lt;/code&gt; seems quite natural.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In this post, we will show some of the ways in which we can handle matrix operations in &lt;code&gt;Tensorflow&lt;/code&gt;. We will mainly use 1D or 2D arrays in our examples. But matrix operations in Tensorflow are not limited to 2D arrays. In fact, the operations can be done on multidimensional arrays. If an array has more than 2 dimensions, the matrix operation is done on the &lt;strong&gt;last two&lt;/strong&gt; dimensions and the same operation is carried across other dimensions. For example, if our array has a shape of (3,5,5), it can be thought of as 3 matrices each of shape (5,5). When we call a matrix function on this array, the matrix function is applied to all 3 matrices of shape (5,5). This is also true for higher dimensional arrays.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;basics&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Tensorflow operates on &lt;code&gt;Tensors&lt;/code&gt;. &lt;code&gt;Tensors&lt;/code&gt; are characterized by their rank. Following table shows different types of tensors and their corresponding rank.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Tensors&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Scalars&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 0 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Vectors (1D array)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 1 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Matrices (2D array)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 2 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3D array&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 3 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a id = &#34;creating_tensors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;creating-tensors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating tensors&lt;/h3&gt;
&lt;p&gt;In this section, we will create &lt;code&gt;tensors&lt;/code&gt; of different rank, starting from scalars to multi-dimensional arrays. Though tensors can be both real or complex, we will mainly focus on real tensors.&lt;/p&gt;
&lt;p&gt;A scalar contains a single (real or complex) value.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant(5.0)
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the result is a &lt;code&gt;tf.Tensor&lt;/code&gt;. As scalars are rank 0 tensors, its shape is empty. Data type of the tensor is &lt;code&gt;float32&lt;/code&gt;. And corresponding numpy array is 5. We can get only the value of the tensor by calling &lt;code&gt;numpy&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, we can define 1D and 2D &lt;code&gt;tensors&lt;/code&gt;. While 1D &lt;code&gt;tensors&lt;/code&gt; are called vectors, 2D &lt;code&gt;tensors&lt;/code&gt; are called matrices.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([1, 3, 7, 9])    # Note the shape in result. Only one shape parameter is used for vectors.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 3, 7, 9], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([[1,2,3,4],
            [5,6,7,8]])     # Note the shape in result. There are two shape parameters (rows, columns).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=
array([[1, 2, 3, 4],
       [5, 6, 7, 8]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to define a 2D array is given below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([1,2,3,4,5,6,7,8.0], shape = (2,4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
array([[1., 2., 3., 4.],
       [5., 6., 7., 8.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;creating_a_sequence_of_numbers&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-sequence-of-numbers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a sequence of numbers&lt;/h3&gt;
&lt;p&gt;There are two ways to generate sequence of numbers in &lt;code&gt;Tensorflow&lt;/code&gt;. Functions &lt;code&gt;tf.range&lt;/code&gt; and &lt;code&gt;tf.linspace&lt;/code&gt; can be used for that purpose. Sequences generated by these functions are equally spaced.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence = tf.range(start = 1,limit = 10, delta = 1)
sequence.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the last element (limit) is not included in the array. This is consistent with &lt;code&gt;Python&lt;/code&gt; behavior but in departure with &lt;code&gt;MATLAB&lt;/code&gt; and &lt;code&gt;Julia&lt;/code&gt; convention. It is also possible to set &lt;code&gt;delta&lt;/code&gt; to a fraction.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.range(start = 1, limit = 10, delta = 1.5).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1. , 2.5, 4. , 5.5, 7. , 8.5], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linspace(start = 1.0, stop = 10, num = 25)  # Start must be a `float`. See documentation for more details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(25,), dtype=float32, numpy=
array([ 1.   ,  1.375,  1.75 ,  2.125,  2.5  ,  2.875,  3.25 ,  3.625,
        4.   ,  4.375,  4.75 ,  5.125,  5.5  ,  5.875,  6.25 ,  6.625,
        7.   ,  7.375,  7.75 ,  8.125,  8.5  ,  8.875,  9.25 ,  9.625,
       10.   ], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though in this post we will mainly focus on matrices, it is easy to create higher dimensional arrays in &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant(tf.range(1,13), shape = (2,3,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=
array([[[ 1,  2],
        [ 3,  4],
        [ 5,  6]],

       [[ 7,  8],
        [ 9, 10],
        [11, 12]]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;slicing&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;slicing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slicing&lt;/h3&gt;
&lt;p&gt;Slicing is similar to that of &lt;code&gt;numpy&lt;/code&gt; slicing. For vectors (rank 1 tensor with only one shape parameter), only one argument is passed that corresponds to the location of starting index and end index of sliced array. For matrices (rank 2 tensor with two shape parameters), two input arguments need to be passed. First one for rows and second one for columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector = tf.range(start = 1, limit = 10)
vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector[3:7].numpy() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([4, 5, 6, 7], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indexing in &lt;code&gt;tensorflow&lt;/code&gt; starts from zero. In the above example, start index is 3. So that corresponds to 4th element of the vector. And end index is not included. This is similar to &lt;code&gt;Python&lt;/code&gt; convention.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix = tf.constant(tf.range(20, dtype = tf.float32), shape = (4,5))
matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix[1:3, 2:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 7.,  8.],
       [12., 13.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix[:3,2:]      # Same behavior as numpy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 2.,  3.,  4.],
       [ 7.,  8.,  9.],
       [12., 13., 14.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;modifying_elements_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modifying-elements-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modifying elements of a matrix&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Tensors&lt;/code&gt; in &lt;code&gt;tensorflow&lt;/code&gt;, once created, can’t be modified. So the following code segment will result in an error.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = tf.constant([1,2,3,4])
&amp;gt;&amp;gt;&amp;gt; a[2] = 5  # Error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a way to modify values of a matrix. Instead of creating a &lt;code&gt;tensor&lt;/code&gt;, we create a &lt;code&gt;Variable&lt;/code&gt;. &lt;code&gt;Variables&lt;/code&gt; work just like &lt;code&gt;tensors&lt;/code&gt; with the added advantage that their values can be modified. So if we want to modify entries of our matrix at a later stage, we have to first create our matrix as a variable. Then we can do assignment using &lt;code&gt;assign&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;variable_mat = tf.Variable(tf.constant(tf.range(12, dtype = tf.float32), shape = (3,4)))
variable_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &amp;#39;Variable:0&amp;#39; shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  6.,  7.],
       [ 8.,  9., 10., 11.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;variable_mat[:2,2:4].assign(-1*tf.ones(shape = (2,2)))
variable_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &amp;#39;Variable:0&amp;#39; shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1., -1., -1.],
       [ 4.,  5., -1., -1.],
       [ 8.,  9., 10., 11.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;creating_a_complex_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-complex-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a complex matrix&lt;/h3&gt;
&lt;p&gt;To create a complex matrix, we have to first create the real part and imaginary part separately. Then both real and imaginary parts can be combined element wise to create a complex matrix. Elements of both real and imaginary part should be floats. This is the hard way of creating complex a complex matrix. We will discuss the simpler way next.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;real_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)
imag_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)
print(&amp;quot;Real part:&amp;quot;)
print(real_part)
print()
print(&amp;quot;Imaginary part:&amp;quot;)
print(imag_part)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Real part:
tf.Tensor(
[[3.8433075 4.279177 ]
 [2.409762  1.238677 ]
 [2.4724636 1.6782365]], shape=(3, 2), dtype=float32)

Imaginary part:
tf.Tensor(
[[2.90653   4.282353 ]
 [1.0855489 1.4715123]
 [1.3954673 4.987824 ]], shape=(3, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat = tf.dtypes.complex(real = real_part, imag = imag_part)
print(complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[3.8433075+2.90653j   4.279177 +4.282353j ]
 [2.409762 +1.0855489j 1.238677 +1.4715123j]
 [2.4724636+1.3954673j 1.6782365+4.987824j ]], shape=(3, 2), dtype=complex64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a simpler way to create a complex matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat_2 = tf.constant([1+2j, 2+3j , 3+4j, 4+5j, 5+6j, 6+7j], shape = (2,3))
complex_mat_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3), dtype=complex128, numpy=
array([[1.+2.j, 2.+3.j, 3.+4.j],
       [4.+5.j, 5.+6.j, 6.+7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transpose of a matrix&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_real_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;transpose-of-a-real-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transpose of a real matrix&lt;/h4&gt;
&lt;p&gt;For real matrices &lt;code&gt;transpose&lt;/code&gt; just means changing the rows into columns and vice versa. There are three functions that achieve this.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.transpose&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.adjoint&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.matrix_transpose&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For real matrices, all three functions give identical results.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.transpose(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.adjoint(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_complex_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-of-a-complex-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transpose of a complex matrix&lt;/h4&gt;
&lt;p&gt;Things are little different when we have a complex matrix. For complex matrices, we can take regular transpose or conjugate transpose if we want. Default is regular transpose. To take conjugate transpose, we have to set &lt;code&gt;conjugate = False&lt;/code&gt; in &lt;code&gt;tf.transpose&lt;/code&gt; and &lt;code&gt;tf.linalg.matrix_transpose&lt;/code&gt; or use &lt;code&gt;tf.linalg.adjoint&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat_2.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.+2.j, 2.+3.j, 3.+4.j],
       [4.+5.j, 5.+6.j, 6.+7.j]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;transpose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = False) # Regular transpose
print(transpose_of_complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[1.+2.j 4.+5.j]
 [2.+3.j 5.+6.j]
 [3.+4.j 6.+7.j]], shape=(3, 2), dtype=complex128)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;conjugate_transpose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = True) # Conjugate transpose
print(conjugate_transpose_of_complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[1.-2.j 4.-5.j]
 [2.-3.j 5.-6.j]
 [3.-4.j 6.-7.j]], shape=(3, 2), dtype=complex128)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also do conjugate transpose by using function &lt;code&gt;linalg.adjoint&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.adjoint(complex_mat_2).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.-2.j, 4.-5.j],
       [2.-3.j, 5.-6.j],
       [3.-4.j, 6.-7.j]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to take transpose of a matrix is to use the function &lt;code&gt;linalg.matrix_transpose&lt;/code&gt;. In this function, we can set argument &lt;code&gt;conjugate&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt; depending on whether we want regular transpose or conjugate transpose. Default is &lt;code&gt;conjugate = False&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(complex_mat_2)   # Conjugate = False is the default&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=
array([[1.+2.j, 4.+5.j],
       [2.+3.j, 5.+6.j],
       [3.+4.j, 6.+7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(complex_mat_2, conjugate = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=
array([[1.-2.j, 4.-5.j],
       [2.-3.j, 5.-6.j],
       [3.-4.j, 6.-7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_common_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-common-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some common matrices&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;identity-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eye(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4,5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[1, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 3, 0, 0],
       [0, 0, 0, 4, 0],
       [0, 0, 0, 0, 5]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create diagonal matrix, we can also use &lt;code&gt;tf.linalg.tensor_diag&lt;/code&gt; with main diagonal as input.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tensor_diag(tf.constant([1,2,3,4,5.]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 3., 0., 0.],
       [0., 0., 0., 4., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create a matrix whose only nonzero entries are on its super-diagonals or sub-diagonals.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4], k = 1)   # Values in super-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[0, 1, 0, 0, 0],
       [0, 0, 2, 0, 0],
       [0, 0, 0, 3, 0],
       [0, 0, 0, 0, 4],
       [0, 0, 0, 0, 0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4], k = -1)  # Values in sub-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[0, 0, 0, 0, 0],
       [1, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 3, 0, 0],
       [0, 0, 0, 4, 0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4,5], k = 0, padding_value = -1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[ 1, -1, -1, -1, -1],
       [-1,  2, -1, -1, -1],
       [-1, -1,  3, -1, -1],
       [-1, -1, -1,  4, -1],
       [-1, -1, -1, -1,  5]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to create a diagonal matrix is by using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.zeros(shape = (5,5))
diag = tf.constant([1,2,3,4,5.])
tf.linalg.set_diag(input = mat, diagonal = diag, k = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 3., 0., 0.],
       [0., 0., 0., 4., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.set_diag(mat, tf.constant([1,2,3,4.]), k = 1)  # Set super-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0., 1., 0., 0., 0.],
       [0., 0., 2., 0., 0.],
       [0., 0., 0., 3., 0.],
       [0., 0., 0., 0., 4.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[1,2,3,4,5],
                     [6,7,8,9,0.]])
tf.linalg.set_diag(mat, diags, k = (-1,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [6., 2., 0., 0., 0.],
       [0., 7., 3., 0., 0.],
       [0., 0., 8., 4., 0.],
       [0., 0., 0., 9., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;note-on-alignment-strategy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Note on alignment strategy&lt;/h4&gt;
&lt;p&gt;Functions like &lt;code&gt;tf.linalg.diag&lt;/code&gt; and &lt;code&gt;tf.linalg.set_diag&lt;/code&gt; and several others to be seen later, take an argument called &lt;code&gt;align&lt;/code&gt; among other things. Though up to this point we have used default parameters of &lt;code&gt;align&lt;/code&gt;, we believe, a note is warranted at this point. There are four different alignment strategies in tensorflow. Those are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LEFT_LEFT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at right&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at right&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LEFT_RIGHT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at right&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at left&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RIGHT_LEFT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at left&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at right&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RIGHT_RIGHT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at left&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at left&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One way to remember the above rules is that if something is aligned to left, it is appended at right. And in &lt;code&gt;LEFT_RIGHT&lt;/code&gt;, first word corresponds to superdiagonals and second word corresponds to subdiagonals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-do-we-need-an-alignment-strategy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Why do we need an alignment strategy?&lt;/h4&gt;
&lt;p&gt;Both superdiagonals and subdiagonals have less number of entries than the main diagonal. If we extract (for some reason) the subdiagonals (or superdiagonals) of a matrix along with the main diagonal, the resulting array will have different lengths. These type of arrays are called &lt;code&gt;ragged array&lt;/code&gt;s. Though &lt;code&gt;tensorflow&lt;/code&gt; can handle &lt;code&gt;ragged array&lt;/code&gt;s, the results of linear algebra library are always uniform arrays (i.e., all the arrays have same number of entries). So while extracting subdiagonals (or superdiagonals), we have to append it either at the left or at right to make it of the same length as the main diagonal. This leads to the question of where the arrays should be appended. &lt;code&gt;Tensorflow&lt;/code&gt; leaves that option to the readers. Depending on where we append the subdiagonals and superdiagonals, there are four alignment strategies as mentioned below.&lt;/p&gt;
&lt;p&gt;In the next section, we will see a way to create tri-diagonal matrix using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;tri-diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tri-diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tri-diagonal matrix&lt;/h4&gt;
&lt;p&gt;Let’s create &lt;a href=&#34;http://www-math.mit.edu/~gs/&#34;&gt;Gilbert Strang’s&lt;/a&gt; favorite matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag(tf.repeat(2,repeats = 5)) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = -1) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[ 2, -1,  0,  0,  0],
       [-1,  2, -1,  0,  0],
       [ 0, -1,  2, -1,  0],
       [ 0,  0, -1,  2, -1],
       [ 0,  0,  0, -1,  2]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tri-digonal-matrix-using-tf.linalg.set_diag&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tri-digonal matrix using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;While setting more that one diagonals using &lt;code&gt;set_diag&lt;/code&gt;, say &lt;code&gt;k = (-2,3)&lt;/code&gt;, we have to have 6 diagonals (2 sub-diagonals, 1 main diagonal, and 3 super-diagonals). First three rows of the input diagonals will correspond to super-diagonals and have to be appended at the right by zeros (according to alignment strategy of &lt;code&gt;&#34;LEFT_RIGHT&#34;&lt;/code&gt;). Fourth row corresponds to main diagonal. Last two rows correspond to sub-diagonals and have to be appended at the left by zeros (according to alignment strategy of &lt;code&gt;&#34;LEFT_RIGHT&#34;&lt;/code&gt;). We could have chosen any other alignment strategy and modified our input accordingly. Using this rule, now we will create a tri-diagonal matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1,-1,-1,-1, 0],
                     [ 2, 2, 2, 2, 2],
                     [ 0,-1,-1,-1,-1]], dtype = tf.float32)
mat = tf.zeros(shape = (5,5))
tf.linalg.set_diag(mat,diags, k = (-1,1), align = &amp;quot;LEFT_RIGHT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2., -1.,  0.,  0.,  0.],
       [-1.,  2., -1.,  0.,  0.],
       [ 0., -1.,  2., -1.,  0.],
       [ 0.,  0., -1.,  2., -1.],
       [ 0.,  0.,  0., -1.,  2.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is yet another simpler way to create a tri-diagonal matrix using a linear operator. We will see that technique in a later section.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_of_all_zeros_and_ones&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-of-all-zeros-and-ones&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix of all zeros and ones&lt;/h4&gt;
&lt;p&gt;Matrices of all 1s or all 0s are not in &lt;code&gt;linalg&lt;/code&gt; library. But those are available in core &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.zeros(shape = (3,5), dtype = tf.float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.ones(shape = (5,4), dtype = tf.int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Random matrices&lt;/h4&gt;
&lt;p&gt;Random matrices are also not part of &lt;code&gt;linalg&lt;/code&gt; library. Rather, they are part of &lt;code&gt;tf.random&lt;/code&gt; library. Using &lt;code&gt;Tensorflow&lt;/code&gt; we can create matrices whose entries come from normal, uniform, poisson, and gamma distributions.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;random_uniform_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;random-uniform-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random uniform matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.uniform(shape = (5,5), minval = 0, maxval = 5, seed= 32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[4.8578553 , 0.26324332, 1.7549878 , 4.434555  , 2.3975224 ],
       [3.219039  , 0.4039365 , 0.92039883, 2.9136662 , 4.9377174 ],
       [4.617196  , 3.6782126 , 4.0351195 , 4.8321657 , 4.206293  ],
       [2.3059547 , 4.922245  , 4.186061  , 2.1761923 , 0.88124394],
       [2.7422066 , 1.5948689 , 2.6099925 , 4.4901986 , 2.4033623 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Random matrix of integers
uniform_int = tf.random.uniform(shape = (5,5), minval= 10, maxval = 20, dtype = tf.int32, seed = 1234)
uniform_int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[19, 15, 13, 14, 10],
       [16, 18, 10, 15, 10],
       [12, 13, 19, 12, 16],
       [18, 11, 10, 18, 12],
       [17, 18, 14, 19, 10]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For further processing we usually require matrix entries to be floating point numbers. This can be achieved by using &lt;code&gt;tf.cast&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.dtypes.cast(uniform_int, dtype = tf.float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[19., 15., 13., 14., 10.],
       [16., 18., 10., 15., 10.],
       [12., 13., 19., 12., 16.],
       [18., 11., 10., 18., 12.],
       [17., 18., 14., 19., 10.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_normal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-normal-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random normal matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.normal(shape = (5,5), mean = 1, stddev= 3, seed = 253)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.5266892 , -5.114835  ,  4.4653835 , -1.013567  , -1.1874261 ],
       [ 5.503375  , -1.4568713 , -1.3270268 ,  0.2747649 ,  3.1374507 ],
       [ 4.211556  ,  4.618066  ,  1.2217634 ,  0.04707384,  1.4131291 ],
       [-2.7024255 ,  0.81293994, -3.11763   , -3.043394  ,  5.5663233 ],
       [ 1.4549919 ,  3.7368293 ,  1.2184538 ,  2.0713992 ,  0.19450545]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;truncated_random_normal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;truncated-random-normal-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Truncated random normal matrix&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;truncated_normal&lt;/code&gt; function gives values within two standard deviations of mean on both sides of normal curve.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.truncated_normal(shape = (5,5), mean = 0, stddev= 2, seed = 82) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2.3130474 ,  1.917585  ,  1.1134342 , -3.6221776 , -2.242488  ],
       [ 2.8108876 , -1.8440692 ,  1.7630143 , -0.4591654 , -0.20763761],
       [-0.4769438 ,  2.3582413 , -0.45690525, -0.4208855 , -1.8990422 ],
       [-2.2638845 ,  2.9536312 ,  0.9591611 ,  2.670887  ,  1.4793464 ],
       [-0.60492915,  3.6320126 ,  3.9752324 , -0.4684417 , -3.2791114 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are ways to create deterministic random numbers using &lt;code&gt;stateless_normal&lt;/code&gt;, &lt;code&gt;stateless_uniform&lt;/code&gt;, etc. To know more about random number generation in &lt;code&gt;Tensorflow&lt;/code&gt;, go to &lt;a href=&#34;https://www.tensorflow.org/guide/random_numbers&#34;&gt;this link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;random_poisson_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-poisson-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random Poisson matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.poisson(shape = (5,5), lam = 2, seed = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 1., 2., 3.],
       [0., 1., 2., 3., 4.],
       [2., 0., 2., 2., 2.],
       [2., 0., 2., 2., 3.],
       [1., 4., 2., 5., 4.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_gamma_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-gamma-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random gamma matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.gamma(shape = (5,5), alpha = 0.7, beta= 0.3, seed = 232)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0.78733766, 2.5200539 , 0.9812998 , 5.141082  , 1.9184761 ],
       [1.1069427 , 0.32923967, 0.13172682, 5.066955  , 2.8487072 ],
       [0.39204285, 0.53647757, 5.3083944 , 1.618826  , 0.41352856],
       [1.0327125 , 0.27330002, 0.34577194, 0.22123706, 0.77021873],
       [0.38616025, 9.153643  , 1.4737413 , 6.029133  , 0.05517024]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_special_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-special-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some special matrices&lt;/h4&gt;
&lt;p&gt;Special matrices like &lt;code&gt;toeplitz&lt;/code&gt;, &lt;code&gt;circulant&lt;/code&gt;, &lt;code&gt;Kronecker&lt;/code&gt;, etc can be created using linear operators. We will discuss this in the &lt;a href=&#34;#special_matrices_using_operators&#34;&gt;linear operator&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;sparse_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sparse-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sparse matrices&lt;/h3&gt;
&lt;p&gt;Sparse matrices are within &lt;code&gt;tf.sparse&lt;/code&gt; library. There are several functions specifically designed for sparse matrices. Full list of function in &lt;code&gt;tf.sparse&lt;/code&gt; library can be found at &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/sparse&#34;&gt;this link&lt;/a&gt;. In this section, we will see how sparse matrices are created. The first argument is set of indices (rows and columns), second argument is the values at those indices. Third argument is the &lt;code&gt;dense_shape&lt;/code&gt; of the sparse matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sparse_mat = tf.sparse.SparseTensor([[0,1],[1,3],[3,2]], [-5, -10, 7], dense_shape= (5,5))
sparse_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f8f505203a0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the actual matrix, we have to convert the sparse matrix to a dense matrix. This is achieved using &lt;code&gt;to_dense&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.sparse.to_dense(sparse_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[  0,  -5,   0,   0,   0],
       [  0,   0,   0, -10,   0],
       [  0,   0,   0,   0,   0],
       [  0,   0,   7,   0,   0],
       [  0,   0,   0,   0,   0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should be noted that special algorithms exist to deal with sparse matrices. Those algorithms don’t require the sparse matrix to be converted into its dense equivalent. By converting a sparse matrix into a dense one, all its special properties are lost. Therefore, sparse matrices should not be converted into dense ones.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_multiplication&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-multiplication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix multiplication&lt;/h3&gt;
&lt;p&gt;To multiply two vectors, or two matrices, or a matrix with a vector in a linear algebra sense, we have to use &lt;code&gt;linalg.matmul&lt;/code&gt; function. Using &lt;code&gt;*&lt;/code&gt; operator in python does element wise multiplication with broadcasting wherever possible. So to multiply two matrices, we have to call &lt;code&gt;linalg.matmul&lt;/code&gt; function. Inputs to &lt;code&gt;linalg.matmul&lt;/code&gt; function are matrices. Therefore, while multiplying two arrays, we have to first convert them into vectors and then multiply. Also note that &lt;code&gt;linalg.matmul&lt;/code&gt; is same as &lt;code&gt;tf.matmul&lt;/code&gt;. Both are aliases.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_column_vectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;multiplying-two-column-vectors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two column vectors&lt;/h4&gt;
&lt;p&gt;Vectors in &lt;code&gt;tensorflow&lt;/code&gt; have only 1 shape parameter, where as a column vector (a matrix with one column) has two shape parameters. For example, a vector &lt;span class=&#34;math inline&#34;&gt;\([1,2,3]\)&lt;/span&gt; has shape &lt;span class=&#34;math inline&#34;&gt;\((3,)\)&lt;/span&gt;, but the column vector &lt;span class=&#34;math inline&#34;&gt;\([1,2,3]^T\)&lt;/span&gt; has shape &lt;span class=&#34;math inline&#34;&gt;\((3,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;inner_product&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;inner-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Inner product&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector_1 = tf.constant([1., 2., 3.], shape = (3,1))
vector_2 = tf.constant([2., 3., 4.], shape = (3,1))
result = tf.matmul(a = vector_1, b = vector_2, transpose_a=True) # Inner product
result.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[20.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;outer_product&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outer-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Outer product&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(a = vector_1, b = vector_2, transpose_b = True) # Outer product&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 2.,  3.,  4.],
       [ 4.,  6.,  8.],
       [ 6.,  9., 12.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_a_matrix_with_a_vector&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-a-matrix-with-a-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying a matrix with a vector&lt;/h4&gt;
&lt;p&gt;There are two ways in which we can achieve this. We can convert the vector into a column vector (matrix with 1 column) and then apply &lt;code&gt;tf.matmul&lt;/code&gt;, or we can use the inbuilt function &lt;code&gt;tf.linalg.matvec&lt;/code&gt; to multiply a matrix with a vector.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat_1 = tf.constant([1,2,3,4,5,6],shape = (2,3), dtype = tf.float32)
mat_1.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(a = mat_1, b = vector_1).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[14.],
       [32.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matvec(mat_1, tf.constant([1,2,3.]))    # Note the shape of input vector and result.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([14., 32.], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-two-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two matrices&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matmul(a = mat, b = mat, transpose_a=True).numpy() # Without `transpose_a` argument, result will be an error.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matmul(a = mat, b = mat, transpose_b=True).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_tri-diagonal_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-two-tri-diagonal-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two tri-diagonal matrices&lt;/h4&gt;
&lt;p&gt;If matrices have sparse structure, usual matrix multiplication is not an efficient method for those type of matrices. Special algorithms are there that exploit the sparsity of the matrices.&lt;/p&gt;
&lt;p&gt;One such sparse matrix is tri-diagonal matrix. It has nonzero entries only on its super-diagonal, main diagonal, and sub-diagonal. To multiply a tri-diagonal matrix with another matrix, we can use &lt;code&gt;tf.linalg.tridiagonal_matmul&lt;/code&gt; function. Its first argument is the diagonals of tri-diagonal matrix and second argument is the matrix with which the tri-diagonal matrix needs to be multiplied.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diagonals = tf.constant([[-1,-1,-1,-1,0],
                         [ 2, 2, 2, 2, 2],
                         [ 0,-1,-1,-1,-1.]])
rhs = tf.constant([[1,2,3],
                   [2,1,3],
                   [4,5,6],
                   [7,8,9],
                   [2,5,4.]])
tf.linalg.tridiagonal_matmul(diagonals, rhs) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 0.,  3.,  3.],
       [-1., -5., -3.],
       [-1.,  1.,  0.],
       [ 8.,  6.,  8.],
       [-3.,  2., -1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify the result by dense matrix multiplication. However, note that this is only for verification. For large matrix multiplications involving tri-diagonal matrix, dense multiplication will be considerably slower.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tridiag_mat = tf.linalg.set_diag(tf.zeros(shape = (5,5)), diagonals, k = (-1,1), align = &amp;quot;LEFT_RIGHT&amp;quot;)
tf.matmul(tridiag_mat, rhs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 0.,  3.,  3.],
       [-1., -5., -3.],
       [-1.,  1.,  0.],
       [ 8.,  6.,  8.],
       [-3.,  2., -1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-multiply-two-tri-diagonal-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to multiply two tri-diagonal matrices?&lt;/h4&gt;
&lt;p&gt;In this case, we have to convert the right tri-diagonal matrix into a full matrix and then multiply it with the left one using only the diagonals of left tri-diagonal matrix. For example, we will multiply the previous tri-diagonal matrix with itself.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tridiagonal_matmul(diagonals, rhs = tridiag_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 5., -4.,  1.,  0.,  0.],
       [-4.,  6., -4.,  1.,  0.],
       [ 1., -4.,  6., -4.,  1.],
       [ 0.,  1., -4.,  6., -4.],
       [ 0.,  0.,  1., -4.,  5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tridiag_mat, tridiag_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 5., -4.,  1.,  0.,  0.],
       [-4.,  6., -4.,  1.,  0.],
       [ 1., -4.,  6., -4.,  1.],
       [ 0.,  1., -4.,  6., -4.],
       [ 0.,  0.,  1., -4.,  5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_common_operations_on_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-common-operations-on-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some common operations on matrices&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;trace&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;trace&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Trace&lt;/h4&gt;
&lt;p&gt;Computes the trace of a tensor. For non-square rank 2 tensors, trace of the main diagonal is computed.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[2,4,6],
                   [5,1,9.]])
tf.linalg.trace(mat).numpy()
              &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;determinant&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;determinant&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Determinant&lt;/h4&gt;
&lt;p&gt;Computes the determinant of the matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = -2*tf.linalg.diag([1,2,3.])
tf.linalg.det(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=-48.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;rank&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rank&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Rank&lt;/h4&gt;
&lt;p&gt;Computes the rank of a matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,4,5],
                 [3,2,5],
                 [2,1,3.]])
rank = tf.linalg.matrix_rank(A)
print(&amp;quot;Rank of A = &amp;quot;, rank.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rank of A =  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_inverse&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-inverse&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix inverse&lt;/h4&gt;
&lt;p&gt;Computes the matrix inverse if it exists. It uses &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition to calculate inverse. What happens if inverse doesn’t exist? Here is the answer taken directly from &lt;code&gt;tensorflow&lt;/code&gt; documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[…] If a matrix is not invertible there is no guarantee what the op does. It may detect the condition and raise an exception or it may simply return a garbage result. […]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having read the documentation, we will apply the function to an invertible matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[2, 2, 3],
                 [4,5,6],
                 [1,2,4.]])
A_inv = tf.linalg.inv(A)
print(A_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[ 1.5999998e+00 -3.9999992e-01 -6.0000008e-01]
 [-1.9999999e+00  9.9999994e-01  7.9472862e-08]
 [ 5.9999996e-01 -3.9999998e-01  3.9999998e-01]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,A_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 9.9999970e-01,  1.1920929e-07, -1.1920929e-07],
       [-5.9604645e-07,  1.0000002e+00,  0.0000000e+00],
       [-2.3841858e-07,  0.0000000e+00,  1.0000000e+00]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inverse-using-lu-factors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Inverse using &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; factors&lt;/h4&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition result is already available from some prior computation, it can be used to compute the inverse using command &lt;code&gt;tf.linalg.lu_matrix_inverse&lt;/code&gt;. This command basically solves the following system:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[AX=I \Leftrightarrow LUX=I\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are lower and upper triangular respectively, two triangular systems can be solved to obtain &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; which is nothing but &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;. The triangular systems are &lt;span class=&#34;math inline&#34;&gt;\(LY = I\)&lt;/span&gt; (this gives &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as result) and &lt;span class=&#34;math inline&#34;&gt;\(UX=Y\)&lt;/span&gt; (this gives &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; as result). Here, the right hand side has more than one column in both the triangular systems. We will see how to solve those triangular systems at a &lt;a href=&#34;#solving_triangular_systems&#34;&gt;later section&lt;/a&gt;. For the time being, we will just compute the inverse using the built-in command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lu, p = tf.linalg.lu(A)
A_inv_by_lu = tf.linalg.lu_matrix_inverse(lu,p)
A_inv_by_lu&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.5999998e+00, -3.9999992e-01, -6.0000008e-01],
       [-1.9999999e+00,  9.9999994e-01,  7.9472862e-08],
       [ 5.9999996e-01, -3.9999998e-01,  3.9999998e-01]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;extract_diagonals_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-diagonals-of-a-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Extract diagonals of a matrix&lt;/h4&gt;
&lt;p&gt;Diagonals of a matrix can be extracted using &lt;code&gt;tf.linalg.diag_part&lt;/code&gt; function. Diagonal entries are obtained by setting &lt;code&gt;k=0&lt;/code&gt; which is the default. By setting &lt;code&gt;k&lt;/code&gt; to any other value, either sub-diagonal or super-diagonal can be obtained.If two values are given to &lt;code&gt;k&lt;/code&gt;, the values correspond respectively to the lower limit and upper limit of the diagonal. And the result contains all diagonals within those limits. The result is not a matrix. It is an array of diagonals, appended if required. Sub-diagonals are appended at the right and super diagonals are appended at the left.&lt;/p&gt;
&lt;p&gt;Another function &lt;code&gt;tf.linalg.tensor_diag_part&lt;/code&gt; can be used to extract the main diagonal of the matrix. But it can extract only the main diagonal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.random.uniform(shape = (5,5), minval = 1, maxval = 20, dtype = tf.int32)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[10, 11,  3, 18,  6],
       [18,  1, 16, 14, 12],
       [ 4, 18, 12, 17,  1],
       [12,  7,  5,  3,  7],
       [ 9, 16, 11, 14,  8]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([10,  1, 12,  3,  8], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tensor_diag_part(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([10,  1, 12,  3,  8], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat, k = (-1,0)).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[10,  1, 12,  3,  8],
       [18, 18,  5, 14,  0]], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat, k = [-2,1])  # 2 subdiagonals, main diagonal, and 1 super diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=int32, numpy=
array([[ 0, 11, 16, 17,  7],
       [10,  1, 12,  3,  8],
       [18, 18,  5, 14,  0],
       [ 4,  7, 11,  0,  0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;extract_band_part_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-band-part-of-a-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Extract band part of a matrix&lt;/h4&gt;
&lt;p&gt;A band matrix is one that has nonzero values along its diagonal and a few sub-diagonals and super-diagonals. All other entries are zero. It is a sparse matrix. All of its nonzero entries are concentrated in a band along the diagonal. For example, tri-diagonal matrix is a banded matrix. It has lower bandwidth of 1 and upper bandwidth of 1. It is possible for a matrix to have different upper and lower bandwidths. It is still called a banded matrix.&lt;/p&gt;
&lt;p&gt;Banded matrices are useful because computations are significantly faster using these matrices as compared to dense matrices of same shape. If for some application, we want the band part of a matrix, we can use &lt;code&gt;linalg.band_part&lt;/code&gt; function to extract it. This function takes three arguments (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;num_lower&lt;/code&gt;, &lt;code&gt;num_upper&lt;/code&gt;). First argument is the tensor whose band part we want to extract. Second argument is the number of sub-diagonals to keep. If set to 0, no sub-diagonal is kept. &lt;code&gt;num_lower = -1&lt;/code&gt; keeps all the sub-diagonals. Similarly for &lt;code&gt;num_upper&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix = tf.constant(tf.range(25, dtype=tf.float32), shape=(5,5))
matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.],
       [20., 21., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.band_part(matrix, num_lower = 2, num_upper = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  1.,  0.,  0.,  0.],
       [ 5.,  6.,  7.,  0.,  0.],
       [10., 11., 12., 13.,  0.],
       [ 0., 16., 17., 18., 19.],
       [ 0.,  0., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.band_part(matrix, num_lower = -1, num_upper = 0)  # Lower triangular part&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 5.,  6.,  0.,  0.,  0.],
       [10., 11., 12.,  0.,  0.],
       [15., 16., 17., 18.,  0.],
       [20., 21., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_factorizations&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-factorizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix factorizations&lt;/h2&gt;
&lt;p&gt;Some of the most common and widely used matrix factorizations are available in &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;lu&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;lu&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LU&lt;/h3&gt;
&lt;p&gt;Matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is factorized into a unit lower triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and an upper triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, such that &lt;span class=&#34;math display&#34;&gt;\[A=LU\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To reduce round-off errors, partial pivoting is used. In partial pivoting, the following factorization is done.
&lt;span class=&#34;math display&#34;&gt;\[PA = LU\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is called the permutation matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1, 4, 7, 8],
                [24, -5, -13, 9],
                [-7, 21, 8, 19],
                [0, 18, 6, 4]], dtype = tf.float32)
lu, p = tf.linalg.lu(A)  # As per documentation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;LU = &amp;quot;)
print(lu)
print()
print(&amp;quot;P = &amp;quot;)
print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LU = 
tf.Tensor(
[[ 24.          -5.         -13.           9.        ]
 [ -0.29166666  19.541666     4.2083335   21.625     ]
 [  0.04166667   0.21535183   6.635394     2.9680166 ]
 [  0.           0.9211088    0.32005137 -16.868896  ]], shape=(4, 4), dtype=float32)

P = 
tf.Tensor([1 2 0 3], shape=(4,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the above result mean? Well, both &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; matrices have been merged into one. And &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; contains permutation indices. In practice, we don’t have to reconstruct individual matrices &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, because &lt;code&gt;tensorflow&lt;/code&gt; has built-in functions for further analysis that uses the result of &lt;code&gt;tf.linalg.lu&lt;/code&gt; as given above. For the sake of demonstration, we will show how to reconstruct those matrices from above result. To reconstruct &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, we will use a linear operator that is discussed next. After constructing the matrices &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[A = P^TLU\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))
U = tf.linalg.band_part(lu, 0, -1)
permu_operator = tf.linalg.LinearOperatorPermutation(p)
P = permu_operator.to_dense()
print(&amp;quot;L:&amp;quot;)
print(L)
print()
print(&amp;quot;U:&amp;quot;)
print(U)
print()
print(&amp;quot;P:&amp;quot;)
print(P)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;L:
tf.Tensor(
[[ 1.          0.          0.          0.        ]
 [-0.29166666  1.          0.          0.        ]
 [ 0.04166667  0.21535183  1.          0.        ]
 [ 0.          0.9211088   0.32005137  1.        ]], shape=(4, 4), dtype=float32)

U:
tf.Tensor(
[[ 24.         -5.        -13.          9.       ]
 [  0.         19.541666    4.2083335  21.625    ]
 [  0.          0.          6.635394    2.9680166]
 [  0.          0.          0.        -16.868896 ]], shape=(4, 4), dtype=float32)

P:
tf.Tensor(
[[0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(P, tf.matmul(L,U), transpose_a = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[  1.       ,   4.0000005,   7.       ,   8.       ],
       [ 24.       ,  -5.       , -13.       ,   9.       ],
       [ -7.       ,  21.       ,   8.       ,  19.       ],
       [  0.       ,  18.       ,   6.       ,   3.999998 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily reconstruct our original matrix from &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; factors using the function &lt;code&gt;tf.linalg.lu_reconstruct&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.lu_reconstruct(lu,p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[  1.       ,   4.0000005,   7.       ,   8.       ],
       [ 24.       ,  -5.       , -13.       ,   9.       ],
       [ -7.       ,  21.       ,   8.       ,  19.       ],
       [  0.       ,  18.       ,   6.       ,   3.999998 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;cholesky&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cholesky&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cholesky&lt;/h3&gt;
&lt;p&gt;It is defined for symmetric positive definite matrices. If A is a symmetric positive definite matrix, its Cholesky decomposition can be written as:
&lt;span class=&#34;math display&#34;&gt;\[ A = LL^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is a lower triangular matrix.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Cholesky decomposition is used as a test for positive definiteness of a matrix. If Cholesky decomposition succeeds, the matrix is positive definite, otherwise it is not. Another widely reported test for positive definiteness is to check the signs of all eigenvalues of the matrix. If all the eigenvalues are positive, the matrix is positive definite. But this method requires computation of all eigenvalues which is computationally nontrivial. Therefore, Cholesky decomposition is the preferred method to test for positive definiteness. If the matrix is not positive definite, Cholesky decomposition will stop at an intermediate step. On the other hand, if it succeeds, along with verifying positive definiteness of the matrix, we will get the Cholesky factor as a byproduct. Cholesky factor can then be used to solve linear systems as we will see later.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,14]], dtype = tf.float32)
L = tf.linalg.cholesky(A)
L&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [1., 2., 0.],
       [1., 2., 3.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(L,L,transpose_b=True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  1.,  1.],
       [ 1.,  5.,  5.],
       [ 1.,  5., 14.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;qr&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;QR&lt;/h3&gt;
&lt;p&gt;Given a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(QR\)&lt;/span&gt; decomposition decomposes the matrix into an orthogonal matrix &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and an upper triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; such that product of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; gives back &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are an orthogonal basis for the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (also known as range of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
Q,R = tf.linalg.qr(A)
print(&amp;quot;Q:&amp;quot;)
print(Q)
print()
print(&amp;quot;R:&amp;quot;)
print(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Q:
tf.Tensor(
[[-0.18257415  0.4079837 ]
 [-0.36514837 -0.44398218]
 [-0.5477225  -0.575977  ]
 [-0.73029673  0.5519779 ]], shape=(4, 2), dtype=float32)

R:
tf.Tensor(
[[-5.477226  -4.7469287]
 [ 0.         2.7778888]], shape=(2, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get full &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; matrices by setting &lt;code&gt;full_matrices = True&lt;/code&gt; in the argument.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_full, R_full = tf.linalg.qr(A, full_matrices = True)
print(&amp;quot;Q full:&amp;quot;)
print(Q_full)
print()
print(&amp;quot;R full:&amp;quot;)
print(R_full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Q full:
tf.Tensor(
[[-0.18257415  0.4079837  -0.17102492 -0.8780469 ]
 [-0.36514837 -0.44398218 -0.81774735  0.02891004]
 [-0.5477225  -0.575977    0.54808205 -0.2604928 ]
 [-0.73029673  0.5519779   0.04056833  0.4004264 ]], shape=(4, 4), dtype=float32)

R full:
tf.Tensor(
[[-5.477226  -4.7469287]
 [ 0.         2.7778888]
 [ 0.         0.       ]
 [ 0.         0.       ]], shape=(4, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;svd&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;svd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SVD&lt;/h3&gt;
&lt;p&gt;Singular value decomposition (SVD) of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in R^{m\times n}\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[A = U\Sigma V^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(U\in R^{m\times m}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\in R^{n\times n}\)&lt;/span&gt; are orthogonal matrices, commonly known as left and right singular vectors respectively. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma \in R^{m\times n}\)&lt;/span&gt; is a diagonal matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6],
                   [7,8,9.]])
s,u,v = tf.linalg.svd(mat)
print(&amp;quot;S:&amp;quot;)
print(s)
print()
print(&amp;quot;U:&amp;quot;)
print(u)
print()
print(&amp;quot;V:&amp;quot;)
print(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S:
tf.Tensor([18.604359   5.459675   2.4636664], shape=(3,), dtype=float32)

U:
tf.Tensor(
[[ 0.2936678   0.40458775  0.7340845 ]
 [ 0.48711583 -0.7956307   0.01849233]
 [ 0.34406567  0.418864   -0.67870086]
 [ 0.7470583   0.16683212  0.01195723]], shape=(4, 3), dtype=float32)

V:
tf.Tensor(
[[ 0.4678568   0.5231253   0.71235543]
 [ 0.6254436  -0.76545155  0.15134181]
 [ 0.62444425  0.3747316  -0.685307  ]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a truncated SVD. To get full SVD decomposition, we have to set &lt;code&gt;full_matrices = True&lt;/code&gt; in the argument.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;s_full,u_full,v_full = tf.linalg.svd(mat, full_matrices = True)
print(&amp;quot;S full:&amp;quot;)
print(s_full)
print()
print(&amp;quot;U full:&amp;quot;)
print(u_full)
print()
print(&amp;quot;V full:&amp;quot;)
print(v_full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S full:
tf.Tensor([18.604359   5.459675   2.4636664], shape=(3,), dtype=float32)

U full:
tf.Tensor(
[[ 0.2936678   0.40458775  0.7340845  -0.45955172]
 [ 0.48711583 -0.7956307   0.01849233 -0.35964906]
 [ 0.34406567  0.418864   -0.67870086 -0.4955166 ]
 [ 0.7470583   0.16683212  0.01195723  0.6433724 ]], shape=(4, 4), dtype=float32)

V full:
tf.Tensor(
[[ 0.4678568   0.5231253   0.71235543]
 [ 0.6254436  -0.76545155  0.15134181]
 [ 0.62444425  0.3747316  -0.685307  ]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If only singular values are of interest, it can be computed without computing singular vectors. In this way, computations can be much faster.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.svd(mat, compute_uv=False).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([18.604359 ,  5.459675 ,  2.4636664], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;eigenvalues_and_eigenvectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalues-and-eigenvectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Eigenvalues and eigenvectors&lt;/h2&gt;
&lt;p&gt;Symmetry is an important consideration while computing eigenvalues and eigenvectors. For symmetric matrices, different set of algorithms are used for eigen analysis that exploit the symmetry of the matrix. Therefore, two functions are available in &lt;code&gt;tensorflow&lt;/code&gt; for eigen analysis. &lt;code&gt;eig&lt;/code&gt; is used to compute eigenvalues and eigenvectors of a dense matrix without any special structure. &lt;code&gt;eigh&lt;/code&gt; is used for eigen analysis of &lt;code&gt;Hermitian&lt;/code&gt; matrices. If only eigenvalues are of interest, &lt;code&gt;eigvals&lt;/code&gt; and &lt;code&gt;eigvalsh&lt;/code&gt; can be used compute just eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;eigen-analysis_of_hermitian_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;eigen-analysis-of-hermitian-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigen-analysis of Hermitian matrices&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[3,1,1],
                 [1,2,1],
                 [1,1,2.]])
values, vectors = tf.linalg.eigh(A)
print(&amp;quot;Eigenvalues:&amp;quot;)
print(values.numpy())
print()
print(&amp;quot;Eigenvectors:&amp;quot;)
print(vectors.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Eigenvalues:
[1.        1.5857866 4.4142137]

Eigenvectors:
[[ 0.         -0.7071068   0.70710665]
 [-0.70710677  0.49999994  0.50000006]
 [ 0.7071068   0.4999999   0.5       ]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is an eigenvector.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matvec(A, vectors[0,:]).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([-1.7881393e-07, -7.0710701e-01,  7.0710647e-01], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are accurate up to 5 decimal digits.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eigvalsh(A).numpy()    # Just eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.       , 1.5857866, 4.4142137], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-happens-if-you-pass-a-nonsymmetric-matrix-to-eigh-by-mistake&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What happens if you pass a nonsymmetric matrix to &lt;code&gt;eigh&lt;/code&gt; by mistake?&lt;/h4&gt;
&lt;p&gt;Well, while using &lt;code&gt;eigh&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; assumes the matrix to be symmetric. &lt;code&gt;Tensorflow&lt;/code&gt; doesn’t check whether the matrix is symmetric or not. It just takes the lower triangular part, assumes that the upper triangular part is same because of symmetry and performs the computations. So be prepared to get a wrong result!&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;eigen-analysis_of_non-Hermitian_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;eigen-analysis-of-non-hermitian-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigen-analysis of non-Hermitian matrices&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,-5,3],
                 [2,4,-7],
                 [3,9,-2.]])
values, vectors = tf.linalg.eig(A)
print(&amp;quot;Eigenvalues:&amp;quot;)
print(values.numpy())
print()
print(&amp;quot;Eigenvectors:&amp;quot;)
print(vectors.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Eigenvalues:
[2.7560833 -7.9942424e-08j 0.12195918-7.5705280e+00j
 0.12195931+7.5705280e+00j]

Eigenvectors:
[[ 0.06142625+0.95019215j  0.16093381-0.34744066j  0.13818482+0.35709903j]
 [-0.01236177-0.19122148j  0.3560446 +0.53046155j  0.38952374-0.5063876j ]
 [ 0.01535368+0.23750281j -0.33046415+0.5796737j  -0.29238018-0.59978485j]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only eigenvalues.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eigvals(A).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.7560833 -7.9942424e-08j, 0.12195918-7.5705280e+00j,
       0.12195931+7.5705280e+00j], dtype=complex64)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-happens-when-you-pass-a-symmetric-matrix-to-eig&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What happens when you pass a symmetric matrix to &lt;code&gt;eig&lt;/code&gt;?&lt;/h4&gt;
&lt;p&gt;Nothing! We will still get the correct answer. &lt;code&gt;Tensorflow&lt;/code&gt; will use more operations to compute results when it could have been done using less computations.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;solving_dense_linear_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-dense-linear-systems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving dense linear systems&lt;/h2&gt;
&lt;p&gt;A linear system is (usually) written as &lt;span class=&#34;math display&#34;&gt;\[ Ax = b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be square or rectangular. In our case, it is dense, i.e., most of its entries are nonzero. Right hand side &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a vector in this case. If we have to solve the linear system for multiple right hand side vectors involving same &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, the RHS can be replaced by a matrix whose columns are different RHS vectors.&lt;/p&gt;
&lt;p&gt;Depending on the structure of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (whether triangular, or tri-diagonal, or positive definite), suitable algorithm is chosen to solve the linear system. &lt;code&gt;Tensorflow&lt;/code&gt; has a function &lt;code&gt;tf.linalg.solve&lt;/code&gt; to solve linear systems. But this function doesn’t take into account the special structure of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
tf.linalg.solve(A,b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;using_lu_decomposition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;using-lu-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using LU decomposition&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition factors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are known, those can be used to solve the linear system.
For example, solve:
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
  1 &amp;amp; 1 &amp;amp; 1\\
  1 &amp;amp; 5 &amp;amp; 5\\
  1 &amp;amp; 5 &amp;amp; 13
 \end{pmatrix} x= 
 \begin{pmatrix}
 3\\
 11\\
 20\\
 \end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
lu, p = tf.linalg.lu(A)   # Factorization result of LU
x_sol_lu = tf.linalg.lu_solve(lu,p,b)
x_sol_lu&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,x_sol_lu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[ 3.],
       [11.],
       [20.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have obtained factors &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, we can use &lt;code&gt;tf.linalg.triangular_solve&lt;/code&gt; to solve the linear system by solving following two triangular system. &lt;span class=&#34;math display&#34;&gt;\[Ly = b\]&lt;/span&gt;
and &lt;span class=&#34;math display&#34;&gt;\[Ux = y\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))
y = tf.linalg.triangular_solve(L,b)    # Solves Ly = b
x = tf.linalg.triangular_solve(lu,y, lower = False)  # Solves Ux = y
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;using_cholesky_decomposition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-cholesky-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using Cholesky decomposition&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive definite, Cholesky decomposition is an efficient method for solving the linear system. For positive definite &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, Cholesky decomposition requires fewer computations than &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition. This is because it exploits the symmetry of the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Once the Cholesky factor &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is found, we solve the linear system by solving two triangular systems. Solving triangular systems only requires &lt;span class=&#34;math inline&#34;&gt;\(O(n^2)\)&lt;/span&gt; operations.
&lt;span class=&#34;math display&#34;&gt;\[ LL^Tx = b\]&lt;/span&gt;
Two triangular systems are:
&lt;span class=&#34;math display&#34;&gt;\[ Ly = b\]&lt;/span&gt;
This gives us &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Using &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; we solve for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; using the following equation.
&lt;span class=&#34;math display&#34;&gt;\[L^Tx = y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;tensorflow&lt;/code&gt;, we solve the system using &lt;code&gt;tf.linalg.cholesky_solve&lt;/code&gt;. It takes cholesky factor &lt;span class=&#34;math inline&#34;&gt;\((L)\)&lt;/span&gt; and right hand side &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; as input.&lt;/p&gt;
&lt;p&gt;For example, solve:
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
  1 &amp;amp; 1 &amp;amp; 1\\
  1 &amp;amp; 5 &amp;amp; 5\\
  1 &amp;amp; 5 &amp;amp; 13
 \end{pmatrix} x= 
 \begin{pmatrix}
 3\\
 11\\
 20\\
 \end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
L = tf.linalg.cholesky(A)
sol_chol = tf.linalg.cholesky_solve(L, b)
sol_chol&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.0000001],
       [0.8750001],
       [1.1249999]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,sol_chol)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[ 3.      ],
       [11.      ],
       [19.999998]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_structured_linear_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-structured-linear-systems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving structured linear systems&lt;/h2&gt;
&lt;p&gt;If coefficient matrix of a linear system has some kind of structure (triangular, banded, tridiagonal, etc.), it can be efficiently solved using far fewer computations than required by dense &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition. Special solvers exist that exploit the structure of the coefficient matrix thus reducing flop count considerably for large structured systems. Linear algebra library of &lt;code&gt;Tensorflow&lt;/code&gt; implements three such specialized solvers: &lt;code&gt;Triangular solver&lt;/code&gt;, &lt;code&gt;Tridiagonal solver&lt;/code&gt;, and &lt;code&gt;Banded triangular solver&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Structured solvers have immensely useful as in many practical applications resulting coefficient matrix has some kind of structure. This is especially true in mechanical applications, such as finite element modeling, computational heat transfer, etc. In finite element modeling, a body is divided into several elements and each element has certain number of nodes. A node is connected to only a few neighboring nodes. So interaction of a node is limited to only those neighboring nodes. This results in banded systems. Other types of structured matrices are also common in applications.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;solving_triangular_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-triangular-systems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving triangular systems&lt;/h2&gt;
&lt;p&gt;We have already used triangular solvers in the section on &lt;a href=&#34;#using_lu_decomposition&#34;&gt;LU decomposition&lt;/a&gt;. For completeness, we will again describe it in this section. &lt;code&gt;tf.linalg.triangular_solve&lt;/code&gt; solves both &lt;code&gt;lower triangular&lt;/code&gt; and &lt;code&gt;upper triangular&lt;/code&gt; systems. Following examples demonstrate that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lower_triangular_mat = tf.constant([[1,0,0],
                                    [2,3,0],
                                    [4,5,6]], dtype = tf.float32)
rhs = tf.constant([[7],[8],[9]], dtype = tf.float32)
res_lower_triangular = tf.linalg.triangular_solve(matrix = lower_triangular_mat, rhs = rhs, lower = True)
res_lower_triangular&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[ 7. ],
       [-2. ],
       [-1.5]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same solver to solve upper triangular system also. This is done by setting &lt;code&gt;adjoint = True&lt;/code&gt; along with &lt;code&gt;lower = True&lt;/code&gt;. &lt;code&gt;lower = True&lt;/code&gt; extracts the lower triangular part of the coefficient matrix and &lt;code&gt;adjoint = True&lt;/code&gt; solves the triangular system by transposing the extracted lower triangular part. For real matrices adjoint is same as transpose. By setting the arguments as above, essentially we solve the upper triangular system.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;res_upper_triangular = tf.linalg.triangular_solve(matrix = lower_triangular_mat, rhs = rhs, lower = True, adjoint = True)
res_upper_triangular&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.6666665 ],
       [0.16666667],
       [1.5       ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verity the above result by multiplying the transpose of the lower triangular matrix by &lt;code&gt;res_upper_triangular&lt;/code&gt;. This indeed gives us the right hand side &lt;span class=&#34;math inline&#34;&gt;\(([7,8,9]^T)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(lower_triangular_mat, res_upper_triangular, transpose_a=True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[7.],
       [8.],
       [9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_tri-diagonal_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;solving-tri-diagonal-systems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving tri-diagonal systems&lt;/h3&gt;
&lt;p&gt;If matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is tri-diagonal, &lt;code&gt;tf.linalg.tridiagonal_solve&lt;/code&gt; can be used to solve the linear system efficiently. For example, we will solve the following tri-diagonal system.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
2 &amp;amp; -1 &amp;amp; 0 &amp;amp;0 &amp;amp; 0\\
-1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; -1 &amp;amp; 2&amp;amp; -1&amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 2 &amp;amp; -1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 2
\end{pmatrix}x=
\begin{pmatrix}
3\\
4\\
-5\\
7\\
9\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1,-1,-1,-1, 0],
                     [ 2, 2, 2, 2, 2],
                     [ 0,-1,-1,-1,-1]], dtype = tf.float32)
b = tf.constant([3,4,-5,7,9.],shape = (5,1))
x = tf.linalg.tridiagonal_solve(diagonals = diags, rhs = b)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 6.5000005],
       [10.000001 ],
       [ 9.500001 ],
       [14.       ],
       [11.5      ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tridiagonal_matmul(diagonals=diags, rhs = x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 3.      ],
       [ 4.000001],
       [-4.999999],
       [ 7.      ],
       [ 9.      ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_banded_triangular_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-banded-triangular-systems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving banded triangular systems&lt;/h3&gt;
&lt;p&gt;Solving an &lt;span class=&#34;math inline&#34;&gt;\((n \times n)\)&lt;/span&gt; full upper triangular (or lower triangular) system uses &lt;span class=&#34;math inline&#34;&gt;\(n^2\)&lt;/span&gt; flops (including addition/subtraction and multiplication/division). But if the upper triangular (or lower triangular) system is banded with, say, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; bands (including main diagonal), the system can be solved in &lt;span class=&#34;math inline&#34;&gt;\(\approx(2k-1)n\)&lt;/span&gt; flops. If &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is not very large, banded triangular systems can be solved at a fraction of computational cost as compared to dense triangular systems.&lt;/p&gt;
&lt;p&gt;However, please keep in mind that flop count is not the only criteria that determines the computational time of a system. For very very large systems, communication time between different parts of computer can be significant as compared to processing required number of flops for that system.&lt;/p&gt;
&lt;p&gt;We can solve both &lt;code&gt;banded lower triangular&lt;/code&gt; and &lt;code&gt;banded upper triangular&lt;/code&gt; systems. To illustrate this we will use the following example.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
6 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 7 &amp;amp; 3 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 8 &amp;amp; 4 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 9 &amp;amp; 5
\end{pmatrix}x=
\begin{pmatrix}
3\\
4\\
-5\\
7\\
9\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As the matrix is banded, we need not create the full dense matrix. Instead, we can only pass the band part to the solver. Subdiagonals (or superdiagonals) contain less number of entries than main diagonal. While passing the band part to &lt;code&gt;tensorflow&lt;/code&gt;, these subdiagonals (or superdiagonals) have to be appended either at left or right to make it of the same length as main diagonal. Whether the subdiagonals or superdiagonals will be appended at the left or right, is decided by the alignment strategy. There are four types of &lt;code&gt;alignment strategies&lt;/code&gt; used in &lt;code&gt;tensorflow&lt;/code&gt;. Those are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LEFT_LEFT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at right&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at right&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LEFT_RIGHT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at right&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at left&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RIGHT_LEFT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at left&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at right&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RIGHT_RIGHT&lt;/code&gt;: Superdiagonals are &lt;code&gt;appended at left&lt;/code&gt; and subdiagonals are &lt;code&gt;appended at left&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One way to remember the above rules is that if something is aligned to left, it is appended at right. And in &lt;code&gt;LEFT_RIGHT&lt;/code&gt;, first word corresponds to superdiagonals and second word corresponds to subdiagonal. For &lt;code&gt;tf.linalg.banded_triangular_solve&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; assumes that input band part has &lt;code&gt;LEFT_RIGHT&lt;/code&gt; alignment. Band part inputs to this function can be modified accordingly. Above problem can be solved as follows.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;bands = tf.constant([[1,2,3,4,5],
                     [0,6,7,8,9]], dtype = tf.float32)  # Because of LEFT_RIGHT alignment as we have a subdiagonal here
rhs = tf.constant([[3],
                   [4],
                   [-5],
                   [7],
                   [9]], dtype = tf.float32)
banded_lower_triangular_sol = tf.linalg.banded_triangular_solve(bands = bands, rhs = rhs, lower = True)
banded_lower_triangular_sol&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[  3.      ],
       [ -7.      ],
       [ 14.666667],
       [-27.583334],
       [ 51.45    ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify the above solution by multiplying the dense banded lower triangular matrix by the solution. This should give us the RHS.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dense_banded_lower_triangular_mat = tf.linalg.set_diag(tf.zeros(shape = (5,5)), diagonal = bands,
                                                       k = (-1,0), align = &amp;quot;LEFT_RIGHT&amp;quot;)
dense_banded_lower_triangular_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [6., 2., 0., 0., 0.],
       [0., 7., 3., 0., 0.],
       [0., 0., 8., 4., 0.],
       [0., 0., 0., 9., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(dense_banded_lower_triangular_mat, banded_lower_triangular_sol)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 3.],
       [ 4.],
       [-5.],
       [ 7.],
       [ 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly we can solve banded upper triangular matrix. For illustration, we will use the transpose of the above coefficient matrix. We will use the same right hand side vector.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
1 &amp;amp; 6 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 2 &amp;amp; 7 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 3 &amp;amp; 8 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 4 &amp;amp; 9\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 5
\end{pmatrix}x=
\begin{pmatrix}
3\\
4\\
-5\\
7\\
9\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;bands = tf.constant([[6,7,8,9,0],
                     [1,2,3,4,5]], dtype = tf.float32)  # Because of LEFT_RIGHT alignment as we have a superdiagonal here
rhs = tf.constant([[3],
                   [4],
                   [-5],
                   [7],
                   [9]], dtype = tf.float32)
banded_upper_triangular_sol = tf.linalg.banded_triangular_solve(bands = bands, rhs = rhs, lower = False)
banded_upper_triangular_sol&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 84.79998  ],
       [-13.63333  ],
       [  4.4666657],
       [ -2.2999997],
       [  1.8      ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will verify the solution.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dense_banded_upper_triangular_mat = tf.linalg.set_diag(tf.zeros(shape = (5,5)), diagonal = bands,
                                                       k = (0,1), align = &amp;quot;LEFT_RIGHT&amp;quot;)
dense_banded_upper_triangular_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 6., 0., 0., 0.],
       [0., 2., 7., 0., 0.],
       [0., 0., 3., 8., 0.],
       [0., 0., 0., 4., 9.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(dense_banded_upper_triangular_mat, banded_upper_triangular_sol)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 3.],
       [ 4.],
       [-5.],
       [ 7.],
       [ 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_least_squares_problems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-least-squares-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving least squares problems&lt;/h2&gt;
&lt;p&gt;&lt;a id = &#34;ordinary_least_squares&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;ordinary-least-squares&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordinary least squares&lt;/h3&gt;
&lt;p&gt;Both over determined and under determined least squares problem can be solved using the command &lt;code&gt;tf.linalg.lstsq&lt;/code&gt;. In the underdetermined case, the output is the least norm solution. Least squares problem can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, we try to find an &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that the residual error is as small as possible.
For example, we will solve following two problems.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
1 &amp;amp; 2\\
2 &amp;amp; 0.5\\
3 &amp;amp; 1\\
4 &amp;amp; 5\\
\end{pmatrix}x_{over}=
\begin{pmatrix}
3\\
4\\
5\\
6\\
\end{pmatrix}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
3 &amp;amp; 1 &amp;amp; 2 &amp;amp; 5\\
7 &amp;amp; 9 &amp;amp; 1 &amp;amp; 4
\end{pmatrix}x_{under}=
\begin{pmatrix}
7.2\\
-5.8\\
\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
A_under = tf.constant([[3,1,2,5],[7,9,1,4.]])
b_over = tf.constant([3,4,5,6.], shape = (4,1))
b_under = tf.constant([7.2,-5.8], shape = (2,1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_over = tf.linalg.lstsq(A_over, b_over)
x_over&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704103  ],
       [-0.04319588]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though it is not advisable, for this simple case, we will directly apply normal equation to get the solution.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.inv(tf.matmul(A_over,A_over, transpose_a = True)), tf.matmul(A_over,b_over, transpose_a = True))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704104  ],
       [-0.04319668]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_under = tf.linalg.lstsq(A_under, b_under)
x_under&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355565 ],
       [ 0.699703  ],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will computer the least norm solution for underdetermined case using the closed form solution. However, it should be remembered that it is not advisable to do so in practice for large systems.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A_under,tf.matmul(tf.linalg.inv(tf.matmul(A_under, A_under, transpose_b = True)), b_under), transpose_a = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355561 ],
       [ 0.6997029 ],
       [ 1.4518325 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;regularized_least_squares&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularized-least-squares&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularized least squares&lt;/h3&gt;
&lt;p&gt;Only &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt; regularization is supported. The following regularized problem is solved.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2^2 + \lambda \|x\|_2^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a hyperparameter. Usually several values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; are tried over a logarithmic scale before choosing the best one.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_over_reg = tf.linalg.lstsq(A_over, b_over, l2_regularizer= 2.0)
x_over_reg.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.3890449 ],
       [0.21348318]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_under_reg = tf.linalg.lstsq(A_under, b_under, l2_regularizer=2.)
x_under_reg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04763567],
       [-1.214508  ],
       [ 0.62748903],
       [ 1.299031  ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_specialized_operations&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-specialized-operations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some specialized operations&lt;/h2&gt;
&lt;p&gt;&lt;a id = &#34;norm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;norm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Norm&lt;/h3&gt;
&lt;p&gt;Norm can be defined for vectors as well as matrices. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; norm of vector is defined as
&lt;span class=&#34;math display&#34;&gt;\[\|x\|_p = (\Sigma_{i=1}^{n}|x_i|^p)^\frac{1}{p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Matrix is norm is defined as
&lt;span class=&#34;math display&#34;&gt;\[\|A\|_p= \max_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tensorflow&lt;/code&gt; supports all the usual vector and matrix norms that are used in practice. Using only &lt;code&gt;tensorflow&lt;/code&gt; we can calculate all norms except &lt;code&gt;infinity&lt;/code&gt; norm. To calculate &lt;code&gt;infinity&lt;/code&gt; norm we have to use &lt;code&gt;ord = np.inf&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3.]), ord = &amp;quot;euclidean&amp;quot;).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.7416575&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3]), ord = 1).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fractional norms for &lt;strong&gt;vectors&lt;/strong&gt; are also supported.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3.]), ord = 0.75).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.46176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,8, 2,3],
                 [2,7,6,5],
                 [0,3,2,8.]])
mat_norm_2 = tf.linalg.norm(A, ord = 2, axis = [0,1])
print(&amp;quot;2 norm of matrix A = &amp;quot;, mat_norm_2.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_23/lib/python3.8/site-packages/tensorflow/python/ops/linalg_ops.py:735: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to tf.sets.difference().
2 norm of matrix A =  15.294547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2 norm of a matrix is equivalent to the largest singular value of the matrix. We will verify that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vals,_ ,_ = tf.linalg.svd(A)
tf.math.reduce_max(vals).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15.294547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;normalizing_a_tensor&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normalizing-a-tensor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normalizing a tensor&lt;/h3&gt;
&lt;p&gt;Computes the norm and normalizes the tensor using that norm. By normalize we mean, divide the entries of the tensor by the norm. Here, we will consider a matrix. But the method can be extended to multi-dimensional tensor.&lt;/p&gt;
&lt;p&gt;If computed norm is a single number, all the entries of the matrix will be divided by that number. If norm is calculated along some axis, normalization happens along that axis using individual norms. Here are some examples.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[1., 8., 2., 3.],
       [2., 7., 6., 5.],
       [0., 3., 2., 8.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;normalized_mat, norm = tf.linalg.normalize(A, ord = 2, axis = [0,1])
print(&amp;quot;Normalized matrix: &amp;quot;)
print(normalized_mat.numpy())
print()
print(&amp;quot;Norm = &amp;quot;, norm.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix: 
[[0.06538278 0.5230622  0.13076556 0.19614834]
 [0.13076556 0.45767945 0.39229667 0.3269139 ]
 [0.         0.19614834 0.13076556 0.5230622 ]]

Norm =  [[15.294547]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will get the same normalized matrix by dividing the entries of the matrix by the norm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.06538278, 0.5230622 , 0.13076556, 0.19614834],
       [0.13076556, 0.45767945, 0.39229667, 0.3269139 ],
       [0.        , 0.19614834, 0.13076556, 0.5230622 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;norm_mat_by_col, norms_col = tf.linalg.normalize(A, ord = 2, axis = 0)
print(&amp;quot;Normalized matrix:&amp;quot;)
print(norm_mat_by_col)
print()
print(&amp;quot;Norms of columns of A:&amp;quot;)
print(norms_col)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix:
tf.Tensor(
[[0.4472136  0.724286   0.30151135 0.30304575]
 [0.8944272  0.63375026 0.904534   0.5050763 ]
 [0.         0.27160725 0.30151135 0.80812204]], shape=(3, 4), dtype=float32)

Norms of columns of A:
tf.Tensor([[ 2.236068  11.045361   6.6332498  9.899495 ]], shape=(1, 4), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(A[:,0], ord = 2).numpy()  # 2 Norm of first column of A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.236068&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norms_col&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.4472136 , 0.724286  , 0.30151135, 0.30304575],
       [0.8944272 , 0.63375026, 0.904534  , 0.5050763 ],
       [0.        , 0.27160725, 0.30151135, 0.80812204]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;norm_mat_by_row, norms_row = tf.linalg.normalize(A, ord = 2, axis = 1)
print(&amp;quot;Normalized matrix:&amp;quot;)
print(norm_mat_by_row)
print()
print(&amp;quot;Norms of rows:&amp;quot;)
print(norms_row)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix:
tf.Tensor(
[[0.11322771 0.9058217  0.22645542 0.33968312]
 [0.18731716 0.6556101  0.56195146 0.4682929 ]
 [0.         0.34188172 0.22792116 0.91168463]], shape=(3, 4), dtype=float32)

Norms of rows:
tf.Tensor(
[[ 8.83176 ]
 [10.677078]
 [ 8.774964]], shape=(3, 1), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(A[0,:], ord = 2).numpy()    # 2 norm of first row of A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.83176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norms_row&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.11322771, 0.9058217 , 0.22645542, 0.33968312],
       [0.18731716, 0.6556101 , 0.56195146, 0.4682929 ],
       [0.        , 0.34188172, 0.22792116, 0.91168463]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;global_norm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;global-norm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global norm&lt;/h3&gt;
&lt;p&gt;Given two or more tensors, &lt;code&gt;tf.linalg.global_norm&lt;/code&gt; computes the 2 norm of a vector generated by resizing all the tensors to one dimensional arrays.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant([1, 2, 3.])
b = tf.constant([[4,5],
                 [6,7.]])
tf.linalg.global_norm([a,b]).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.83216&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm([1,2,3,4,5,6,7.], ord = 2).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.83216&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;cross_product_of_vectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-product-of-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross product of vectors&lt;/h3&gt;
&lt;p&gt;It is defined for 3-element vectors. For two vectors &lt;span class=&#34;math inline&#34;&gt;\(a = (a_1, a_2, a_3)^T\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b = (b_1, b_2, b_3)^T\)&lt;/span&gt;, cross product is defined as the determinant of following matrix&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
i &amp;amp; j &amp;amp; k\\
a_1 &amp;amp; a_2 &amp;amp; a_3\\
b_1 &amp;amp; b_2 &amp;amp; b_3
\end{pmatrix}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(i, j\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; are unit direction vectors along three perpendicular right handed system.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant([1,2,3])
b = tf.constant([2,3,4])
tf.linalg.cross(a,b).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([-1,  2, -1], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First element in the output corresponds to the value along &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th direction. Similarly for other outputs.&lt;/p&gt;
&lt;p&gt;It is also possible to calculate cross product of more that one pair of vectors simultaneously.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;c = tf.random.normal(shape = (5,3))
d = tf.random.normal(shape = (5,3))
tf.linalg.cross(c,d).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[-0.8989703 , -0.89722276,  0.05096105],
       [-0.44832098, -0.32573706, -0.11998086],
       [ 0.7280822 ,  0.19632304,  0.8862282 ],
       [ 0.0998309 ,  0.48842978, -0.03491247],
       [ 1.0497653 , -1.5643073 ,  0.5671499 ]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First row of output is the cross product of first rows of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Similarly for other rows.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_square_root&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-square-root&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix square root&lt;/h3&gt;
&lt;p&gt;Square root of a matrix is defined for invertible matrices whose real eigenvalues are positive.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6.]])
mat_root = tf.linalg.sqrtm(mat)
mat_root&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[2.1185937 , 0.35412252, 0.6189322 ],
       [0.30147368, 2.9409115 , 0.7257953 ],
       [0.6540313 , 0.33657336, 2.3132057 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(mat_root, mat_root)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[4.9999986, 2.0000007, 3.0000038],
       [2.0000005, 9.000003 , 4.0000057],
       [3.0000033, 2.000003 , 6.0000052]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_exponential&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-exponential&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix exponential&lt;/h3&gt;
&lt;p&gt;Exponential of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ e^A = \sum_{n=0}^\infty \frac{A^n}{n!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, the sum is not taken to infinity. Rather, approximations are used to compute matrix exponential. &lt;code&gt;Tensorflow&lt;/code&gt; implementation is based on &lt;a href=&#34;https://epubs.siam.org/doi/10.1137/04061101X&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When the matrix has a full set of independent eigenvectors, the formula can be simplified to
&lt;span class=&#34;math display&#34;&gt;\[e^A = Se^{\Lambda}S^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the eigenvector matrix and &lt;span class=&#34;math inline&#34;&gt;\(e^\Lambda\)&lt;/span&gt; is a diagonal matrix whose diagonal entries are exponentials of eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[0,1],
                 [1,0.]])
tf.linalg.expm(A)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1.5430806, 1.1752012],
       [1.1752012, 1.5430806]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_logarithm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-logarithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix logarithm&lt;/h3&gt;
&lt;p&gt;Computes logarithm of the matrix such that matrix exponential of the result gives back the original matrix. Refer to the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/linalg/logm&#34;&gt;documentation&lt;/a&gt; for further details. It is defined only for complex matrices.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6.]], dtype = tf.complex64)
mat_log = tf.linalg.logm(mat)
mat_log&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=
array([[1.4031177 +0.j, 0.25731087+0.j, 0.53848237+0.j],
       [0.16580153+0.j, 2.1160111 +0.j, 0.54512537+0.j],
       [0.5994888 +0.j, 0.2268081 +0.j, 1.5622762 +0.j]], dtype=complex64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.expm(mat_log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=
array([[4.999999 +0.j, 1.9999989+0.j, 2.9999986+0.j],
       [1.9999995+0.j, 9.000006 +0.j, 4.0000005+0.j],
       [2.9999995+0.j, 2.000001 +0.j, 5.9999995+0.j]], dtype=complex64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;log-determinant_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-determinant-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log-determinant of a matrix&lt;/h3&gt;
&lt;p&gt;Computes the natural logarithm of the determinant of a matrix. There are two functions in &lt;code&gt;tensorflow&lt;/code&gt; to calculate this.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If matrix is symmetric positive definite, use &lt;code&gt;tf.linalg.logdet&lt;/code&gt; (Uses Cholesky decomposition)&lt;/li&gt;
&lt;li&gt;For other matrices, use &lt;code&gt;tf.linalg.slogdet&lt;/code&gt; (Uses &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;slogdet&lt;/code&gt; computes the sign of the determinant as well as the log of the absolute value of the determinant.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,2],
                   [3,2,6.]])   # Symmetric positive definite
tf.linalg.logdet(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.1298985&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.math.log(tf.linalg.det(mat)).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.1298985&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat_2 = tf.constant([[5,2,3],
                     [0,-2,2],
                     [0,0,6.]])
sign, log_abs_det = tf.linalg.slogdet(mat_2)
print(&amp;quot;Sign of determinant = &amp;quot;, sign.numpy())
print(&amp;quot;Log of absolute value of determinant = &amp;quot;, log_abs_det.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sign of determinant =  -1.0
Log of absolute value of determinant =  4.0943446&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.math.log(tf.abs(tf.linalg.det(mat_2))).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0943446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;pseudo_inverse_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pseudo-inverse-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pseudo inverse of a matrix&lt;/h3&gt;
&lt;p&gt;While matrix inverse is defined only for square matrices, pseudo inverse is defined for matrices of any shape. It is also defined for singular matrices. Pseudo inverse can also be used to solve ordinary least squares problem. For the problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the approximate least squares solution can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{ls} = A^{\dagger}b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, &lt;span class=&#34;math inline&#34;&gt;\(A^{\dagger}\)&lt;/span&gt; is the pseudo inverse of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.pinv(tf.constant([[2,0,0],[0,3,0],[5,0,0.]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[0.06896552, 0.        , 0.17241378],
       [0.        , 0.33333334, 0.        ],
       [0.        , 0.        , 0.        ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
A_under = tf.constant([[3,1,2,5],[7,9,1,4.]])
b_over = tf.constant([3,4,5,6.], shape = (4,1))
b_under = tf.constant([7.2,-5.8], shape = (2,1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_ls_over = tf.linalg.lstsq(A_over,b_over)
x_ls_over&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704103  ],
       [-0.04319588]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.pinv(A_over),b_over)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.7041038 ],
       [-0.04319668]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_ls_under = tf.linalg.lstsq(A_under, b_under)
x_ls_under&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355565 ],
       [ 0.699703  ],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.pinv(A_under), b_under)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100376],
       [-1.3355565 ],
       [ 0.69970286],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;linear_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-operators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear operators&lt;/h2&gt;
&lt;p&gt;Linear operators are a powerful way of defining matrices and associated operators without even doing actual computations. What does this mean? Do we ever get result of our computations using operators? Well, the computations are done only when we ask for the results. Before that the operators just act on each other (like chaining of operators) without doing any computation. We will show this by examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We will mainly use operators to form dense matrices. But the scope of applicability of operators is far bigger than that.&lt;/p&gt;
&lt;p&gt;To define a matrix as an operator, we use &lt;code&gt;tf.linalg.LinearOperatorFullMatrix&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],
                                                           [2,3,5],
                                                           [7,8,9.]]))
operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.ops.linalg.linear_operator_full_matrix.LinearOperatorFullMatrix at 0x7f8f501e7580&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get only the memory location. No result is shown. To see the actual matrix we have to call the method &lt;code&gt;to_dense&lt;/code&gt; on this operator. There are many methods that can be called on an operator. For the full list, refer the documentation.&lt;/p&gt;
&lt;p&gt;Before seeing the result, we will apply adjoint operator to our old operator and apply &lt;code&gt;to_dense&lt;/code&gt; to the adjoint operator. If everything works well, we should see the transpose of the matrix as result.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;adj_operator = tf.linalg.LinearOperatorAdjoint(operator)
adj_operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_23/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_adjoint.py:145: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.
Instructions for updating:
Do not call `graph_parents`.





&amp;lt;tensorflow.python.ops.linalg.linear_operator_adjoint.LinearOperatorAdjoint at 0x7f8f501e71f0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again no result. At this point we want to see the result. So we will apply &lt;code&gt;to_dense&lt;/code&gt; method to adj_operator.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;adj_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 7.],
       [2., 3., 8.],
       [3., 5., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compare it with our original matrix, we will also show the original matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the adjoint operator gives the correct answer.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;common_methods_on_linear_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;common-methods-on-linear-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common methods on linear operators&lt;/h3&gt;
&lt;p&gt;There are many methods that can be called on the operator. Depending on the operator, the methods vary. In this section we will discuss some of the methods of &lt;code&gt;LinearOperatorFullMatrix&lt;/code&gt; operator. Some of the methods are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cond (To find condition number)&lt;/li&gt;
&lt;li&gt;determinant&lt;/li&gt;
&lt;li&gt;cholesky (To compute Cholesky factors of operator)&lt;/li&gt;
&lt;li&gt;eigvals (Compute eigenvalues only for self-adjoint (Hermitian) matrices)&lt;/li&gt;
&lt;li&gt;trace&lt;/li&gt;
&lt;li&gt;inverse&lt;/li&gt;
&lt;li&gt;solve (Solve linear system using operator)&lt;/li&gt;
&lt;li&gt;adjoint&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and many others.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.cond()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=68.21983&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.trace()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=13.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.determinant()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:Using (possibly slow) default implementation of determinant.  Requires conversion to a dense matrix and O(N^3) operations.





&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.adjoint().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 7.],
       [2., 3., 8.],
       [3., 5., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.inverse().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.16666669],
       [ 2.8333333 , -2.        ,  0.16666669],
       [-0.83333325,  1.        , -0.16666669]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.matmul(operator.inverse().to_dense())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.0000000e+00,  0.0000000e+00,  0.0000000e+00],
       [-2.3841858e-07,  1.0000000e+00,  0.0000000e+00],
       [-2.3841858e-07,  0.0000000e+00,  1.0000000e+00]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;special_matrices_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;special-matrices-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Special matrices using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;toeplitz_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;toeplitz-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Toeplitz matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;col = tf.constant([1,2,3,4,5.])
row = tf.constant([1,6,7,8,9.])
toeplitz_operator = tf.linalg.LinearOperatorToeplitz(col = col, row = row )
toeplitz_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 6., 7., 8., 9.],
       [2., 1., 6., 7., 8.],
       [3., 2., 1., 6., 7.],
       [4., 3., 2., 1., 6.],
       [5., 4., 3., 2., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;circulant_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;circulant-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Circulant matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;kernel = [1,2,3,4,5]
spectrum = tf.signal.fft(tf.cast(kernel, dtype = tf.complex64))
circ_operator = tf.linalg.LinearOperatorCirculant(spectrum = spectrum, input_output_dtype = tf.float32)
circ_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1.0000002 , 4.9999995 , 3.9999993 , 3.        , 2.0000005 ],
       [2.0000005 , 0.99999934, 4.9999995 , 3.9999993 , 2.9999998 ],
       [3.0000002 , 2.        , 0.9999998 , 4.9999995 , 3.9999998 ],
       [3.9999993 , 2.9999993 , 2.        , 1.        , 4.9999995 ],
       [4.9999995 , 3.9999993 , 3.        , 2.0000005 , 1.0000002 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;circ_operator.convolution_kernel()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5,), dtype=float32, numpy=
array([1.       , 2.0000002, 3.       , 3.9999993, 4.9999995],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;block_diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;block-diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Block diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1 = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],
                                                             [4,5,6],
                                                             [7,8,9]], dtype = tf.float32))
operator_2 = tf.linalg.LinearOperatorFullMatrix(-1*tf.constant([[9,8],
                                                           [7,6]], dtype = tf.float32))
blk_diag_operator = tf.linalg.LinearOperatorBlockDiag([operator_1,operator_2])
blk_diag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.,  2.,  3.,  0.,  0.],
       [ 4.,  5.,  6.,  0.,  0.],
       [ 7.,  8.,  9.,  0.,  0.],
       [ 0.,  0.,  0., -9., -8.],
       [ 0.,  0.,  0., -7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;block_lower_triangular_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;block-lower-triangular-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Block lower triangular matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_3 = tf.linalg.LinearOperatorFullMatrix(tf.constant(tf.repeat(6.,repeats = 6), shape = (2,3)))
blk_lower = tf.linalg.LinearOperatorBlockLowerTriangular([[operator_1], [operator_3, operator_2]])
blk_lower.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.,  2.,  3.,  0.,  0.],
       [ 4.,  5.,  6.,  0.,  0.],
       [ 7.,  8.,  9.,  0.,  0.],
       [ 6.,  6.,  6., -9., -8.],
       [ 6.,  6.,  6., -7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;householder_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;householder-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Householder matrix&lt;/h4&gt;
&lt;p&gt;Householder matrix can be used to triangularize a matrix using orthogonal matrices (the process is called orthogonal triangularization). But we will not pursue that point here. We will only show the method using only a column vector. Given a vector &lt;span class=&#34;math inline&#34;&gt;\(v = [1, 4, 7]^T\)&lt;/span&gt;, Householder transform can transform the vector into &lt;span class=&#34;math inline&#34;&gt;\(v = \|v\|_2[1,0,0]^T\)&lt;/span&gt;. It is achieved by multiplying the vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; by an orthogonal Householder matrix &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H\begin{pmatrix}
v_1\\
v_2\\
v_3\end{pmatrix}=\|v\|_2\begin{pmatrix}
1\\
0\\
0\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This process can be repeated with other columns of a matrix to transform it into an upper triangular one. For more details, have a look at the references.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;first_column_of_operator_1 = operator_1.to_dense()[:,0]
norm = tf.linalg.norm(first_column_of_operator_1)
vec = first_column_of_operator_1 - norm*tf.linalg.eye(3)[:,0]    # Whether to take positive or negative sign? See references.
householder = tf.linalg.LinearOperatorHouseholder(reflection_axis = vec)
householder.matmul(tf.reshape(first_column_of_operator_1, shape = (3,1))).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[8.1240387e+00],
       [4.7683716e-07],
       [4.7683716e-07]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(householder.to_dense(), tf.reshape(first_column_of_operator_1, shape = (3,1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[8.1240387e+00],
       [4.7683716e-07],
       [7.1525574e-07]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;kronecker_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kronecker-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Kronecker matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [4., 5., 6.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_2.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-9., -8.],
       [-7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;kron_operator = tf.linalg.LinearOperatorKronecker([operator_1,operator_2])
kron_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(6, 6), dtype=float32, numpy=
array([[ -9.,  -8., -18., -16., -27., -24.],
       [ -7.,  -6., -14., -12., -21., -18.],
       [-36., -32., -45., -40., -54., -48.],
       [-28., -24., -35., -30., -42., -36.],
       [-63., -56., -72., -64., -81., -72.],
       [-49., -42., -56., -48., -63., -54.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;permutation_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Permutation matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;perm_operator = tf.linalg.LinearOperatorPermutation(tf.constant([2,4,0,3,1]))
perm_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0., 0., 1., 0., 0.],
       [0., 0., 0., 0., 1.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;common_matrices_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-matrices-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common matrices using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;identity-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;iden_operator = tf.linalg.LinearOperatorIdentity(num_rows = 5)
iden_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;scaled_identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scaled-identity-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Scaled identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scaled_iden_operator = tf.linalg.LinearOperatorScaledIdentity(num_rows = 5, multiplier = 5.)
scaled_iden_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[5., 0., 0., 0., 0.],
       [0., 5., 0., 0., 0.],
       [0., 0., 5., 0., 0.],
       [0., 0., 0., 5., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scaled_iden_operator_2 = tf.linalg.LinearOperatorScaledIdentity(num_rows = 3, multiplier = tf.constant([-5,7]))
scaled_iden_operator_2.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=
array([[[-5,  0,  0],
        [ 0, -5,  0],
        [ 0,  0, -5]],

       [[ 7,  0,  0],
        [ 0,  7,  0],
        [ 0,  0,  7]]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonal-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diag_operator = tf.linalg.LinearOperatorDiag(tf.constant([1,2,3,4.]))
diag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 0.],
       [0., 2., 0., 0.],
       [0., 0., 3., 0.],
       [0., 0., 0., 4.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;tri-diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tri-diagonal-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tri-diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1, -1, -1, -1, 0],
                         [ 2,  2,  2,  2, 2],
                         [ 0, -1, -1, -1, -1.]])
tridiag_operator = tf.linalg.LinearOperatorTridiag(diagonals = diags)
tridiag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2., -1.,  0.,  0.,  0.],
       [-1.,  2., -1.,  0.,  0.],
       [ 0., -1.,  2., -1.,  0.],
       [ 0.,  0., -1.,  2., -1.],
       [ 0.,  0.,  0., -1.,  2.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;lower_triangular_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lower-triangular-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Lower triangular matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[2,4,7,8],
                   [1,2,3,4],
                   [5,8,9,6],
                   [4,2,3,1]], dtype = tf.float32)
lower_tri_operator = tf.linalg.LinearOperatorLowerTriangular(mat)
lower_tri_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[2., 0., 0., 0.],
       [1., 2., 0., 0.],
       [5., 8., 9., 0.],
       [4., 2., 3., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_of_zeros&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-of-zeros&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix of zeros&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;zeros_operator = tf.linalg.LinearOperatorZeros(num_rows = 4, num_columns = 5, is_square = False,  is_self_adjoint=False)
zeros_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_operations_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-operations-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix operations using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;low-rank_update&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;low-rank-update&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Low-rank update&lt;/h4&gt;
&lt;p&gt;When a low rank matrix is added to a given matrix, the resulting matrix is called a low-rank update of the original matrix. Let’s suppose our original matrix was &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and we add a rank 1 update to it. The resulting matrix is &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. So&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[B = A + uv^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; are column vectors. It should be noted that low-rank matrix update doesn’t always increase the rank of the original matrix. For example, if rank of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; was 2, updating it with a rank 1 matrix will not always make its rank 3. Here is an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,2,3],
                 [2,4,6],
                 [3,4,5]], dtype = tf.float32)
u = tf.constant([[5],
                 [6],
                 [7]], dtype = tf.float32)
v = tf.constant([[7],
                 [8],
                 [9]], dtype = tf.float32)
B = A + tf.matmul(u,v, transpose_b=True)
print(&amp;quot;Rank of A = &amp;quot;, tf.linalg.matrix_rank(A).numpy())
print(&amp;quot;Rank of B = &amp;quot;, tf.linalg.matrix_rank(B).numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rank of A =  2
Rank of B =  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Why is it useful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It turns out that this low rank update appears in many applications. One of the applications is in least squares. Imagine that you have solved the least squares problem using the available data. And now you get some new data. The problem is to get the new least squares fit. Well, you can start form scratch by including the new data to your old data and then fit the model on the whole data. But this is a wasteful approach. A better alternative is to use the new data to modify your old fit. If you do some mathematics, you will arrive at matrix update equation. We will not do the math here. Interested readers can check references.&lt;/p&gt;
&lt;p&gt;Computations can be much faster if we use low-rank matrix update equation. In tensorflow it is done using &lt;code&gt;tf.linalg.OperatorLowRankUpdate&lt;/code&gt; operator. Though the operator can handle more than rank 1 update, we will use it only for rank 1 update.&lt;/p&gt;
&lt;p&gt;As an example, let’s suppose we want to compute the inverse of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. We can do so by modifying the inverse of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; that we have previously computed. The result is the famous Sherman–Morrison-Woodbury formula.
&lt;span class=&#34;math display&#34;&gt;\[B^{-1} = (A+uv^T)^{-1} = A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Provided that the denominator is not equal to zero. Note that denominator is a scalar for rank 1 update. This equation show that we can compute new inverse from the old inverse by using matrix-vector and vector-vector multiplications.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update = tf.linalg.LinearOperatorLowRankUpdate(operator,u = u, diag_update = None, v = v)
low_rank_update.inverse().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.625     ],
       [ 2.8333333 , -2.        , -0.24999982],
       [-0.83333325,  1.        , -0.25000012]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using Sherman–Morrison-Woodbury formula.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_inv = operator.inverse()
second_factor_numer = tf.matmul(operator_inv.matmul(u), tf.matmul(v,operator_inv.to_dense(), transpose_a=True))
second_factor_denom = 1 + tf.matmul(v,operator_inv.matmul(u), transpose_a = True)
update_inv = operator.inverse().to_dense() - (1/second_factor_denom)*second_factor_numer
print(update_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[-2.1666667   1.          0.6250001 ]
 [ 2.8333333  -2.         -0.25      ]
 [-0.83333325  1.         -0.25000006]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[36., 42., 48.],
       [44., 51., 59.],
       [56., 64., 72.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense() + tf.matmul(u, v, transpose_b = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[36., 42., 48.],
       [44., 51., 59.],
       [56., 64., 72.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Along with inverse, other methods can be applied to &lt;code&gt;LinearOperatorLowRankUpdate&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update.diag_part().numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([36., 51., 72.], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;operator_inversion&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operator-inversion&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Operator inversion&lt;/h4&gt;
&lt;p&gt;Computes inverse operator of a given operator.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;inv_operator = tf.linalg.LinearOperatorInversion(operator = operator)
inv_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.16666669],
       [ 2.8333333 , -2.        ,  0.16666669],
       [-0.83333325,  1.        , -0.16666669]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;operator_composition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operator-composition&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Operator composition&lt;/h4&gt;
&lt;p&gt;Like composition of function, this operator applies one operator over another. In terms of matrices, it just means matrix multiplication. But the result of composition is another operator. That new operator can be used for further analysis.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1 = tf.linalg.LinearOperatorFullMatrix([[1,2],
                                                 [2,5],
                                                 [7,-3.]])
operator_2 = tf.linalg.LinearOperatorFullMatrix([[2,-1,3],
                                                 [-1,4,5.]])
operator_comp = tf.linalg.LinearOperatorComposition([operator_1,operator_2])
operator_comp.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[  0.,   7.,  13.],
       [ -1.,  18.,  31.],
       [ 17., -19.,   6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;conclusion&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As we have seen, using only &lt;code&gt;tensorflow&lt;/code&gt; we can do quite a bit of linear algebra. In this post, we have only glossed over some of the functionalities. Clever use of the functions and operators will enable us to do much more than what has been covered here. At times, it might feel a little verbose. But the flexibility that it offers will make the exploration a rewarding experience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/linalg&#34;&gt;Tensorflow documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Datta, Biswa Nath. Numerical linear algebra and applications. Vol. 116. Siam, 2010.&lt;/li&gt;
&lt;li&gt;(The Book) Golub, Gene H., and Charles F. Van Loan. Matrix computations. Vol. 3. JHU press, 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last updated: 30 July, 2020.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple files in Tensorflow 2</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/o4aevvuqr39kq20/Reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will read multiple &lt;code&gt;.csv&lt;/code&gt; files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 &lt;code&gt;.csv&lt;/code&gt; files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of &lt;code&gt;.csv&lt;/code&gt; files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and &lt;code&gt;.csv&lt;/code&gt; files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. &lt;code&gt;Tensorflow&lt;/code&gt; is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/li&gt;
&lt;li&gt;Write a generator that reads data from the folder in chunks and preprocesses it.&lt;/li&gt;
&lt;li&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





[&amp;#39;./random_data/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;Generator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. &lt;code&gt;yield&lt;/code&gt; keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For our application, we will use a generator that runs indefinitely.&lt;/p&gt;
&lt;p&gt;The following generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;. &lt;code&gt;batch_size&lt;/code&gt; determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there is no need for generators. In case our data size is huge, we can process chunks of it.&lt;/p&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re            # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  # This loop is used to run the generator indefinitely.
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
                data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
                pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
                for j in range(len(label_classes)):
                    if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                        labels.append(j)  
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the generator change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the generator loop.&lt;/p&gt;
&lt;p&gt;Now we will check whether the generator works as intended or not. We will set &lt;code&gt;batch_size&lt;/code&gt; to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use &lt;code&gt;np.random.shuffle(file_list)&lt;/code&gt; to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;generated_data = data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in generated_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)  # Just to see the labels
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;Now we will create a &lt;code&gt;tensorflow dataset&lt;/code&gt; using the generator. &lt;code&gt;Tensorflow&lt;/code&gt; datasets can conveniently be used to train &lt;code&gt;tensorflow&lt;/code&gt; models.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;tensorflow dataset&lt;/code&gt; can be created form numpy arrays or from generators.Here, we will create it using a generator. Use of the previously created generator as it is in &lt;code&gt;tensorflow datasets&lt;/code&gt; doesn’t work (Readers can verify this). This happens because of the inability of regular expression to compare a “string” with a “byte string”. “byte strings” are generated by default in tensorflow. As a way around, we will make modifications to the earlier generator and use it with tensorflow datasets. Note that we will only modified three lines. Modified lines are accompanied by commented texts beside it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) # This line has changed.
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
                data.append(temp.values.reshape(32,32,1)) 
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  # This line has changed
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()):  # This line has changed.
                        labels.append(j)
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test whether modified generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_data = tf_data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in check_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the new generator created by using a few &lt;code&gt;tensorflow&lt;/code&gt; commands works just fine as our previous generator. This new generator can now be integrated with a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,32,32,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether &lt;code&gt;dataset&lt;/code&gt; works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in dataset:
    print(data.shape, labels.shape)
    print(labels)
    print()
    num = num + 1
    if num &amp;gt; 7: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle data (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,32,32,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,32,32,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,32,32,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  35
validation_steps =  5
steps =  10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
35/35 [==============================] - 1s 40ms/step - loss: 1.6268 - accuracy: 0.2029 - val_loss: 1.6111 - val_accuracy: 0.2000
Epoch 2/10
35/35 [==============================] - 1s 36ms/step - loss: 1.6101 - accuracy: 0.2114 - val_loss: 1.6079 - val_accuracy: 0.2600
Epoch 3/10
35/35 [==============================] - 1s 35ms/step - loss: 1.6066 - accuracy: 0.2343 - val_loss: 1.6076 - val_accuracy: 0.2000
Epoch 4/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5993 - accuracy: 0.2143 - val_loss: 1.6085 - val_accuracy: 0.2400
Epoch 5/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5861 - accuracy: 0.2657 - val_loss: 1.6243 - val_accuracy: 0.2000
Epoch 6/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5620 - accuracy: 0.3514 - val_loss: 1.6363 - val_accuracy: 0.2000
Epoch 7/10
35/35 [==============================] - 1s 36ms/step - loss: 1.5370 - accuracy: 0.2857 - val_loss: 1.6171 - val_accuracy: 0.2600
Epoch 8/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5015 - accuracy: 0.4057 - val_loss: 1.6577 - val_accuracy: 0.2000
Epoch 9/10
35/35 [==============================] - 1s 35ms/step - loss: 1.4415 - accuracy: 0.5086 - val_loss: 1.6484 - val_accuracy: 0.1400
Epoch 10/10
35/35 [==============================] - 1s 36ms/step - loss: 1.3363 - accuracy: 0.6143 - val_loss: 1.6672 - val_accuracy: 0.2200





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fcab40f6150&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 25ms/step - loss: 1.6974 - accuracy: 0.1500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6973648071289062
Test accuracy: 0.15000000596046448&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But often times, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just call &lt;code&gt;model.predict()&lt;/code&gt; on this data and then use &lt;code&gt;np.argmax()&lt;/code&gt; to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a generator to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,32,32,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(5, 32, 32, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,32,32,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)

array([[0.28138927, 0.3383776 , 0.17806269, 0.18918239, 0.01298801],
       [0.16730548, 0.20139892, 0.32996896, 0.16305783, 0.13826886],
       [0.08079846, 0.35669118, 0.4091237 , 0.13286887, 0.02051783],
       [0.01697877, 0.79075295, 0.17063092, 0.01676028, 0.00487713],
       [0.19006915, 0.02615157, 0.39364284, 0.09650648, 0.29362988],
       [0.05416911, 0.682985  , 0.19086388, 0.0668761 , 0.00510592],
       [0.21325852, 0.27782622, 0.10314588, 0.39539766, 0.01037181],
       [0.23633875, 0.3308002 , 0.30727112, 0.09573858, 0.02985144],
       [0.06442448, 0.34153524, 0.47356713, 0.08497778, 0.03549532],
       [0.37901744, 0.32311487, 0.12875995, 0.16359715, 0.00551067],
       [0.12227482, 0.49774405, 0.26021793, 0.1060346 , 0.01372868],
       [0.07139122, 0.17324339, 0.5490784 , 0.10136751, 0.10491937],
       [0.18757634, 0.2833261 , 0.3367256 , 0.14390293, 0.04846917],
       [0.23564269, 0.2800771 , 0.19150141, 0.2686058 , 0.02417296],
       [0.4835618 , 0.03908279, 0.09785527, 0.31918615, 0.06031401],
       [0.03285189, 0.5866938 , 0.3362034 , 0.0313101 , 0.01294078],
       [0.31367007, 0.05583594, 0.24806198, 0.2707511 , 0.1116809 ],
       [0.11204866, 0.05982558, 0.44611645, 0.16678827, 0.21522103],
       [0.04504926, 0.7100154 , 0.16532828, 0.0747861 , 0.00482096],
       [0.22441828, 0.01738338, 0.36729604, 0.0961706 , 0.29473177],
       [0.22392808, 0.23958267, 0.11669649, 0.41423568, 0.00555711],
       [0.11768451, 0.16422512, 0.49695587, 0.13158153, 0.08955302],
       [0.04941175, 0.31670955, 0.46190843, 0.12606393, 0.04590632],
       [0.19507076, 0.03239974, 0.3885634 , 0.14447391, 0.23949222],
       [0.3530666 , 0.08613478, 0.11636773, 0.4088019 , 0.03562902],
       [0.12874755, 0.3140329 , 0.3858064 , 0.1278494 , 0.0435637 ],
       [0.3001929 , 0.02791574, 0.11502622, 0.5044482 , 0.05241694],
       [0.0929171 , 0.1467541 , 0.6005069 , 0.06660035, 0.09322156],
       [0.10712272, 0.5518521 , 0.2632791 , 0.06340495, 0.01434106],
       [0.27723876, 0.25847596, 0.18952209, 0.25228631, 0.02247689],
       [0.12578863, 0.44461673, 0.25048074, 0.14304985, 0.03606399],
       [0.09593316, 0.06914104, 0.49921316, 0.1389045 , 0.19680816],
       [0.22185169, 0.0878747 , 0.33703303, 0.23808932, 0.11515129],
       [0.0850782 , 0.06328611, 0.57307494, 0.08615369, 0.19240707],
       [0.41479778, 0.07033634, 0.22154689, 0.2007963 , 0.09252268],
       [0.22052608, 0.10761442, 0.33570328, 0.25846007, 0.07769614],
       [0.03679338, 0.4369671 , 0.42453632, 0.07080499, 0.03089818],
       [0.17414902, 0.3666445 , 0.26953018, 0.16861232, 0.02106389],
       [0.04334973, 0.04427214, 0.5819794 , 0.02825493, 0.30214384],
       [0.23099631, 0.31964707, 0.31392127, 0.11803907, 0.01739628],
       [0.03072637, 0.6739159 , 0.25826213, 0.0309101 , 0.00618558],
       [0.20030826, 0.05058228, 0.42536664, 0.14415787, 0.17958501],
       [0.25894472, 0.0410106 , 0.25135538, 0.15487678, 0.29381245],
       [0.31544876, 0.05200702, 0.20838396, 0.31984535, 0.10431487],
       [0.10788545, 0.31769663, 0.44471365, 0.08522549, 0.04447879],
       [0.01864015, 0.35556656, 0.551683  , 0.02805553, 0.04605483],
       [0.20043266, 0.1211144 , 0.26670808, 0.33885604, 0.07288874],
       [0.29432756, 0.19128233, 0.19503927, 0.2826192 , 0.03673161],
       [0.2151616 , 0.05391361, 0.34218988, 0.11304423, 0.27569073],
       [0.241943  , 0.05663572, 0.23858468, 0.36390153, 0.09893499],
       [0.24665013, 0.22702417, 0.33673155, 0.11996701, 0.06962712],
       [0.05448309, 0.33466634, 0.49283266, 0.07876839, 0.03924957],
       [0.3060696 , 0.03565398, 0.33453086, 0.12989788, 0.19384763],
       [0.1417291 , 0.40642622, 0.20021752, 0.22896914, 0.02265806],
       [0.10395318, 0.20624556, 0.46823606, 0.12000521, 0.10156006]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 2, 2, 1, 2, 1, 3, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 3, 2,
       2, 2, 3, 2, 3, 2, 1, 0, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 4, 3,
       2, 2, 3, 0, 2, 3, 2, 2, 2, 1, 2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are randomly generated. So we should not be surprised by this result. Also note that the for each new data, softmax outputs are close to each other. This means that the network is not very sure about the classification result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a generator, and trained a model using that generator. The above code can be tweaked slightly to read any type of files other than &lt;code&gt;.csv&lt;/code&gt;. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 1&lt;/strong&gt;: While generators are convenient for handling chunks of data from a large dataset, they have limited portability and scalability. Therefore, in Tensorflow Sequences are preferred instead of generators. &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34;&gt;See this blog&lt;/a&gt; for a complete workflow for reading multiple files using Sequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 2&lt;/strong&gt;: If along with reading, one has to perform complex transformations on extracted data (say, doing spectrogram on each segment of data, etc.), the naive approach presented in this blog may turn out to be slow. But there are ways to make these computations faster. One such speedup technique can be found at &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last modified: 27th April, 2020.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Python Generators</title>
      <link>https://biswajitsahoo1111.github.io/post/using-python-generators/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/using-python-generators/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/ax24jc2rdmg4dlo/Using_python_generators.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful for beginners.&lt;/p&gt;
&lt;p&gt;While functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It’s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.&lt;/p&gt;
&lt;p&gt;First let’s create artificial data that we will extract later batch by batch.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data = np.random.randint(100,150, size = (10,2,2))
labels = np.random.permutation(10)
print(data)
print(&amp;quot;labels:&amp;quot;, labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]

 [[126 129]
  [116 146]]

 [[145 104]
  [143 143]]

 [[114 122]
  [102 148]]

 [[122 118]
  [145 134]]

 [[131 134]
  [122 104]]

 [[145 103]
  [136 138]]

 [[128 119]
  [141 118]]

 [[106 115]
  [124 130]]]
labels: [3 5 8 4 0 9 1 6 7 2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def my_gen(data, labels, batch_size = 2):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(labels):
            i = 0
            idx = np.random.permutation(len(labels))
            data, labels = data[idx], labels[idx]
            continue
        else:
            X = data[i*batch_size:(i+1)*batch_size,:]
            y = labels[i*batch_size:(i+1)*batch_size]
            i += 1
            yield X,y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found &lt;a href=&#34;https://stackoverflow.com/questions/42727412/efficient-way-to-partially-read-large-numpy-file&#34;&gt;here&lt;/a&gt;. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using “np.shuffle()”.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_data = my_gen(data,labels)
for i in range(10):
    X,y = next(get_data)
    print(X,y)
    print(X.shape, y.shape)
    print(&amp;quot;=========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]] [3 5]
(2, 2, 2) (2,)
=========================
[[[126 129]
  [116 146]]

 [[145 104]
  [143 143]]] [8 4]
(2, 2, 2) (2,)
=========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
=========================
[[[131 134]
  [122 104]]

 [[145 103]
  [136 138]]] [1 6]
(2, 2, 2) (2,)
=========================
[[[128 119]
  [141 118]]

 [[106 115]
  [124 130]]] [7 2]
(2, 2, 2) (2,)
=========================
[[[132 119]
  [126 119]]

 [[145 104]
  [143 143]]] [3 4]
(2, 2, 2) (2,)
=========================
[[[131 134]
  [122 104]]

 [[126 129]
  [116 146]]] [1 8]
(2, 2, 2) (2,)
=========================
[[[133 126]
  [144 140]]

 [[106 115]
  [124 130]]] [5 2]
(2, 2, 2) (2,)
=========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
=========================
[[[128 119]
  [141 118]]

 [[145 103]
  [136 138]]] [7 6]
(2, 2, 2) (2,)
=========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow Version: &amp;quot;, tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow Version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class my_new_gen(tf.keras.utils.Sequence):
    def __init__(self, data, labels, batch_size= 2 ):
        self.x, self.y = data, labels
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])

    def __len__(self):
        return tf.math.floor(self.x.shape[0] / self.batch_size)

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x = self.x[inds]
        batch_y = self.y[inds]
        return batch_x, batch_y
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we must add &lt;code&gt;len&lt;/code&gt; method and &lt;code&gt;getitem&lt;/code&gt; method within the class and if we want to shuffle data between epochs, we have to add &lt;code&gt;on_epoch_end()&lt;/code&gt; method. &lt;code&gt;len&lt;/code&gt; finds out the number of batches possible in an epoch and &lt;code&gt;getitem&lt;/code&gt; extracts batches one by one. When one epoch is complete, &lt;code&gt;on_epoch_end()&lt;/code&gt; shuffles the data and the process continues. We will test it with an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_new_data = my_new_gen(data, labels)

for i in range(10):
    if i == 5:
        get_new_data.on_epoch_end()
        i = 0
    elif i &amp;gt; 5:
        i = i-5
    dat,labs = get_new_data.__getitem__(i)
    print(dat,labs)
    print(dat.shape, labs.shape)
    print(&amp;quot;===========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[132 119]
  [126 119]]

 [[133 126]
  [144 140]]] [3 5]
(2, 2, 2) (2,)
===========================
[[[126 129]
  [116 146]]

 [[145 104]
  [143 143]]] [8 4]
(2, 2, 2) (2,)
===========================
[[[114 122]
  [102 148]]

 [[122 118]
  [145 134]]] [0 9]
(2, 2, 2) (2,)
===========================
[[[131 134]
  [122 104]]

 [[145 103]
  [136 138]]] [1 6]
(2, 2, 2) (2,)
===========================
[[[128 119]
  [141 118]]

 [[106 115]
  [124 130]]] [7 2]
(2, 2, 2) (2,)
===========================
[[[145 103]
  [136 138]]

 [[133 126]
  [144 140]]] [6 5]
(2, 2, 2) (2,)
===========================
[[[126 129]
  [116 146]]

 [[122 118]
  [145 134]]] [8 9]
(2, 2, 2) (2,)
===========================
[[[145 104]
  [143 143]]

 [[128 119]
  [141 118]]] [4 7]
(2, 2, 2) (2,)
===========================
[[[131 134]
  [122 104]]

 [[114 122]
  [102 148]]] [1 0]
(2, 2, 2) (2,)
===========================
[[[132 119]
  [126 119]]

 [[106 115]
  [124 130]]] [3 2]
(2, 2, 2) (2,)
===========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both the generators work fine. Now we will use it to implement a CNN model on MNIST data. Note that this example is bit stretched and strange. We don’t need generators to implement small data sets like MNIST. Whole of MNIST can be loaded into RAM. By this example the aim is just to show a different way of implementing it using generators. Of course the codes can be modified to handle cases where we indeed need generators to do analysis.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras.models import Sequential
from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(train_data, train_labels),(test_data,test_labels) = tf.keras.datasets.mnist.load_data()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_data = train_data.reshape(60000,28,28,1)/255.
id = np.random.permutation(len(train_labels))
training_data, training_labels = train_data[id[0:48000]], train_labels[id[0:48000]]
val_data, val_labels = train_data[id[48000:60000]], train_labels[id[48000:60000]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = Sequential([
    layers.Conv2D(32, 3, activation = &amp;#39;relu&amp;#39;, input_shape = (28,28,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(64,5,activation = &amp;#39;relu&amp;#39;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(32,activation = &amp;#39;relu&amp;#39;),
    layers.Dense(10, activation = &amp;#39;sigmoid&amp;#39;)
])
model.compile(optimizer = &amp;#39;adam&amp;#39;, loss = &amp;#39;categorical_crossentropy&amp;#39;, metrics = [&amp;#39;accuracy&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Keras requires the generator to run indefinitely
class data_gen(tf.keras.utils.Sequence):
    def __init__(self, data, labels, batch_size=128):
        self.x, self.y = data, labels
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])

    def __len__(self):
        return int(tf.math.ceil(self.x.shape[0] / self.batch_size))

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x = self.x[inds]
        batch_y = self.y[inds]
        return batch_x, tf.keras.utils.to_categorical(batch_y)
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_gen = data_gen(train_data, train_labels,batch_size = 128)
val_gen = data_gen(val_data, val_labels,batch_size = 128)
batch_size = 128
steps_per_epoch = np.floor(len(train_labels)/batch_size)
val_steps = np.floor(len(val_labels)/batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_gen, steps_per_epoch = steps_per_epoch, epochs = 10,
          validation_data = val_gen, validation_steps = val_steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
468/468 [==============================] - 10s 10ms/step - loss: 0.5769 - accuracy: 0.8351 - val_loss: 0.0858 - val_accuracy: 0.9716
Epoch 2/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0454 - val_accuracy: 0.9860
Epoch 3/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0512 - accuracy: 0.9839 - val_loss: 0.0377 - val_accuracy: 0.9883
Epoch 4/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0389 - accuracy: 0.9879 - val_loss: 0.0278 - val_accuracy: 0.9908
Epoch 5/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.0279 - val_accuracy: 0.9899
Epoch 6/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0170 - val_accuracy: 0.9950
Epoch 7/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0118 - val_accuracy: 0.9966
Epoch 8/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0146 - val_accuracy: 0.9952
Epoch 9/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0107 - val_accuracy: 0.9974
Epoch 10/10
468/468 [==============================] - 3s 7ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0078 - val_accuracy: 0.9977

&amp;lt;tensorflow.python.keras.callbacks.History at 0x2543f4e64c0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_data.reshape(10000,28,28,1)/255., tf.keras.utils.to_categorical(test_labels), verbose = 2)
print(&amp;quot;Test Loss:&amp;quot;, test_loss)
print(&amp;quot;Test Accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;313/313 - 1s - loss: 0.0282 - accuracy: 0.9922
Test Loss: 0.0281691811978817
Test Accuracy: 0.9922000169754028&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have reached close to 99% accuracy which is not bad! This example might seem a bit stretched as we don’t need generators for small datasets like MNIST. The aim of the example is just to show different implementation using generators.&lt;/p&gt;
&lt;p&gt;Perhaps the most detailed blog about using generators for deep learning is &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;this one&lt;/a&gt;. I also found &lt;a href=&#34;https://github.com/keras-team/keras/issues/9707#issuecomment-374609666&#34;&gt;these comments&lt;/a&gt; helpful.&lt;/p&gt;
&lt;p&gt;Update 1: With the release of &lt;code&gt;Tensorflow-2.0&lt;/code&gt;, it is much easier to use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API for handling large datasets. Generators can still be used for training using &lt;code&gt;tf.keras&lt;/code&gt;. As a final note, use generators if it is absolutely essential to do so. Otherwise, use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API. Check out &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;this post&lt;/a&gt; for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Update 2: See &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34;&gt;this blog&lt;/a&gt; for a complete workflow for reading multiple files using &lt;code&gt;Tensorflow Sequence&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fault Diagnosis of Machines</title>
      <link>https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;This story was originally written for &lt;a href=&#34;https://www.awsar-dst.in/&#34;&gt;“Augmenting Writing Skills for Articulating Research (AWSAR)”&lt;/a&gt; award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. Full list of awardees and their stories can be found &lt;a href=&#34;https://www.awsar-dst.in/awsar_awarded_articles_2018/download_book&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;prelude&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prelude&lt;/h2&gt;
&lt;p&gt;Rising sun with its gentle light marks the arrival of morning. Birds’ chirp as well as time on our clock, sometimes with a blaring alarm, confirm the arrival of morning. Each of these, among several others, is an indicator of the morning. But can we know about morning by following only one indicator? Let’s deliberate. What if the sky is cloudy and we don’t see the sun rising, will this mean that morning is yet to come? Of course not! Our alarm will remind us of morning irrespective of whether there is sun or not. But what if, on some occasion, our clock doesn’t work. In that case, birds may chirp or sun may rise or our near and dear ones may remind us that it’s morning already. So in essence, we usually don’t look for only one indicator. Rather, we consider several indicators. If one indicator fails, we can check another and thus be sure. It is very unlikely that all the indicators will fail simultaneously.&lt;/p&gt;
&lt;p&gt;So the best way to get an idea about an event, it seems, is not to rely on only one indicator. Rather, observe several indicators and depending on their collective state, arrive at some conclusion. In this way, we deliberately add redundancy in order to get reliable results. This is exactly what we do in fault diagnosis of machines. Fault diagnosis is a broad term that addresses mainly three questions. First, find out whether fault is there in the machine or not. If fault is present, next question is to find the location of the fault. Once location of the fault is found, finally, find out the type of fault and its severity. In this article, we will only limit ourselves to the last aspect. But for simplicity, we will still use the term fault diagnosis to address that particular problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The method&lt;/h2&gt;
&lt;p&gt;To determine the health of a machine, we collect a set of indicators that best explain the condition of the machine. In scientific jargon, we call those features. Before discussing further, let’s first discuss what are those features and how they are calculated.&lt;/p&gt;
&lt;p&gt;First, data needs to be collected from a machine whose health needs to be assessed. Data might pertain to vibration level of the machine or its temperature distribution or the sound produced by the machine or something else. Sensors are needed to collect each type of data. By analogy, a thermometer, which is used to measure body temperature of humans, is a sensor that measures temperature. Likewise, different types of sensors are available to measure different quantities of interest related to the machine. From research it has been found that vibration based data are more suitable for fault diagnosis as compared to other types of data, say, temperature or sound. So in this article, we will limit our attention to vibration based fault diagnosis. And the sensor that is most commonly used to measure the vibration of a machine is called an accelerometer. Form the data collected by accelerometer(s) we calculate features like the maximum level of vibration, similarly, the minimum level and other statistical features like skewness, kurtosis, etc. It is not uncommon to collect 10-15 features.&lt;/p&gt;
&lt;p&gt;After feature collection, the next task is to find out what type of faults are present by using those features. One way to do this is by comparing the obtained feature values to pre-existing standards. But standards are available for few specialized cases when each feature is considered in isolation. For multiple features, no concrete information can be obtained from standards. The way out of this problem is to come up with an algorithm that takes all feature values as input and produces the output related to the type of fault present.&lt;/p&gt;
&lt;p&gt;Construction of such an algorithm requires prior faulty and non-faulty data of similar machines be fed to it. The algorithm should ideally work well on this prior data. Once fine-tuning of its parameters are done, new data are fed into the algorithm and from its output, we infer the fault type. If the algorithm is carefully constructed, error in prediction of fault type will be very small. In some cases, it is also possible to get perfect accuracy. The approach just considered is a sub-class of a broad field called pattern recognition. In pattern recognition, we try to find underlying patterns in features that correspond to different fault types. This type of pattern recognition tasks are best performed by machine learning algorithms. The simple technique just described works fine for a large class of problems. But there exist some problems for which the features previously calculated are not sufficient to identify fault. However, it is possible to modify the technique by using transformation of data as well as features. Transformations are a way of converting the original data into another type such that after transformation more insight is gained out of it. This is similar to using logarithms in mathematics to do complex calculations. While direct computation of complex multiplications and divisions is difficult, using logarithm we transform the original problem into a simpler form that can be solved easily in less time. The transformation trick along with pattern recognition methods are surprisingly effective for most fault diagnosis tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-recent-advances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some recent advances&lt;/h2&gt;
&lt;p&gt;Up to this point, we have argued that redundancy is important. It helps us take reliable decisions. However, it requires collection of huge amounts of data. Thus, continuous monitoring of machine, also known as online monitoring, becomes infeasible. So we seek an algorithm that is capable of finding fault types using only a few measurements. One way to do this is to select a few important features that can perform fault diagnosis. Research shows that it is indeed possible. But merely finding best features is not enough. Because to calculate the features, even though small in number, we need to collect all data. Hence issues related to online monitoring will still exist. A way around this problem is not to collect all data but only a fraction of it randomly in time. And the data should be collected in such a way that all information regarding the machine can be extracted from these limited observations. An even optimistic goal is to reconstruct the original data from the limited collected data. By analogy, this is similar to reconstructing the speech of a person, who speaks, say, 3000 words, from 300 random words that you have remembered of their entire speech. The problem just described is known as compressed sensing. And no matter how much counter-intuitive it may seem, encouraging results for this problem have been obtained in signal processing and these methods are beginning to get applied to problems of fault diagnosis. The problem is still in its infancy in fault diagnosis field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-we-learned-and-what-we-didnt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What we learned (and what we didn’t!)&lt;/h2&gt;
&lt;p&gt;In summary, we have learned that to diagnose faults, we need multiple features and sometimes we have to transform the data into different domains to get better accuracy. We then observed that we can get rid of the redundancy inherent in this method by using compressed sensing methods. All these techniques come under data-driven methods. It is called data-driven because all analyses are done after we collect relevant data from the machine. These methods are quite general purpose and can be used to diagnose faults in different components, say, detecting faults in cars or in other machines.&lt;/p&gt;
&lt;p&gt;Apart from data-driven methods there also exists another class of techniques that go by the broad name of model-based methods. In model-based methods, we formulate a full mathematical model of the machine and then try to find out how the response of the model changes if a fault is introduced and using this fact, try to find the nature of fault for a new problem. Though model-based techniques are important in their own right, in many cases it becomes very difficult to find an accurate model of the system because of the uncertainties involved. In contrast, data-driven methods are more robust against external noise and are flexible, meaning, we can perform different analysis using the same data and obtain deeper insights. Another advantage of using data-driven methods is that the whole process of fault diagnosis can easily be automated.&lt;/p&gt;
&lt;p&gt;In this article, we have only considered the field of fault diagnosis. In fault diagnosis, faults are already present and we wish to either detect them or segregate them depending on fault type. But there exists another branch that deals with ways to predict the time of occurrence of fault in future, given the present state. Basically, they determine the remaining useful life of the machine. This sub-branch is called fault prognosis which is also an active area of research.&lt;/p&gt;
&lt;p&gt;Given the advancement of research and scope for automation, it may be possible, in not so distant future, to get updates on your phone about possible malfunction of a part of your car while driving your car or while enjoying a ride in a self-driving car, maybe!!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.awsar-dst.in/2019/assets/winner_article_2018/30_PhD.pdf&#34;&gt;Published story can be found at this link&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Revisiting Systems of Linear Equations</title>
      <link>https://biswajitsahoo1111.github.io/post/revisiting-systems-of-linear-equations/</link>
      <pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/revisiting-systems-of-linear-equations/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Almost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable. Then the final solution is obtained by back substituting the obtained value of the variable into remaining equations. This method, in mathematical jargon, is called Gaussian elimination and back substitution.&lt;/p&gt;
&lt;p&gt;It turns out (surprisingly) that linear systems form the basis of many interesting engineering applications. Ultimately the problem boils down to solution (or approximate solution) of a system of linear equations. So a thorough understanding of linear systems is essential to appreciate the applications. In this post we will outline all possible cases of finding solutions to linear systems and briefly outline two most important applications.&lt;/p&gt;
&lt;p&gt;We will use matrix notation to represent the equations succinctly. It also gives us better insight into their solution. Using matrix notation the system can be represented as
&lt;span class=&#34;math display&#34;&gt;\[\textbf{Ax}=\textbf{b}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{A}\)&lt;/span&gt; is the matrix of coefficients of size &lt;span class=&#34;math inline&#34;&gt;\((m\times n)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt; is a vector of variables of size &lt;span class=&#34;math inline&#34;&gt;\((n\times 1)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\textbf{b}\)&lt;/span&gt;
is a vector of size &lt;span class=&#34;math inline&#34;&gt;\((m\times 1)\)&lt;/span&gt; representing constant right hand sides. Note that &lt;span class=&#34;math inline&#34;&gt;\(\textbf{b}\)&lt;/span&gt; can be a vector of all zeros, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} = \textbf{0}\)&lt;/span&gt; or it can be any arbitrary vector with some nonzero values, i.e.,&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b}\neq \textbf{0}\)&lt;/span&gt;. The solution(s) of linear systems depend to a large extent on what the right hand side is as we will see shortly.&lt;/p&gt;
&lt;p&gt;Apart from notation, we need two other concepts from matrix theory. One is of rank and other is the range space (or column space) of a matrix. Rank &lt;span class=&#34;math inline&#34;&gt;\((Rank(\textbf{A}))\)&lt;/span&gt; of a matrix (say, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{A}\)&lt;/span&gt;) is defined as number of independent rows or columns of a matrix. It is a well known result in matrix theory that row rank (number of independent rows) is equal to column rank (number of independent columns) and &lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A})\leq min(m,n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Range space &lt;span class=&#34;math inline&#34;&gt;\((\mathcal{R}(A))\)&lt;/span&gt;(in short, Range) of a matrix is the vector space of all possible linear combinations of columns of the matrix. As we take all possible linear combination of columns, it is also known as column space. Readers who are slightly more familiar with linear algebra may know that Range is the span of columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{A}\)&lt;/span&gt;. Zero vector &lt;span class=&#34;math inline&#34;&gt;\((\textbf{0})\)&lt;/span&gt; is &lt;strong&gt;always&lt;/strong&gt; in the range of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{A}\)&lt;/span&gt; because if we take linear combination of columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{A}\)&lt;/span&gt; with all coefficients as 0’s, we get zero vector. Hence &lt;span class=&#34;math inline&#34;&gt;\(\textbf{b}=0 \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; is always true.&lt;/p&gt;
&lt;p&gt;Let’s now discuss different cases separately and their solutions. We will assume that our system of equations has real entries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Case - I: &lt;span class=&#34;math inline&#34;&gt;\((m = n)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) = m\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Unique solution (for any &lt;span class=&#34;math inline&#34;&gt;\(\textbf{b}\)&lt;/span&gt;). For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 8 \\
  3 &amp;amp; 5 &amp;amp; 7 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  5 \\
  7
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has unique solution.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Impossible (This case will never happen because &lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A})=m\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) &amp;lt; m\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Infinitely many solutions. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 6 \\
  3 &amp;amp; 5 &amp;amp; 7 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  6 \\
  8
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has infinitely many solutions.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : No solution. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 6 \\
  3 &amp;amp; 5 &amp;amp; 7 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  1 \\
  5 \\
  7
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has no solution.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Case - II: &lt;span class=&#34;math inline&#34;&gt;\((m &amp;gt; n)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) = n\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Unique solution. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 \\
  2 &amp;amp; 7 \\
  3 &amp;amp; 8 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  9 \\
  11
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has unique solution.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : No solution. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 \\
  2 &amp;amp; 7 \\
  3 &amp;amp; 8 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  9 \\
  11
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has no solution. But this case is immensely useful from application point of view. Sometimes it is not desirable to obtain the exact solution. Rather an approximate solution suffices for all practical purposes. Finding an approximate solution to an overdetermined system leads to the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Least_squares&#34;&gt;Least Squares&lt;/a&gt; problem.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) &amp;lt; n\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Infinitely many solutions. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 \\
  2 &amp;amp; 4 \\
  3 &amp;amp; 6 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  6 \\
  9
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; It has infinitely many solutions.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : No solution. For example,&lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 \\
  2 &amp;amp; 4 \\
  3 &amp;amp; 6 \\
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 
  \end{bmatrix}
  = 
  \begin{bmatrix}
  3 \\
  6 \\
  8
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has no solution.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Case - III: &lt;span class=&#34;math inline&#34;&gt;\((m &amp;lt; n)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) = m\)&lt;/span&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Infinitely many solutions. For example, &lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 5 
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  2 \\
  3 
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has infinitely many solutions. This case is also used in many applications. As there are infinitely many solutions, a natural choice is to choose the best solution. The qualifier ‘best’ determines what application we have in our mind. If we seek minimum &lt;span class=&#34;math inline&#34;&gt;\((l_2)\)&lt;/span&gt; norm, we get the so called minimum energy solution, a concept used in signal processing. Yet another concern is to seek for the sparsest solution (a solution with only a few nonzero entries and all other entries being zero). This idea is used in &lt;a href=&#34;https://en.wikipedia.org/wiki/Compressed_sensing&#34;&gt;Compressed Sensing&lt;/a&gt;, an active research area with many interesting applications.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Impossible. This case will never happen since &lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A})=m\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Rank(\textbf{A}) &amp;lt; m\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : Infinitely many solutions. For example, &lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 6 
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  4 \\
  8 
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has infinitely many solutions.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{b} \not\in \mathcal{R}(\textbf{A})\)&lt;/span&gt; : No solution. For example, &lt;span class=&#34;math display&#34;&gt;\[ \begin{equation}
  \begin{bmatrix}
  1 &amp;amp; 2 &amp;amp; 3 \\
  2 &amp;amp; 4 &amp;amp; 6 
  \end{bmatrix}
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3
  \end{bmatrix}
  = 
  \begin{bmatrix}
  1 \\
  5 
  \end{bmatrix}
  \end{equation}\]&lt;/span&gt; This system has no solution.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hope this post gives a clear overview of linear systems of equations. Interested reader may explore further applications. Comments and clarifications are welcome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part III</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/PCA_Abdi_in_python.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run Python code in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/gn415uh3quou3a8/PCA_Abdi_in_python.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download Python code&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/tp56l6v7upa1apf/PCA_blog_Biswajit_Sahoo_part_3.Rmd?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download R code (R Markdown)&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘&lt;a href=&#34;https://personal.utdallas.edu/~herve/abdi-awPCA2010.pdf&#34;&gt;Principal component analysis&lt;/a&gt;’ and is authored by &lt;a href=&#34;https://personal.utdallas.edu/~herve/&#34;&gt;Herve Abdi&lt;/a&gt; and &lt;a href=&#34;https://ljwilliams.github.io/&#34;&gt;Lynne J. Williams&lt;/a&gt;. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).&lt;/p&gt;
&lt;p&gt;This post is Part-III of a three part series on PCA. Other parts of the series can be found at the links below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34;&gt;Part-I: Basic Theory of PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II: PCA Implementation with and without using built-in functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post contains code snippets in R. Equivalent &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34;&gt;MATLAB codes&lt;/a&gt; can be written using commands of &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt;. For figures, the reader has to write his/her own code in MATLAB.&lt;/p&gt;
&lt;div id=&#34;structure-of-the-paper&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Structure of the paper&lt;/h3&gt;
&lt;p&gt;Along with basic theory, the paper contains three examples on PCA, one example on correspondence analysis, and one example on multiple factor analysis. We will only focus on PCA examples in this post.&lt;/p&gt;
&lt;p&gt;To run following R codes seamlessly, readers have to load following packages. If these packages have not been installed previously, use &lt;code&gt;install.packages(&#34;package_name&#34;)&lt;/code&gt; to install those.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-get-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to get data&lt;/h3&gt;
&lt;p&gt;Data for the examples have been taken from the paper [1]. The datasets are pretty small. So one way to read the data is to create a dataframe itself in R using the values given in paper. Otherwise, the values can first be stored in a csv file and then read into R. To make this post self-sufficient, we will adopt the former approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Throughout this article, additional comments have been made beside code segments. It would be a good idea to read those commented lines along with the codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 1
# Create a dataframe
Words = c(&amp;quot;Bag&amp;quot;, &amp;quot;Across&amp;quot;, &amp;quot;On&amp;quot;, &amp;quot;Insane&amp;quot;, &amp;quot;By&amp;quot;, &amp;quot;Monastery&amp;quot;, &amp;quot;Relief&amp;quot;, &amp;quot;Slope&amp;quot;, &amp;quot;Scoundrel&amp;quot;, &amp;quot;With&amp;quot;, &amp;quot;Neither&amp;quot;, &amp;quot;Pretentious&amp;quot;, &amp;quot;Solid&amp;quot;, &amp;quot;This&amp;quot;, &amp;quot;For&amp;quot;, &amp;quot;Therefore&amp;quot;, &amp;quot;Generality&amp;quot;, &amp;quot;Arise&amp;quot;, &amp;quot;Blot&amp;quot;, &amp;quot;Infectious&amp;quot;)
Word_length = c(3, 6, 2, 6, 2, 9, 6, 5, 9, 4, 7, 11, 5, 4, 3, 9, 10, 5, 4, 10)
Lines_in_dict = c(14, 7, 11, 9, 9, 4, 8, 11, 5, 8, 2, 4, 12, 9, 8, 1, 4, 13, 15, 6)
words = data.frame(Words, Word_length, Lines_in_dict, stringsAsFactors = F)
words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         Words Word_length Lines_in_dict
1          Bag           3            14
2       Across           6             7
3           On           2            11
4       Insane           6             9
5           By           2             9
6    Monastery           9             4
7       Relief           6             8
8        Slope           5            11
9    Scoundrel           9             5
10        With           4             8
11     Neither           7             2
12 Pretentious          11             4
13       Solid           5            12
14        This           4             9
15         For           3             8
16   Therefore           9             1
17  Generality          10             4
18       Arise           5            13
19        Blot           4            15
20  Infectious          10             6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(words_centered = scale(words[,2:3],scale = F)) # Centering after reemoving the first column&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      Word_length Lines_in_dict
 [1,]          -3             6
 [2,]           0            -1
 [3,]          -4             3
 [4,]           0             1
 [5,]          -4             1
 [6,]           3            -4
 [7,]           0             0
 [8,]          -1             3
 [9,]           3            -3
[10,]          -2             0
[11,]           1            -6
[12,]           5            -4
[13,]          -1             4
[14,]          -2             1
[15,]          -3             0
[16,]           3            -7
[17,]           4            -4
[18,]          -1             5
[19,]          -2             7
[20,]           4            -2
attr(,&amp;quot;scaled:center&amp;quot;)
  Word_length Lines_in_dict 
            6             8 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;covariance-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Covariance PCA&lt;/h3&gt;
&lt;p&gt;Covariance PCA uses centered data matrix. But data matrix is not scaled. &lt;code&gt;prcomp()&lt;/code&gt; centers data by default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA
factor_scores_words = pca_words_cov$x
round(factor_scores_words,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        PC1   PC2
 [1,] -6.67  0.69
 [2,]  0.84 -0.54
 [3,] -4.68 -1.76
 [4,] -0.84  0.54
 [5,] -2.99 -2.84
 [6,]  4.99  0.38
 [7,]  0.00  0.00
 [8,] -3.07  0.77
 [9,]  4.14  0.92
[10,] -1.07 -1.69
[11,]  5.60 -2.38
[12,]  6.06  2.07
[13,] -3.91  1.30
[14,] -1.92 -1.15
[15,] -1.61 -2.53
[16,]  7.52 -1.23
[17,]  5.52  1.23
[18,] -4.76  1.84
[19,] -6.98  2.07
[20,]  3.83  2.30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observer that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as principal directions are orthogonal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;principal-directions-are-orthogonal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Principal directions are orthogonal&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#  It can also be checked that both the principal components are orthogonal.
sum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -4.773959e-15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;contribution-of-each-factor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contribution of each factor&lt;/h3&gt;
&lt;p&gt;It is defined as square of factor score divided by sum of squares of factor scores in that column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 11.36  0.18  5.58  0.18  2.28  6.34  0.00  2.40  4.38  0.29  8.00  9.37
[13]  3.90  0.94  0.66 14.41  7.78  5.77 12.43  3.75&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1]  0.92  0.55  5.98  0.55 15.49  0.28  0.00  1.13  1.63  5.48 10.87  8.25
[13]  3.27  2.55 12.32  2.90  2.90  6.52  8.25 10.18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The calculations in above two lines can be done in a single line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        PC1   PC2
 [1,] 11.36  0.92
 [2,]  0.18  0.55
 [3,]  5.58  5.98
 [4,]  0.18  0.55
 [5,]  2.28 15.49
 [6,]  6.34  0.28
 [7,]  0.00  0.00
 [8,]  2.40  1.13
 [9,]  4.38  1.63
[10,]  0.29  5.48
[11,]  8.00 10.87
[12,]  9.37  8.25
[13,]  3.90  3.27
[14,]  0.94  2.55
[15,]  0.66 12.32
[16,] 14.41  2.90
[17,]  7.78  2.90
[18,]  5.77  6.52
[19,] 12.43  8.25
[20,]  3.75 10.18&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-distance-to-center-of-gravity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared distance to center of gravity&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(dist = rowSums(factor_scores_words^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 45  1 25  1 17 25  0 10 18  4 37 41 17  5  9 58 32 26 53 20&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-cosine-of-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared cosine of observations&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sq_cos = round(factor_scores_words^2/rowSums(factor_scores_words^2)*100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      PC1 PC2
 [1,]  99   1
 [2,]  71  29
 [3,]  88  12
 [4,]  71  29
 [5,]  53  47
 [6,]  99   1
 [7,] NaN NaN
 [8,]  94   6
 [9,]  95   5
[10,]  29  71
[11,]  85  15
[12,]  90  10
[13,]  90  10
[14,]  74  26
[15,]  29  71
[16,]  97   3
[17,]  95   5
[18,]  87  13
[19,]  92   8
[20,]  74  26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nan’s are produced because of division by zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Figue 1
p = ggplot(words,aes(x = Lines_in_dict,y = Word_length,label = Words))+
  geom_point()+ geom_text_repel()+ 
  geom_hline(yintercept = 6)+geom_vline(xintercept = 8)+
  labs(x = &amp;quot;Lines in dictionary&amp;quot;,y = &amp;quot;Word length&amp;quot;)
print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Show directions of PCs
# Note that intercept argument in geom_abline considers the line to be at the origin. In our case the data are mean shifted.
# So we have to adjust the intercept taking new origin into consideration. These adjustments have been made below.
slope1 = pca_words_cov$rotation[1,1]/pca_words_cov$rotation[2,1] # Slope of first PC
slope2 = pca_words_cov$rotation[1,2]/pca_words_cov$rotation[2,2] # Slope of second PC
(new_origin = c(mean(words$Lines_in_dict),mean(words$Word_length)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intercept1 = 6 - slope1*8
intercept2 = 6 - slope2*8
p+geom_abline(slope = slope1,intercept = intercept1,linetype = &amp;quot;dashed&amp;quot;,size = 1.2,col = &amp;quot;red&amp;quot;)+
  geom_abline(slope = slope2,intercept = intercept2,linetype = &amp;quot;dashed&amp;quot;,size = 1.2,col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;
In the above figure red dashed line is the 1st principal component (PC) and blue dashed line is the 2nd PC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rotated-pcs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rotated PCs&lt;/h3&gt;
&lt;p&gt;This figure is obtained by plotting factor scores. Note that we will plot negative of the factor scores of 1st PC to make the figure consistent with the paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(as.data.frame(pca_words_cov$x),aes(-pca_words_cov$x[,1],pca_words_cov$x[,2],label = words$Words))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&amp;quot;Factor score along PC1&amp;quot;)+ylab(&amp;quot;Factor score along PC2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;with-supplementary-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;With supplementary data&lt;/h3&gt;
&lt;p&gt;Given a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-score-of-the-new-word-sur&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factor score of the new word ‘sur’&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sur = c(3,12) # It has 3 letter and 12 lines of dictionary entry
(sur_centered = sur - colMeans(words[,2:3]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Word_length Lines_in_dict 
           -3             4 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(factor_scores_sur = round(sur_centered %*% pca_words_cov$rotation,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       PC1   PC2
[1,] -4.99 -0.38&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalues-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigenvalues and variance&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt; for details.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;total-variance-before-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Total variance before transformation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(total_var_before = round(sum(diag(var(words_centered))),3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 23.368&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;total-variance-after-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Total variance after transformation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(total_var_after = round(sum(diag(var(pca_words_cov$x))),3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 23.368&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Correlation between principal components and original variables
(In the paper,this correlation is also termed loading. But we will strictly reserve the loading term to mean loading matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; (see &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34;&gt;Part-I&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The sum of correlation coefficients between variables and principal components is 1. Intuitively, this means that variables are orthogonally projected onto the principal components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation matrix&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation between PCs and original variables
(cor(pca_words_cov$x,words_centered))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Word_length Lines_in_dict
PC1   0.8679026    -0.9741764
PC2   0.4967344     0.2257884&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that the answers for correlation coefficients don’t match with that of the paper. Readers who get actual answers as given in paper are encouraged to send me an email using my contact details. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-correlation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared correlation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(cor(pca_words_cov$x,words_centered)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Word_length Lines_in_dict
PC1   0.7532549    0.94901961
PC2   0.2467451    0.05098039&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sum of correlation coefficients between variables and principal components is 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colSums((cor(pca_words_cov$x,words_centered)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Word_length Lines_in_dict 
            1             1 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading matrix&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(loading_matrix = pca_words_cov$rotation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     PC1       PC2
Word_length    0.5368755 0.8436615
Lines_in_dict -0.8436615 0.5368755&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-score-for-supplementary-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation score for supplementary variables&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Supplementary variable (Table 4)
Frequency = c(8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1)
Num_entries = c(6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2)
supp_data = data.frame(Frequency,Num_entries) # Supplementary data
supp_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Frequency Num_entries
1          8           6
2        230           3
3        700          12
4          1           2
5        500           7
6          1           1
7          9           1
8          2           6
9          1           1
10       700           5
11         7           2
12         1           1
13         4           5
14       500           9
15       900           7
16         3           1
17         1           1
18        10           4
19         1           4
20         1           2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;centered-supplementary-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Centered supplementary data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;supp_data_cent = scale(supp_data,scale = F) # Centered supplementary data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-score-for-supplementary-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation score for supplementary data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(corr_score_supp = round(cor(pca_words_cov$x,supp_data),4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Frequency Num_entries
PC1   -0.3012     -0.6999
PC2   -0.7218     -0.4493&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that correlation score doesn’t depend on whether supplementary data is centered or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(cor(pca_words_cov$x,supp_data_cent),4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Frequency Num_entries
PC1   -0.3012     -0.6999
PC2   -0.7218     -0.4493&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-correlation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared correlation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(cor(pca_words_cov$x,supp_data_cent)^2,4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Frequency Num_entries
PC1    0.0907      0.4899
PC2    0.5210      0.2019&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;column-sums-of-squared-correlation-for-support-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Column sums of squared correlation for support data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(colSums(cor(pca_words_cov$x,supp_data_cent)^2),4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Frequency Num_entries 
     0.6118      0.6918 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-circle-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation circle plot&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First plot correlation circle
x = seq(0,2*pi,length.out = 300)
circle = ggplot() + geom_path(data = data.frame(a = cos(x),b = sin(x)),
                     aes(cos(x),sin(x)),alpha = 0.3, size = 1.5)+
            geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  annotate(&amp;quot;text&amp;quot;,x = c(1.08,0.05),y = c(0.05,1.08),label = c(&amp;quot;PC1&amp;quot;,&amp;quot;PC2&amp;quot;),angle = c(0,90))+
            xlab(NULL)+ylab(NULL)
# Plotting original variables
cor_score = as.data.frame(cor(words_centered,pca_words_cov$x))
variable_plot_original = circle + geom_point(data = cor_score,  aes(cor_score[,1],cor_score[,2]))+
  geom_text_repel(aes(cor_score[,1],cor_score[,2],
                      label = c(&amp;quot;Length of words&amp;quot;,&amp;quot;Number of lines in Dict.&amp;quot;))) 
print(variable_plot_original)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-supplementary-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting supplementary variables&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;variable_plot_original+
  geom_point(data = as.data.frame(corr_score_supp),
             aes(corr_score_supp[,1],corr_score_supp[,2]))+
  geom_text_repel(aes(corr_score_supp[,1],corr_score_supp[,2],
                      label = c(&amp;quot;Frequency&amp;quot;,&amp;quot;Number of entries&amp;quot;))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observe that our correlation circle plot is flipped about y-axis (i.e., PC2) when compared to the plot given in paper. This is because our first principal component is negative of the one given in paper. So while computing correlation score, this negative principal component results in negative correlation scores. Hence, our plot flips about y-axis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-wine-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2 (Wine example)&lt;/h2&gt;
&lt;div id=&#34;correlation-pca-with-wine-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation PCA with wine data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 6
wine_type = c(paste(&amp;quot;wine&amp;quot;, 1:5, sep = &amp;quot;_&amp;quot;))
hedonic = c(14, 10, 8, 2, 6)
for_meat = c(7, 7, 5, 4, 2)
for_dessert = c(8, 6, 5, 7, 4)
price = c(7, 4, 10, 16, 13)
sugar = c(7, 3, 5, 7, 3)
alcohol = c(13, 14, 12, 11, 10)
acidity = c(7, 7, 5, 3, 3)
wine = data.frame(wine_type, hedonic, for_meat, for_dessert, price, sugar, alcohol, acidity, stringsAsFactors = F)
wine&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  wine_type hedonic for_meat for_dessert price sugar alcohol acidity
1    wine_1      14        7           8     7     7      13       7
2    wine_2      10        7           6     4     3      14       7
3    wine_3       8        5           5    10     5      12       5
4    wine_4       2        4           7    16     7      11       3
5    wine_5       6        2           4    13     3      10       3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_wine_cor = prcomp(wine[2:8],scale = T)
ggplot(as.data.frame(pca_wine_cor$x),aes(x = pca_wine_cor$x[,1],y =  pca_wine_cor$x[,2], label = paste0(&amp;quot;wine &amp;quot;,1:5)))+
  geom_point()+geom_text_repel()+ geom_vline(xintercept = 0)+ geom_hline(yintercept = 0)+
  xlab(&amp;quot;Factor score along PC1&amp;quot;)+ylab(&amp;quot;Factor score along PC2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again our figure seems upside down than that of the paper. This is a minor discrepancy. Our 2nd eigenvector is negative of the one considered in paper. We can match the plot with that of the paper by just flipping the second principal component but we will not do that here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-scores-along-1st-and-2nd-pc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factor scores along 1st and 2nd PC&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 7
(pca_wine_cor$x[,1:2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            PC1       PC2
[1,] -2.3301649  1.095284
[2,] -2.0842419 -1.223185
[3,]  0.1673228 -0.370258
[4,]  1.7842392  1.712563
[5,]  2.4628448 -1.214405&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;contribution-of-each-observation-to-principal-component&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contribution of each observation to principal component&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(pca_wine_cor$x[,1:2]^2/matrix(rep(colSums(pca_wine_cor$x[,1:2]^2),nrow(wine)),ncol = 2,byrow = T)*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       PC1   PC2
[1,] 28.50 16.57
[2,] 22.80 20.66
[3,]  0.15  1.89
[4,] 16.71 40.51
[5,] 31.84 20.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-cosine-of-observations-of-first-pc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared cosine of observations of first PC&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sq_cos = round(pca_wine_cor$x[,1:2]^2/rowSums(pca_wine_cor$x^2)*100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     PC1 PC2
[1,]  77  17
[2,]  69  24
[3,]   7  34
[4,]  50  46
[5,]  78  19&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-scores-corresponding-to-first-two-principal-components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading scores corresponding to first two principal components&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              PC1   PC2
hedonic     -0.40 -0.11
for_meat    -0.45  0.11
for_dessert -0.26  0.59
price        0.42  0.31
sugar       -0.05  0.72
alcohol     -0.44 -0.06
acidity     -0.45 -0.09&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-score-variables-with-first-two-principal-components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation score variables with first two principal components&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(corr_score_wine = round(cor(pca_wine_cor$x,wine[,2:8])[1:2,],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    hedonic for_meat for_dessert price sugar alcohol acidity
PC1   -0.87    -0.97       -0.58  0.91 -0.11   -0.96   -0.99
PC2   -0.15     0.15        0.79  0.42  0.97   -0.07   -0.12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-circle-for-wine-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation circle for wine data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Figure 6
corr_score_wine = t(corr_score_wine)
circle + 
  geom_point(data = as.data.frame(corr_score_wine),
             aes(corr_score_wine[,1],corr_score_wine[,2]))+
  geom_text_repel(aes(corr_score_wine[,1],corr_score_wine[,2],
                      label = c(&amp;quot;Hedonic&amp;quot;,&amp;quot;For Meat&amp;quot;,&amp;quot;For dessert&amp;quot;,&amp;quot;Price&amp;quot;,&amp;quot;Sugar&amp;quot;,&amp;quot;Alcohol&amp;quot;,&amp;quot;Acidity&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;varimax-rotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Varimax rotation&lt;/h3&gt;
&lt;p&gt;Rotation is applied to loading matrix such that after rotation principal components are interpretable. By interpretable, we mean, some of the loading scores will have higher values and some other loading scores will have lower values. So it can be said that the variables whose loading scores have higher value, contribute significantly towards principal components as compared to other variables with lesser loading scores. Though rotation works in certain cases, it must be remembered that it is no magic wand for principal component interpretability. One of the popular rotations is Varimax rotation. R has a built-in command to perform varimax rotation.&lt;/p&gt;
&lt;p&gt;Varimax rotation can be performed on the whole loading matrix or on a few components only. In the paper, varimax has been applied to first two principal components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-scores-of-first-two-principal-components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading scores of first two principal components&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              PC1   PC2
hedonic     -0.40 -0.11
for_meat    -0.45  0.11
for_dessert -0.26  0.59
price        0.42  0.31
sugar       -0.05  0.72
alcohol     -0.44 -0.06
acidity     -0.45 -0.09&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;varimax-applied-to-first-two-principal-components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Varimax applied to first two principal components&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rotated_loading_scores = varimax(pca_wine_cor$rotation[,1:2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-scores-after-rotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading scores after rotation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 10
(round(rotated_loading_scores$loadings[,1:2],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              PC1   PC2
hedonic     -0.41 -0.02
for_meat    -0.41  0.21
for_dessert -0.12  0.63
price        0.48  0.21
sugar        0.12  0.72
alcohol     -0.44  0.05
acidity     -0.46  0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same result can also be obtained by multiplying the original loading matrix by the rotation matrix obtained from varimax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(pca_wine_cor$rotation[,1:2] %*% rotated_loading_scores$rotmat,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             [,1]  [,2]
hedonic     -0.41 -0.02
for_meat    -0.41  0.21
for_dessert -0.12  0.63
price        0.48  0.21
sugar        0.12  0.72
alcohol     -0.44  0.05
acidity     -0.46  0.02&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-of-loading-scores-before-rotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot of loading scores before rotation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Figure 7
ggplot(as.data.frame(pca_wine_cor$rotation[,1:2]),aes(x = pca_wine_cor$rotation[,1],y = pca_wine_cor$rotation[,2],
                                                      label = c(&amp;quot;Hedonic&amp;quot;,&amp;quot;For Meat&amp;quot;,&amp;quot;For dessert&amp;quot;,&amp;quot;Price&amp;quot;,&amp;quot;Sugar&amp;quot;,&amp;quot;Alcohol&amp;quot;,&amp;quot;Acidity&amp;quot;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  xlab(&amp;quot;Loading score along PC1&amp;quot;)+ylab(&amp;quot;Loading score along PC2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-of-loading-scores-after-rotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot of loading scores after rotation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(as.data.frame(rotated_loading_scores$loadings[,1:2]),
                     aes(x = rotated_loading_scores$loadings[,1],
                         y = rotated_loading_scores$loadings[,2],
                         label = c(&amp;quot;Hedonic&amp;quot;,&amp;quot;For Meat&amp;quot;,&amp;quot;For dessert&amp;quot;,&amp;quot;Price&amp;quot;,&amp;quot;Sugar&amp;quot;,&amp;quot;Alcohol&amp;quot;,&amp;quot;Acidity&amp;quot;)))+
  geom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
    xlab(&amp;quot;Loading score along PC1 after rotation&amp;quot;)+
    ylab(&amp;quot;Loading score along PC2 after rotation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;div id=&#34;french-food-example-covariance-pca-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;French food example (Covariance PCA example)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 11 
class = rep(c(&amp;quot;Blue_collar&amp;quot;, &amp;quot;White_collar&amp;quot;, &amp;quot;Upper_class&amp;quot;), times = 4)
children = rep(c(2,3,4,5), each = 3)
bread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)
vegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)
fruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)
meat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)
poultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)
milk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)
wine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)
food = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)
food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          class children bread vegetables fruit meat poultry milk wine
1   Blue_collar        2   332        428   354 1437     526  247  427
2  White_collar        2   293        559   388 1527     567  239  258
3   Upper_class        2   372        767   562 1948     927  235  433
4   Blue_collar        3   406        563   341 1507     544  324  407
5  White_collar        3   386        608   396 1501     558  319  363
6   Upper_class        3   438        843   689 2345    1148  243  341
7   Blue_collar        4   534        660   367 1620     638  414  407
8  White_collar        4   460        699   484 1856     762  400  416
9   Upper_class        4   385        789   621 2366    1149  304  282
10  Blue_collar        5   655        776   423 1848     759  495  486
11 White_collar        5   584        995   548 2056     893  518  319
12  Upper_class        5   515       1097   887 2630    1167  561  284&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_food_cov = prcomp(food[,3:9],scale = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factor scores&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 12
(factor_scores_food = round(pca_food_cov$x[,1:2],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          PC1     PC2
 [1,] -635.05  120.89
 [2,] -488.56  142.33
 [3,]  112.03  139.75
 [4,] -520.01  -12.05
 [5,] -485.94   -1.17
 [6,]  588.17  188.44
 [7,] -333.95 -144.54
 [8,]  -57.51  -42.86
 [9,]  571.32  206.76
[10,]  -39.38 -264.47
[11,]  296.04 -235.92
[12,]  992.83  -97.15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;contribution-of-each-observation-to-principal-component-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contribution of each observation to principal component&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(pca_food_cov$x[,1:2]^2/matrix(rep(colSums(pca_food_cov$x[,1:2]^2),nrow(food)),ncol = 2,byrow = T)*100,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        PC1   PC2
 [1,] 13.34  5.03
 [2,]  7.90  6.97
 [3,]  0.42  6.72
 [4,]  8.94  0.05
 [5,]  7.81  0.00
 [6,] 11.44 12.22
 [7,]  3.69  7.19
 [8,]  0.11  0.63
 [9,] 10.80 14.71
[10,]  0.05 24.07
[11,]  2.90 19.15
[12,] 32.61  3.25&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist = pca_food_cov$x[,1]^2+pca_food_cov$x[,2]^2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-cosine-of-observations-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared cosine of observations&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sq_cos = round(pca_food_cov$x[,1:2]^2/rowSums(pca_food_cov$x^2)*100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      PC1 PC2
 [1,]  95   3
 [2,]  86   7
 [3,]  26  40
 [4,] 100   0
 [5,]  98   0
 [6,]  89   9
 [7,]  83  15
 [8,]  40  22
 [9,]  86  11
[10,]   2  79
[11,]  57  36
[12,]  97   1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-loading-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared loading scores&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Table 13
(round(pca_food_cov$rotation[,1:2]^2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            PC1  PC2
bread      0.01 0.33
vegetables 0.11 0.17
fruit      0.09 0.01
meat       0.57 0.01
poultry    0.22 0.06
milk       0.01 0.40
wine       0.00 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that this table doesn’t match with that of the paper. We will stick to our analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-score&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation score&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(corr_score_food = round((cor(pca_food_cov$x,food[,3:9])[1:2,]),2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    bread vegetables fruit meat poultry  milk  wine
PC1  0.36       0.91  0.96 1.00    0.98  0.41 -0.43
PC2 -0.87      -0.35  0.10 0.04    0.16 -0.88 -0.33&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;squared-correlation-score&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Squared correlation score&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round((cor(pca_food_cov$x,food[,3:9])[1:2,])^2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    bread vegetables fruit meat poultry milk wine
PC1  0.13       0.83  0.92    1    0.96 0.17 0.18
PC2  0.76       0.12  0.01    0    0.03 0.77 0.11&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-circle-for-food-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation circle for food data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Figure 9
corr_score_food = t(corr_score_food)
circle + geom_point(data = as.data.frame(corr_score_food), 
                    aes(x = corr_score_food[,1],y = corr_score_food[,2]))+
  geom_text_repel(data = as.data.frame(corr_score_food), 
                  aes(x = corr_score_food[,1],y = corr_score_food[,2],
                      label = c(&amp;quot;Bread&amp;quot;,&amp;quot;Vegetables&amp;quot;,&amp;quot;Fruit&amp;quot;,&amp;quot;Meat&amp;quot;,&amp;quot;Poultry&amp;quot;,&amp;quot;Milk&amp;quot;,&amp;quot;Wine&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;
Now observe that our correlation circle plot is almost close to that of the papers (though in opposite quadrants. But this is not a problem as we have previously mentioned).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigenvalues&lt;/h3&gt;
&lt;p&gt;Eigenvalues of data covariance matrix is square of singular values of centered data matrix. Hence eigenvalues of data covariance matrix can be obtained as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Table 14
cent_food = food[,3:9]-matrix(rep(colMeans(food[,3:9]),times = 12),nrow = 12,
                              byrow = T)
svd_food = svd(cent_food)
(Eigenvalues = (svd_food$d)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3023141.2354  290575.8390   68795.2333   25298.9496   22992.2474
[6]    3722.3214     723.9238&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; These eigenvalues are not the same as variance of factor scores in principal components. Variance of principal component factor scores can be obtained by dividing the eigenvalues by &lt;span class=&#34;math inline&#34;&gt;\((n-1)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is number of data points (in this case &lt;span class=&#34;math inline&#34;&gt;\(n = 12\)&lt;/span&gt;). If this point is still not clear, refer to &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentage-contribution-of-each-principal-component&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Percentage contribution of each principal component&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(Eigenvalues/sum(Eigenvalues),2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.88 0.08 0.02 0.01 0.01 0.00 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-sum-of-eigenvalues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative sum of eigenvalues&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(cumsum(Eigenvalues),2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3023141 3313717 3382512 3407811 3430804 3434526 3435250&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-percentage-contribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative percentage contribution&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(cumsum(Eigenvalues)/sum(Eigenvalues),2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.88 0.96 0.98 0.99 1.00 1.00 1.00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ress-refer-to-the-paper-for-a-description&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RESS (Refer to the paper for a description)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RESS = array(rep(0,7))
for (i in 1:7){
  RESS[i] = sum(Eigenvalues)-sum(Eigenvalues[1:i])
}
RESS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 412108.5146 121532.6756  52737.4423  27438.4927   4446.2453    723.9238
[7]      0.0000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ratio-of-ress-and-sum-of-eigenvalues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ratio of RESS and sum of eigenvalues&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(RESS/sum(Eigenvalues),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.12 0.04 0.02 0.01 0.00 0.00 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will not calculate the value of PRESS in this post as it requires us to consider random models. We will not pursue that here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R version 4.0.5 (2021-03-31)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19042)

Matrix products: default

locale:
[1] LC_COLLATE=English_India.1252  LC_CTYPE=English_India.1252   
[3] LC_MONETARY=English_India.1252 LC_NUMERIC=C                  
[5] LC_TIME=English_India.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggrepel_0.9.0 ggplot2_3.3.2

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.5       knitr_1.30       magrittr_2.0.1   tidyselect_1.1.0
 [5] munsell_0.5.0    colorspace_2.0-0 R6_2.5.0         rlang_0.4.9     
 [9] dplyr_1.0.2      stringr_1.4.0    tools_4.0.5      grid_4.0.5      
[13] gtable_0.3.0     xfun_0.22        withr_2.3.0      htmltools_0.5.0 
[17] ellipsis_0.3.1   yaml_2.2.1       digest_0.6.27    tibble_3.0.4    
[21] lifecycle_0.2.0  crayon_1.3.4     bookdown_0.21    farver_2.0.3    
[25] purrr_0.3.4      vctrs_0.3.6      glue_1.4.2       evaluate_0.14   
[29] rmarkdown_2.7    blogdown_1.3     labeling_0.4.2   stringi_1.5.3   
[33] compiler_4.0.5   pillar_1.4.7     generics_0.1.0   scales_1.1.1    
[37] pkgconfig_2.0.3 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though unusually long, I hope, this post will be of help to (courageous) readers who work there way through it till end. Comments regarding any errors or omissions may be sent to &lt;a href=&#34;https://biswajitsahoo1111.github.io/&#34;&gt;author’s&lt;/a&gt; email.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last updated: 19th January, 2020&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part II</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Principal_Component_Analysis_Part_II.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run Python code in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/zkftnkv31neuxgq/Principal_Component_Analysis_Part_II.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download Python code&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/7bzat96tt6r9iks/Principal_component_analysis_part_II.Rmd?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download R code (R Markdown)&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/u9gbbviswkfgmsj/pca_part_2_MATLAB_code.pdf?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download MATLAB code&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;This post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34;&gt;Part-I: Basic Theory of PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34;&gt;Part-III: Reproducing results of a published paper on PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions. Rather, in our opinion, it enhances understanding by knowing what happens under the hood when a built-in function is called. In actual applications, readers should always use built functions as they are robust(almost always) and tested for efficiency.&lt;/p&gt;
&lt;p&gt;This post is written in R. Equivalent &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34;&gt;MATLAB code&lt;/a&gt; for the same can be obtained from this &lt;a href=&#34;https://github.com/biswajitsahoo1111/PCA/blob/master/pca_part_II_MATLAB_codes.pdf&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use French food data form reference [2]. Refer to the paper to know about the original source of the data. We will apply different methods to this data and compare the result. As the dataset is pretty small, one way to load the data into R is to create a dataframe in R using the values in the paper. Another way is to first create a csv file and then read the file into R/MATLAB. We have used the later approach.&lt;/p&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Create a dataframe of food data
class = rep(c(&amp;quot;Blue_collar&amp;quot;, &amp;quot;White_collar&amp;quot;, &amp;quot;Upper_class&amp;quot;), times = 4)
children = rep(c(2,3,4,5), each = 3)
bread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)
vegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)
fruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)
meat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)
poultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)
milk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)
wine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)
food = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)
food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          class children bread vegetables fruit meat poultry milk wine
1   Blue_collar        2   332        428   354 1437     526  247  427
2  White_collar        2   293        559   388 1527     567  239  258
3   Upper_class        2   372        767   562 1948     927  235  433
4   Blue_collar        3   406        563   341 1507     544  324  407
5  White_collar        3   386        608   396 1501     558  319  363
6   Upper_class        3   438        843   689 2345    1148  243  341
7   Blue_collar        4   534        660   367 1620     638  414  407
8  White_collar        4   460        699   484 1856     762  400  416
9   Upper_class        4   385        789   621 2366    1149  304  282
10  Blue_collar        5   655        776   423 1848     759  495  486
11 White_collar        5   584        995   548 2056     893  518  319
12  Upper_class        5   515       1097   887 2630    1167  561  284&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Centerd data matrix
cent_food = scale(food[,3:9],scale = F)
# Scaled data matrix
scale_food = scale(food[,3:9],scale = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;covariance-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Covariance PCA&lt;/h2&gt;
&lt;div id=&#34;using-built-in-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using built-in function&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using built-in function
pca_food_cov = prcomp(food[,3:9],scale = F)
# Loading scores (we have printed only four columns out of seven)
(round(pca_food_cov$rotation[,1:4],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             PC1   PC2   PC3   PC4
bread       0.07 -0.58 -0.40  0.11
vegetables  0.33 -0.41  0.29  0.61
fruit       0.30  0.10  0.34 -0.40
meat        0.75  0.11 -0.07 -0.29
poultry     0.47  0.24 -0.38  0.33
milk        0.09 -0.63  0.23 -0.41
wine       -0.06 -0.14 -0.66 -0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor score (we have printed only four PCs out of seven)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have printed only four columns of loading scores out of seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(round(pca_food_cov$x[,1:4],2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          PC1     PC2     PC3    PC4
 [1,] -635.05  120.89  -21.14 -68.97
 [2,] -488.56  142.33  132.37  34.91
 [3,]  112.03  139.75  -61.86  44.19
 [4,] -520.01  -12.05    2.85 -13.70
 [5,] -485.94   -1.17   65.75  11.51
 [6,]  588.17  188.44  -71.85  28.56
 [7,] -333.95 -144.54  -34.94  10.07
 [8,]  -57.51  -42.86  -26.26 -46.55
 [9,]  571.32  206.76  -38.45   3.69
[10,]  -39.38 -264.47 -126.43 -12.74
[11,]  296.04 -235.92   58.84  87.43
[12,]  992.83  -97.15  121.13 -78.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have printed only four principal components out of seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variances using built-in function
(round(pca_food_cov$sdev^2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total variance
(sum(round(pca_food_cov$sdev^2,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 312295.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-of-variance-before-and-after-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison of variance before and after transformation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total variance before transformation
sum(diag(cov(food[,3:9])))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 312295.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total variance after transformation
sum(diag(cov(pca_food_cov$x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 312295.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along variables before transformation
round(diag(cov(food[,3:9])),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     bread vegetables      fruit       meat    poultry       milk       wine 
  11480.61   35789.09   27255.45  156618.39   62280.52   13718.75    5152.63 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that calculation of variance is unaffected by centering data matrix. So variance of original data matrix as well as centered data matrix is same. Check it for yourself. Now see how PCA transforms these variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along principal compoennts
round(diag(cov(pca_food_cov$x)),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      PC1       PC2       PC3       PC4       PC5       PC6       PC7 
274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can obtain the same result using built-in fucntion
round(pca_food_cov$sdev^2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;performing-covariance-pca-manually-using-svd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performing covariance PCA manually using SVD&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svd_food_cov = svd(cent_food)
# Loading scores
round(svd_food_cov$v[,1:4],2) # We have printed only four columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,]  0.07 -0.58 -0.40  0.11
[2,]  0.33 -0.41  0.29  0.61
[3,]  0.30  0.10  0.34 -0.40
[4,]  0.75  0.11 -0.07 -0.29
[5,]  0.47  0.24 -0.38  0.33
[6,]  0.09 -0.63  0.23 -0.41
[7,] -0.06 -0.14 -0.66 -0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor scores
round((cent_food %*% svd_food_cov$v)[,1:4],2) # only 4 columns printed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         [,1]    [,2]    [,3]   [,4]
 [1,] -635.05  120.89  -21.14 -68.97
 [2,] -488.56  142.33  132.37  34.91
 [3,]  112.03  139.75  -61.86  44.19
 [4,] -520.01  -12.05    2.85 -13.70
 [5,] -485.94   -1.17   65.75  11.51
 [6,]  588.17  188.44  -71.85  28.56
 [7,] -333.95 -144.54  -34.94  10.07
 [8,]  -57.51  -42.86  -26.26 -46.55
 [9,]  571.32  206.76  -38.45   3.69
[10,]  -39.38 -264.47 -126.43 -12.74
[11,]  296.04 -235.92   58.84  87.43
[12,]  992.83  -97.15  121.13 -78.39&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance of principal components
round(svd_food_cov$d^2/11,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data matrix contains 12 data points. So to find variance of principal components we have to divide the square of the diagonal matrix by 11. To know the theory behind it, refer &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34;&gt;Part-I&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performing-covariance-pca-using-eigen-decomopositionnot-recommended&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performing covariance PCA using Eigen-decomoposition(Not recommended)&lt;/h3&gt;
&lt;p&gt;This procedure is not recommended because forming a covariance matrix is computationally not efficient for large matrices if data matrix contains smaller entries. So doing eigen analysis on covariance matrix may give erroneous results. However, for our example we can use it to obtain results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eigen_food_cov = eigen(cov(cent_food))
# Loading scores
round(eigen_food_cov$vectors[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,] -0.07  0.58 -0.40 -0.11
[2,] -0.33  0.41  0.29 -0.61
[3,] -0.30 -0.10  0.34  0.40
[4,] -0.75 -0.11 -0.07  0.29
[5,] -0.47 -0.24 -0.38 -0.33
[6,] -0.09  0.63  0.23  0.41
[7,]  0.06  0.14 -0.66  0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor scores
round((cent_food %*% eigen_food_cov$vectors)[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         [,1]    [,2]    [,3]   [,4]
 [1,]  635.05 -120.89  -21.14  68.97
 [2,]  488.56 -142.33  132.37 -34.91
 [3,] -112.03 -139.75  -61.86 -44.19
 [4,]  520.01   12.05    2.85  13.70
 [5,]  485.94    1.17   65.75 -11.51
 [6,] -588.17 -188.44  -71.85 -28.56
 [7,]  333.95  144.54  -34.94 -10.07
 [8,]   57.51   42.86  -26.26  46.55
 [9,] -571.32 -206.76  -38.45  -3.69
[10,]   39.38  264.47 -126.43  12.74
[11,] -296.04  235.92   58.84 -87.43
[12,] -992.83   97.15  121.13  78.39&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along principal components
round(eigen_food_cov$values,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of using the ‘cov()’ command to find the covariance matrix manually and perform its eigen analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov_matrix_manual_food = (1/11)*t(cent_food) %*% cent_food
eigen_food_new = eigen(cov_matrix_manual_food)
# Loading scores
round(eigen_food_new$vectors[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,] -0.07  0.58 -0.40  0.11
[2,] -0.33  0.41  0.29  0.61
[3,] -0.30 -0.10  0.34 -0.40
[4,] -0.75 -0.11 -0.07 -0.29
[5,] -0.47 -0.24 -0.38  0.33
[6,] -0.09  0.63  0.23 -0.41
[7,]  0.06  0.14 -0.66 -0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along principal components
round(eigen_food_new$values,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 274831.02  26415.99   6254.11   2299.90   2090.20    338.39     65.81&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also different ways to find total variance of the data matrix. We will explore some of the options.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total varaiance before transformation
sum(diag(cov(cent_food)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 312295.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that total variance is invariant to translations. So calculating the total variance on raw data will also give the same answer. Check it to convince yourself.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlation PCA&lt;/h2&gt;
&lt;p&gt;When PCA is performed on a scaled data matrix (each variable is centered as well as variance of each variable is one), it is called correlation PCA. Before discussing correlation PCA we will take some time to see different ways in which we can obtain correlation matrix.&lt;/p&gt;
&lt;div id=&#34;different-ways-to-obtain-correlation-matrix.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Different ways to obtain correlation matrix.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using built-in command
round(cor(food[,3:9]),2)[,1:4] # We have printed only four columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           bread vegetables fruit  meat
bread       1.00       0.59  0.20  0.32
vegetables  0.59       1.00  0.86  0.88
fruit       0.20       0.86  1.00  0.96
meat        0.32       0.88  0.96  1.00
poultry     0.25       0.83  0.93  0.98
milk        0.86       0.66  0.33  0.37
wine        0.30      -0.36 -0.49 -0.44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# manually
round((1/11)*t(scale_food) %*% scale_food,2)[,1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           bread vegetables fruit  meat
bread       1.00       0.59  0.20  0.32
vegetables  0.59       1.00  0.86  0.88
fruit       0.20       0.86  1.00  0.96
meat        0.32       0.88  0.96  1.00
poultry     0.25       0.83  0.93  0.98
milk        0.86       0.66  0.33  0.37
wine        0.30      -0.36 -0.49 -0.44&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performing-correlation-pca-using-built-in-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performing correlation PCA using built-in function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_food_cor = prcomp(food[,3:9],scale = T)
# Loading scores
round(pca_food_cor$rotation[,1:4],2) # Printed only four&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             PC1   PC2   PC3   PC4
bread       0.24 -0.62  0.01 -0.54
vegetables  0.47 -0.10  0.06 -0.02
fruit       0.45  0.21 -0.15  0.55
meat        0.46  0.14 -0.21 -0.05
poultry     0.44  0.20 -0.36 -0.32
milk        0.28 -0.52  0.44  0.45
wine       -0.21 -0.48 -0.78  0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor scores
round(pca_food_cor$x[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        PC1   PC2   PC3   PC4
 [1,] -2.86  0.36 -0.40  0.36
 [2,] -1.89  1.79  1.31 -0.16
 [3,] -0.12  0.73 -1.42  0.20
 [4,] -2.04 -0.32  0.11  0.10
 [5,] -1.69  0.16  0.51  0.16
 [6,]  1.69  1.35 -0.99 -0.43
 [7,] -0.93 -1.37  0.28 -0.26
 [8,] -0.25 -0.63 -0.27  0.29
 [9,]  1.60  1.74 -0.10 -0.40
[10,]  0.22 -2.78 -0.57 -0.25
[11,]  1.95 -1.13  0.99 -0.32
[12,]  4.32  0.10  0.57  0.72&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variances along principal componentes
round(pca_food_cor$sdev^2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sum of vairances
sum(pca_food_cor$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-of-variance-before-and-after-transformation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison of variance before and after transformation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total variance before transformation
sum(diag(cov(scale_food)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Total variance after transformation
sum(diag(cov(pca_food_cor$x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along variables before transformation
round(diag(cov(scale_food)),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     bread vegetables      fruit       meat    poultry       milk       wine 
         1          1          1          1          1          1          1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is obvious as we have scaled the matrix. Now see how PCA transforms these variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along principal compoennts
round(diag(cov(pca_food_cor$x)),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; PC1  PC2  PC3  PC4  PC5  PC6  PC7 
4.33 1.83 0.63 0.13 0.06 0.02 0.00 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We can obtain the same result using built-in fucntion
round(pca_food_cor$sdev^2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;performing-correlation-pca-manually-using-svd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performing correlation PCA manually using SVD&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svd_food_cor = svd(scale_food)
# Loading scores
round(svd_food_cor$v[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1]  [,2]  [,3]  [,4]
[1,]  0.24 -0.62  0.01 -0.54
[2,]  0.47 -0.10  0.06 -0.02
[3,]  0.45  0.21 -0.15  0.55
[4,]  0.46  0.14 -0.21 -0.05
[5,]  0.44  0.20 -0.36 -0.32
[6,]  0.28 -0.52  0.44  0.45
[7,] -0.21 -0.48 -0.78  0.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor scores
round((scale_food %*% svd_food_cor$v)[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       [,1]  [,2]  [,3]  [,4]
 [1,] -2.86  0.36 -0.40  0.36
 [2,] -1.89  1.79  1.31 -0.16
 [3,] -0.12  0.73 -1.42  0.20
 [4,] -2.04 -0.32  0.11  0.10
 [5,] -1.69  0.16  0.51  0.16
 [6,]  1.69  1.35 -0.99 -0.43
 [7,] -0.93 -1.37  0.28 -0.26
 [8,] -0.25 -0.63 -0.27  0.29
 [9,]  1.60  1.74 -0.10 -0.40
[10,]  0.22 -2.78 -0.57 -0.25
[11,]  1.95 -1.13  0.99 -0.32
[12,]  4.32  0.10  0.57  0.72&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance along each principcal component
round(svd_food_cor$d^2/11,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sum of variances
sum(svd_food_cor$d^2/11)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we have to divide by 11 to get eigenvalues of correlation matrix. Check the formulation of correlation matrix using scaled data matrix to convince yourself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-eigen-decomposition-not-recommended&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using eigen-decomposition (Not Recommended)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eigen_food_cor = eigen(cor(food[,3:9]))
# Loading scores
round(eigen_food_cor$vectors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    0    1    0   -1    0    1    0
[2,]    0    0    0    0    1    0    0
[3,]    0    0    0    1    0    1    0
[4,]    0    0    0    0    0    0   -1
[5,]    0    0    0    0    0    0    1
[6,]    0    1    0    0    0    0    0
[7,]    0    0   -1    0    0    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Factor scores
round((scale_food %*% eigen_food_cor$vectors)[,1:4],2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       [,1]  [,2]  [,3]  [,4]
 [1,]  2.86 -0.36 -0.40  0.36
 [2,]  1.89 -1.79  1.31 -0.16
 [3,]  0.12 -0.73 -1.42  0.20
 [4,]  2.04  0.32  0.11  0.10
 [5,]  1.69 -0.16  0.51  0.16
 [6,] -1.69 -1.35 -0.99 -0.43
 [7,]  0.93  1.37  0.28 -0.26
 [8,]  0.25  0.63 -0.27  0.29
 [9,] -1.60 -1.74 -0.10 -0.40
[10,] -0.22  2.78 -0.57 -0.25
[11,] -1.95  1.13  0.99 -0.32
[12,] -4.32 -0.10  0.57  0.72&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variances along each principal component
round(eigen_food_cor$values,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this post would help clear some of the confusions that a beginner might have while encountering PCA for the first time. Please send me a note if you find any errors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.&lt;/li&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part I</title>
      <link>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post, we will discuss about Principal Component Analysis (PCA),
one of the most popular dimensionality reduction techniques used in
machine learning. Applications of PCA and its variants are ubiquitous.
Thus, a through understanding of PCA is considered essential to start
one’s journey into machine learning. In this and subsequent posts, we
will first briefly discuss relevant theory of PCA. Then we will
implement PCA from scratch without using any built-in function. This
will give us an idea as to what happens under the hood when a built-in
function is called in any software environment. Simultaneously, we will
also show how to use built-in commands to obtain results. Finally, we
will reproduce the results of a popular paper on PCA. Including all this
in a single post will make it very very long. Therefore, the post has
been divided into three parts. Readers totally familiar with PCA should
read none and leave this page immediately to save their precious time.
Other readers, who have a passing knowledge of PCA and want to see
different implementations, should pick and choose material from
different parts as per their need. Absolute beginners should start with
Part-I and work their way through gradually. Beginners are also
encouraged to explore the references at the end of this post for further
information. Here is the outline of different parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/&#34;&gt;Part-I: Basic Theory of
PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II: PCA Implementation with and without using built-in
functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34;&gt;Part-III: Reproducing results of a published paper on
PCA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt;,
Python, R, and MATLAB code are available to reproduce all the results.
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/&#34;&gt;Part-III&lt;/a&gt;
contains both R and Python code to reproduce results of the paper. In
this post, we will discuss the theory behind PCA in brief.&lt;/p&gt;
&lt;div id=&#34;principal-component-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Principal Component Analysis&lt;/h1&gt;
&lt;div id=&#34;theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theory:&lt;/h2&gt;
&lt;p&gt;Given a data matrix, we apply PCA to transform it in a way such that the
transformed data reveals maximum information. So we have to first get
the data on which we want to perform PCA. The usual convention in
storing data is to place variables as columns and different observations
as rows (Data frames in R follow this convention by default). For
example, let’s suppose we are collecting data about daily weather for a
year. Our variables of interest may include maximum temperature in a
day, minimum temperature, humidity, max. wind speed, etc. Everyday we
collect observations for each of these variables. In vector form, our
data point for one day will contain number of observations equal to the
number of variables under study and this becomes one row of our data
matrix. Assuming that we are observing 10 variables everyday, our data
matrix for one year (assuming it’s not a leap year) will contain 365
rows and 10 columns. Once data matrix is obtained, further analysis is
done on this data matrix to obtain important hidden information
regarding the data. We will use notations from matrix theory to simplify
our analysis.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt; be the data matrix of size &lt;span class=&#34;math inline&#34;&gt;\(n\times p\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is
the number of data points and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of variables. We can
assume without any loss of generality that data is centered, meaning its
column means are zero. This only shifts the data towards the origin
without changing their relative orientation. So if originally not
centered, it is first centered before doing PCA. From now onward we will
assume that data matrix is always centered.&lt;/p&gt;
&lt;p&gt;Variance of a variable (a column) in &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt; is equal to sum of
squares of entries (because the column is centered) of that column
divided by (n - 1) (to make it unbiased). So sum of variance of all
variables is &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n - 1}\)&lt;/span&gt; times sum of squares of all elements of
the matrix . Readers who are familiar with matrix norms would instantly
recognize that total variance is &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n - 1}\)&lt;/span&gt; times the square of
&lt;strong&gt;Frobenius norm&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;. Frobenius norm is nothing but square
root of sum of squares of all elements of a matrix.
&lt;span class=&#34;math display&#34;&gt;\[ \|\textbf{X}\|_{F} = (\sum_{i,j}{x_{ij}^2})^{\frac{1}{2}}=trace(\textbf{X}^T\textbf{X})=trace(\textbf{X}\textbf{X}^T)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using this definition, total variance before transformation =
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n-1}\sum_{i,j}{x_{ij}^2}=trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})=\frac{1}{n-1}\|\textbf{X}\|_{F}^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, trace of a matrix is the sum of its diagonal entries and
&lt;span class=&#34;math inline&#34;&gt;\(\|\textbf{X}\|_{F}^2\)&lt;/span&gt; is the square of &lt;strong&gt;Frobenius norm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The aim of PCA is to transform the data in such a way that along first
principal direction, variance of transformed data is maximum. It
subsequently finds second principal direction orthogonal to the first
one in such a way that it explains maximum of the remaining variance
among all possible direction in the orthogonal subspace.&lt;/p&gt;
&lt;p&gt;In matrix form the transformation can be written as
&lt;span class=&#34;math display&#34;&gt;\[\textbf{Y}_{n\times p}=\textbf{X}_{n\times p}\textbf{P}_{p\times p}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; is the transformed data matrix. The columns of
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; are called principal components and &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is usually
called loading matrix. Our aim is to find matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt;. Once we
find &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; we can then find &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; just by a matrix
multiplication. We will show in the next section that matrix
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is the eigenvector matrix of the covariance matrix. Before
that, let’s first define the covariance matrix.&lt;/p&gt;
&lt;p&gt;Given a data matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;(centered), its covariance matrix
&lt;span class=&#34;math inline&#34;&gt;\((\textbf{S})\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}\]&lt;/span&gt;
Now we will show how to compute the loading vectors (columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt;) and consequently the principal components (columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;) from the given centered data matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;sketch-of-the-proof&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sketch of the Proof&lt;/h3&gt;
&lt;p&gt;We call it a sketch because we will not be giving the full proof.
Rather, we will give the proof only for the first principal component
and then give a commentary as to how it can be extended for other
principal components.&lt;/p&gt;
&lt;p&gt;The first principal component is the result obtained by transforming
original data matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{n\times p}\)&lt;/span&gt; in such a way that
variance of data along first principal component is the highest. The
transformation is a linear transformation that is obtained by taking
linear combination of the columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{n\times p}\)&lt;/span&gt;. The
coefficients of the linear combination are called loading scores
corresponding to original variables of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{n\times p}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assuming &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}_{p\times 1} = [\alpha_1, \alpha_2, ..., \alpha_p]^T\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_2\)&lt;/span&gt;, … , &lt;span class=&#34;math inline&#34;&gt;\(\alpha_p\)&lt;/span&gt; are scalars, to be the
loading vector (we don’t know, as of now, from where to get
&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{p \times 1}\)&lt;/span&gt;. We will find that out shortly.), first principal
component is obtained by the the product
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1}\)&lt;/span&gt;. This product can be written
as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{X}_{n\times p}\boldsymbol{\alpha}_{p\times 1} = \alpha_1 \textbf{X}_{[:,1]} +\alpha_2 \textbf{X}_{[:,2]} + ...  + \alpha_p \textbf{X}_{[:,p]} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{[:,1]}\)&lt;/span&gt; is the first column of
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}_{n\times p}\)&lt;/span&gt;. Similarly for other columns. The above
equation makes it clear as to why first principal component is a linear
combination of variables of original data matrix. In the original data
matrix, each column corresponds to a variable.&lt;/p&gt;
&lt;p&gt;Variance of first principal component is given by
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\)&lt;/span&gt; (As the columns are already
centered. We have also ignored the factor &lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{n-1})\)&lt;/span&gt; as it is
just a scaling factor.). Now our goal is to find an &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}_{p\times 1}\)&lt;/span&gt;
that maximizes &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\)&lt;/span&gt;. As
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}_{p\times 1}\)&lt;/span&gt; is arbitrary, we can choose its entries in such a
way that variance increases as much as we please. So to get any
meaningful solution, we have to apply some constraints on
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}_{p\times 1}\)&lt;/span&gt;. The conventional condition is
&lt;span class=&#34;math inline&#34;&gt;\(\|\boldsymbol{\alpha}_{p\times 1}\|^2 = 1\)&lt;/span&gt;. The optimization problem becomes
&lt;span class=&#34;math display&#34;&gt;\[ maximize \ \ \   \boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[s.t. \|\boldsymbol{\alpha}\|^2 = 1\]&lt;/span&gt; Using Lagrange multipliers, this problem can
be written as
&lt;span class=&#34;math display&#34;&gt;\[maximize \ \ \  \mathcal{L}(\boldsymbol{\alpha}, \lambda)=\boldsymbol{\alpha}^T\textbf{X}^T \textbf{X}\boldsymbol{\alpha} + \lambda (1 - \boldsymbol{\alpha}^T\boldsymbol{\alpha})\]&lt;/span&gt;
Taking gradient of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(\boldsymbol{\alpha}, \lambda)\)&lt;/span&gt; with respect to
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt; we get, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}\)&lt;/span&gt;. So
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt; is the eigenvector of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\)&lt;/span&gt;. It turns out
that for first principal component, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt; is the eigenvector
corresponding to the largest eigenvalue.&lt;/p&gt;
&lt;p&gt;Loading vector for second principal component is computed with the added condition that second loading vector is orthogonal to the first one. With little bit of more work it can be shown that loading vectors for successive principal components are obtained from eigenvectors corresponding to eigenvalues in decreasing order. More details can be found in reference [1].&lt;/p&gt;
&lt;p&gt;Now, it is straightforward to first form the covariance matrix and by
placing its eigenvectors as columns, we can find matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; and
consequently the principal components. The eigenvectors are arranged in
such a way that first column is the eigenvector corresponding to largest
eigenvector, second column (second eigenvector) corresponds to second
largest eigenvalue and so on. Here we have assumed that we will always
be able to find all the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; orthogonal eigenvectors. In fact, we will
always be able to find &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; orthogonal eigenvectors as the matrix is
symmetric. It can also be shown that the transformed matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;
is centered and more remarkably, total variance of columns of
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; is same as total variance of columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;. We
will prove these two propositions as the proofs are short.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-pca-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Properties of PCA Transformation&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Principal components are centered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}\)&lt;/span&gt; be a column vector of all ones of size
&lt;span class=&#34;math inline&#34;&gt;\((n\times 1)\)&lt;/span&gt;. To prove that columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; are centered,
just premultiply it by &lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}^T\)&lt;/span&gt; (this finds column sum for
each column). So
&lt;span class=&#34;math display&#34;&gt;\[\textbf{1}^T \textbf{Y} = \textbf{1}^T\textbf{X}\textbf{P}\]&lt;/span&gt; But
columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt; are already centered, so
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}^T\textbf{X}=\textbf{0}\)&lt;/span&gt;. Thus
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}^T \textbf{Y}= \textbf{0}\)&lt;/span&gt;. Hence columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;
are centered.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sum of variance of principal components is equal to sum of variance
of variables before transformation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: To prove that total variance of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; also remains
same, observe that&lt;/p&gt;
&lt;p&gt;total covariance of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; =
&lt;span class=&#34;math display&#34;&gt;\[trace(\frac{1}{n-1}\textbf{Y}^{T}\textbf{Y})=\frac{1}{n-1}trace((\textbf{P}^T\textbf{X}^{T}\textbf{X})\textbf{P})=\\\frac{1}{n-1}trace((\textbf{P}\textbf{P}^T)\textbf{X}^{T}\textbf{X})=trace(\frac{1}{n-1}\textbf{X}^T\textbf{X})\]&lt;/span&gt;
The previous equation uses the fact that trace is
commutative(i.e.&lt;span class=&#34;math inline&#34;&gt;\(trace(\textbf{AB})=trace(\textbf{BA})\)&lt;/span&gt;) and
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is orthogonal (i.e.
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\textbf{P}^T=\textbf{I}\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Principal components are orthogonal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: To prove the above claim, it is sufficient to show that
the matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}^T\textbf{Y}\)&lt;/span&gt; is diagonal. Remember that columns of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; are principal components. So if we can somehow show &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}^T\textbf{Y}\)&lt;/span&gt; to be diagonal, it would automatically mean that principal components are orthogonal.
We know, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y} = \textbf{X}\textbf{P}\)&lt;/span&gt;. So &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}^T\textbf{Y} = \textbf{P}^T\textbf{X}^T\textbf{X}\textbf{P}\)&lt;/span&gt;. From sketch of the proof, we know that &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is orthogonal as we have required successive loading vectors to be orthogonal to previous ones. We also know that &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is the eigenvector matrix of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\)&lt;/span&gt;. So from &lt;a href=&#34;https://mathworld.wolfram.com/EigenDecompositionTheorem.html&#34;&gt;Eigen Decomposition Theorem&lt;/a&gt;, it follows that &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}^T(\textbf{X}^T\textbf{X})\textbf{P}\)&lt;/span&gt; is diagonal as &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is the eigenvector matrix of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt; is orthogonal (so &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}^{-1} = \textbf{P}^T\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;link-between-total-variance-and-eigenvalues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Link between total variance and eigenvalues&lt;/h3&gt;
&lt;p&gt;Total variance is sum of eigenvalues of covariance matrix
&lt;span class=&#34;math inline&#34;&gt;\((\textbf{S})\)&lt;/span&gt;. This follows from the fact that &lt;span style=&#34;color: hotpink&#34;&gt;&lt;em&gt;trace of a matrix is sum of its eigenvalues&lt;/em&gt;&lt;/span&gt;. Total variance of original data matrix is &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}trace(\textbf{X}^T\textbf{X}) =trace(\frac{1}{n-1}\textbf{X}^T\textbf{X}) = trace(\textbf{S})\)&lt;/span&gt;. We will show these calculations using a publicly available dataset in
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variations-of-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variations of PCA&lt;/h3&gt;
&lt;p&gt;Sometimes our data matrix contains variables that are measured in
different units. So we might have to scale the centered matrix to reduce
the effect of variables with large variation. So depending on the matrix
on which PCA is performed, it is divided into two types.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Covariance PCA (Data matrix is centered but &lt;strong&gt;not&lt;/strong&gt; scaled)&lt;/li&gt;
&lt;li&gt;Correlation PCA (Data matrix is centered and scaled)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of these two types can be found in
&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/&#34;&gt;Part-II&lt;/a&gt;.
Please note that the above two variations are just two among many
variations. There are &lt;strong&gt;Sparse PCA&lt;/strong&gt;, &lt;strong&gt;Kernel PCA&lt;/strong&gt;, &lt;strong&gt;Robust PCA&lt;/strong&gt;,
&lt;strong&gt;Non-negative PCA&lt;/strong&gt; and many others. We have mentioned the two that are
most widely used.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-common-terminologies-associated-with-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some common terminologies associated with PCA&lt;/h3&gt;
&lt;p&gt;In literature, there is no standard terminology for different terms in
PCA. Different people use different (often contradictory) terminology
thus confusing newcomers. Therefore, it is better to stick to one set of
terminologies and notations and use those consistently. We will stick to
the terminology used in reference [2].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Factor scores&lt;/strong&gt; corresponding to a principal component: Values of
that column of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt; that corresponds to the desired
principal component.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Loading score&lt;/strong&gt;: Values corresponding to a column of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt;.
For example,loading scores of variables corresponding to first
principal component are the values of the first column of
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Inertia&lt;/strong&gt;: Square of Frobenius norm of the matrix.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-actually-are-principal-components-computed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How actually are principal components computed?&lt;/h3&gt;
&lt;p&gt;The previously stated method of finding eigenvectors of covariance
matrix is not computationally efficient. In practice, singular value
decomposition (SVD) is used to compute the matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{P}\)&lt;/span&gt;. SVD
theorem tells that any real matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt; can be decomposed into
three matrices such that &lt;span class=&#34;math display&#34;&gt;\[ \textbf{X} = \textbf{U}\Sigma\textbf{V}^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt; is of size &lt;span class=&#34;math inline&#34;&gt;\(n\times p\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\textbf{U}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{V}\)&lt;/span&gt; are orthogonal matrices of size &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\times p\)&lt;/span&gt;
respectively. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is a diagonal matrix of size &lt;span class=&#34;math inline&#34;&gt;\(n\times p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given the SVD decomposition of a matrix &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\textbf{X}^T\textbf{X}=\textbf{V}\Sigma^2\textbf{V}^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the eigen-decomposition of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\)&lt;/span&gt;. So
&lt;span class=&#34;math inline&#34;&gt;\(\textbf{V}\)&lt;/span&gt; is the eigenvector matrix of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}^T\textbf{X}\)&lt;/span&gt;. For
PCA we need eigenvector matrix of covariance matrix. So converting the
equation into convenient form, we get
&lt;span class=&#34;math display&#34;&gt;\[\textbf{S} = \frac{1}{n-1}\textbf{X}^T\textbf{X}=\textbf{V}(\frac{1}{n-1}\Sigma^2)\textbf{V}^T\]&lt;/span&gt;
Thus eigenvalues of S are diagonal entries of &lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{n-1}\Sigma^2)\)&lt;/span&gt;.
As SVD is computationally efficient, all built-in functions use SVD to
compute the loading matrix and then use the loading matrix to find
principal components.&lt;/p&gt;
&lt;p&gt;In the interest of keeping the post at a reasonable length, we will stop
our exposition of theory here. Whatever we have discussed is only a
fraction of everything. Entire books have been written on PCA.
Interested readers who want to pursue this further can refer the
references of this post as a starting point. Readers are encouraged to
bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;Last modified: May 5, 2021&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New
York,2002.&lt;/li&gt;
&lt;li&gt;Abdi, H., &amp;amp; Williams, L. J. (2010). Principal component analysis.
Wiley interdisciplinary reviews: computational statistics, 2(4),
433-459.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data-Driven Remaining Useful Life (RUL) Prediction</title>
      <link>https://biswajitsahoo1111.github.io/project/rul_codes_open/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/project/rul_codes_open/</guid>
      <description>

&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;left&#34;&gt;
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=rul_codes_open&amp;type=star&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;

  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  
  &lt;!----
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
  &lt;/td&gt;
  ----&gt;
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/rul_codes_open/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download all code (.zip)&lt;/a&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;right&#34;&gt;  
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=rul_codes_open&amp;type=fork&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Remaining useful life (RUL) prediction is the study of predicting when something is going to fail, given its present state. The problem has a prophetic charm associated with it. While a soothsayer can make a prediction about almost anything (including RUL of a machine) confidently, many people will not accept the prediction because of its lack of scientific basis. Here, we will try to solve the problem with scientific reasoning.&lt;/p&gt;

&lt;p&gt;A component (or a machine) is said to have failed when it can no longer perform its desired task to the satisfaction of the user. For example, Li-Ion battery of an electric vehicle is said to have failed when it requires frequent recharging to travel a small distance. Similarly, a bearing of a machine is said to have failed, if level of vibration produced at the bearing goes above some acceptable limit. Other examples can be thought of for different applications. The goal then is to predict beforehand when something is going to fail. Knowledge of a component&amp;rsquo;s expected time of failure will help us prepare well for the inevitable. In industrial setting, where any unplanned shutdown of a critical component has huge monetary cost, knowing when a machine is going to fail will result in significant monetary gains.&lt;/p&gt;

&lt;p&gt;There are many techniques developed over the years to predict RUL of a component. All those techniques can be broadly divided into two categories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model Based Methods&lt;/li&gt;
&lt;li&gt;Data-Driven Methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In model based methods, we try to formulate a mathematical model of the system under consideration. Then using that model we try to predict RUL of the component. Though model based methods are used in some cases, there are many other applications where formulating a full mathematical model of the system is extremely difficult. In some cases, the underlying physics is so complex that we have to make many simplifying assumptions. Whether the simplifying assumptions are justified or not is determined by collecting real data from the machine. Therefore, it requires extensive domain knowledge and thus is a territory of only a select few who can actually do these things.&lt;/p&gt;

&lt;p&gt;In contrast, in data-driven methods all information about a machine is gained from the data collected from it. With readily available sensors we can collect huge amounts of data for almost any application. By analyzing that data we can get an idea about the condition of the machine. That will help us in making an informed decision about the RUL of the machine. In this process we make no assumptions about the machine. Increasingly, data-driven methods are getting better at making reliable predictions. As the name of the project suggests, we will only focus on data-driven methods for RUL prediction. The problem of RUL prediction is also know as prognosis in some fields. Some people also call it prognostics. We will only use the term RUL prediction. In the beginning, we will mainly focus on predicting RUL of mechanical components. Later we will explore other application areas.&lt;/p&gt;

&lt;h3 id=&#34;aim-of-the-project&#34;&gt;Aim of the project&lt;/h3&gt;

&lt;p&gt;Like my previous project on &lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;fault diagnosis&lt;/a&gt;, aim of this project is to produce reproducible results for RUL prediction. RUL prediction is a broad subject that can be applied to many problems such as RUL prediction of Li-Ion batteries, RUL prediction of machinery bearings, RUL prediction of machine tool, etc. We will start with mechanical applications and then gradually move to other applications over time. As our aim is reproducibility, we will use publicly available datasets. Interested readers can download the data and use our code to get exact results as we have obtained. As we will use well known datasets, readers might observe that, in some cases, our results are in fact worse than some reported results elsewhere. Our goal is not to verify someone else&amp;rsquo;s claim. If someone else claims a better result, onus is on them to demonstrate their result. Here, whatever results I have claimed can be reproduced by readers by just running the jupyter notebooks after downloading relevant data.&lt;/p&gt;

&lt;p&gt;This is an ongoing project and modifications and additions of new techniques will be done over time. &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; are two popular programming languages that are used in machine learning applications. We will use &lt;strong&gt;Python&lt;/strong&gt; to demonstrate our results. At a later stage we might add equivalent &lt;strong&gt;R&lt;/strong&gt; code. To implement deep learning models, we will use &lt;strong&gt;Tensorflow&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;results-using-nasa-s-turbofan-engine-degradation-dataset-https-ti-arc-nasa-gov-tech-dash-groups-pcoe-prognostic-data-repository-turbofan&#34;&gt;Results using &lt;a href=&#34;https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan&#34;&gt;NASA&amp;rsquo;s Turbofan Engine Degradation Dataset&lt;/a&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;We will first apply classical machine learning methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;Dataset description and preprocessing&lt;/a&gt; steps can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;this link&lt;/a&gt;. We will use the same preprocessing steps, with minor changes, in all notebooks. We strongly encourage readers to first go over &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;data preparation notebook&lt;/a&gt; before using results notebooks. In the table below, we report Root Mean Square Error (RMSE) values. &lt;span style=&#34;color:blue&#34;&gt;Click on the numbers in the table to view corresponding notebooks&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note on last column of following table&lt;/strong&gt;: The last column specifies the degradation model used in the notebooks. There are two common degradation models that are used for this particular turbofan dataset: Linear degradation model and Piecewise linear degradation model. For more details about both, see &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;this&lt;/a&gt;. When we use piecewise linear degradation model, we have to assume an early RUL value. This is nothing but the value of RUL that is assumed when the component is relatively new. In literature, different people use different early RUL values. In our examples, when we specify an early RUL value, that means that we apply the same early RUL across all 4 datasets.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD001&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD002&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD003&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD004&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Degradation Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Gradient Boosting&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;19.06&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;28.97&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;20.55&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;29.49&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Random Forest&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;19.15&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;29.00&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;20.53&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;29.75&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Gradient Boosting&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_xgboost_linear_degradation_model.ipynb&#34;&gt;33.24&lt;/a&gt;&lt;sup&gt;* &lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_xgboost_linear_degradation_model.ipynb&#34;&gt;29.88&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_xgboost_linear_degradation_model.ipynb&#34;&gt;47.94&lt;/a&gt;&lt;sup&gt;* &lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_xgboost_linear_degradation_model.ipynb&#34;&gt;40.34&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;See the notebook to get a complete picture.&lt;/p&gt;

&lt;h2 id=&#34;enter-deep-learning&#34;&gt;Enter Deep Learning&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;In this section, we will apply deep learning to predict RUL of Turbofan dataset. Due to the nondeterministic nature of operations used in deep learning and dependence of libraries like &lt;code&gt;Tensorflow&lt;/code&gt; on computer architecture, readers might obtain slightly different results than those in the notebooks. For reproducibility of our results, we also share the saved models of each notebook. All saved models for Turbofan dataset can be found at this &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/tree/master/saved_models/cmapss&#34;&gt;link&lt;/a&gt;. A notebook describing the steps to use the saved models can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_using_saved_model_deep_learning.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD001&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD002&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD003&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD004&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Degradation Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;LSTM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_LSTM_piecewise_linear_degradation_model.ipynb&#34;&gt;15.16&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_LSTM_piecewise_linear_degradation_model.ipynb&#34;&gt;15.54&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1D CNN&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_1D_CNN_piecewise_linear_degradation_model.ipynb&#34;&gt;15.84&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_1D_CNN_piecewise_linear_degradation_model.ipynb&#34;&gt;15.78&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(This table will be updated gradually.)&lt;/p&gt;

&lt;h2 id=&#34;some-other-related-stuff&#34;&gt;Some other related stuff&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;Data-Driven Machinery Fault Diagnosis&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/&#34;&gt;Fault diagnosis of machines (A non-technical introduction)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/categories/blog/&#34;&gt;Blog articles by yours truly&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;why-have-i-used-only-jupyter-notebooks&#34;&gt;Why have I used only Jupyter notebooks?&lt;/h3&gt;

&lt;p&gt;These notebooks are for educational purposes only. Our experiments are relatively small scale and can be run in a reasonable amount of time in a notebook. I personally love the interactive nature of jupyter notebooks. We can see what we are doing. So the answer to the above question is: personal choice. I also don&amp;rsquo;t intend to deploy these, at least for the time being, in a production environment. Readers who wish to build deployment ready systems should bear in mind that they have to do many other things than just run an algorithm in a jupyter notebook.&lt;/p&gt;

&lt;p&gt;For attribution, cite this project as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{sahoo2018datadrivenrul,
  author = {Sahoo, Biswajit},
  title = {Data-Driven Remaining Useful Life (RUL) Prediction},
  url = {https://biswajitsahoo1111.github.io/rul_codes_open/},
  year = {2018}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readers should cite original datasets separately.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data-Driven Machinery Fault Diagnosis</title>
      <link>https://biswajitsahoo1111.github.io/project/cbm_codes_open/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/project/cbm_codes_open/</guid>
      <description>

&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;left&#34;&gt;
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=cbm_codes_open&amp;type=star&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  &lt;!----
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
  &lt;/td&gt;
  ----&gt;
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/cbm_codes_open/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download all code (.zip)&lt;/a&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;right&#34;&gt;  
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=cbm_codes_open&amp;type=fork&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Condition based maintenance (CBM) is the process of doing maintenance only when it is required. Adoption of this maintenance strategy leads to significant monetary gains as it precludes periodic maintenance and reduces unplanned downtime. Another term commonly used for condition based maintenance is predictive maintenance. As the name suggests, in this method we predict in advance when to perform maintenance. Maintenance is required, if fault has already occurred or is imminent. This leads us to the problem of fault diagnosis and prognosis.&lt;/p&gt;

&lt;p&gt;In fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognosis, our aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model Based Approaches&lt;/li&gt;
&lt;li&gt;Data-Driven Approaches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In model based approach, a complete model of the system is formulated and it is then used for fault diagnosis and prognosis. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modeling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.&lt;/p&gt;

&lt;p&gt;In data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognosis. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognosis tasks.&lt;/p&gt;

&lt;h2 id=&#34;aim-of-the-project&#34;&gt;Aim of the project&lt;/h2&gt;

&lt;p&gt;In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.&lt;/p&gt;

&lt;p&gt;This is an ongoing project and modifications and additions of new techniques will be done over time. &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; are two popular programming languages that are used in machine learning applications. We will use those for our demonstrations. &lt;strong&gt;Tensorflow&lt;/strong&gt; will be used for deep learning applications. This page contains results on fault diagnosis only. Results on fault prognosis can be found &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;results-using-case-western-reserve-university-bearing-data-https-csegroups-case-edu-bearingdatacenter-pages-welcome-case-western-reserve-university-bearing-data-center-website-sup-sup&#34;&gt;Results using &lt;a href=&#34;https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-western-reserve-university-bearing-data-center-website&#34;&gt;Case Western Reserve University Bearing Data&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;We will first apply classical feature based methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. In feature based methods, we will extensively use &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_energy_features.ipynb&#34;&gt;wavelet packet energy features&lt;/a&gt; and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_entropy_features.ipynb&#34;&gt;wavelet packet entropy featues&lt;/a&gt; that are calculated from raw time domain data. Dataset description and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/CWRU_time_domain_data_preprocessing.ipynb&#34;&gt;time domain preprocessing&lt;/a&gt; steps can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/CWRU_time_domain_data_preprocessing.ipynb&#34;&gt;here&lt;/a&gt;. Steps to &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Calculating_time_domain_features_CWRU.ipynb&#34;&gt;compute time domain features&lt;/a&gt; are explained in &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Calculating_time_domain_features_CWRU.ipynb&#34;&gt;this notebook&lt;/a&gt;. The procedure detailing calculation of wavelet packet energy features can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_energy_features.ipynb&#34;&gt;this link&lt;/a&gt; and similar calculations for wavelet packet entropy features can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_entropy_features.ipynb&#34;&gt;this link&lt;/a&gt;. Also see the following two notebooks for computation of wavelet packet features in Python: &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Wavelet_packet_energy_features_python.ipynb&#34;&gt;Wavelet packet energy features in Python&lt;/a&gt; and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Wavelet_packet_entropy_features_python.ipynb&#34;&gt;Wavelet packet entropy features in Python&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time_cwru_python.ipynb&#34;&gt;SVM on time domain
features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;96.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru_python.ipynb&#34;&gt;SVM on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.3%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Visualizing High Dimensional Data Using Dimensionality Reduction Techniques&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Python Code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/dimensionality_reduction_projection.pdf&#34;&gt;R Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru_python.ipynb&#34;&gt;SVM on wavelet packet entropy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.3%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru_python.ipynb&#34;&gt;SVM on time and wavelet packet features&lt;/a&gt; (12 classes, sampling frequency: 12k) (&lt;strong&gt;Achieves 100% test accuracy in one case&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_python.ipynb&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k_python.ipynb&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.7%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k_python.ipynb&#34;&gt;LDA on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;89.8%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k_python.ipynb&#34;&gt;LDA on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k_python.ipynb&#34;&gt;QDA on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;96.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k_python.ipynb&#34;&gt;QDA on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k_python.ipynb&#34;&gt;kNN on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;89.8%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k_python.ipynb&#34;&gt;kNN on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k_python.ipynb&#34;&gt;Decision tree on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;94.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k_python.ipynb&#34;&gt;Decision tree on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.7%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k_python.ipynb&#34;&gt;Bagging on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;97%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k_python.ipynb&#34;&gt;Bagging on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k_python.ipynb&#34;&gt;Boosting on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k_python.ipynb&#34;&gt;Boosting on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k_python.ipynb&#34;&gt;Random forest on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.1%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k_python.ipynb&#34;&gt;Random forest on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; This hyperlink points to the actual website of CWRU bearing dataset. Unfortunately, it has been recently observed that the original website remains down most of the time. As the dataset is well known, it can still be found on the internet at different places. Interested readers who want to experiment with this dataset can find it &lt;a href=&#34;https://data.mendeley.com/datasets/fkp3nn4tp7/1#folder-a8bb9715-4b7b-4fa1-8550-5b0cdcf62602&#34;&gt;here&lt;/a&gt; (If it&amp;rsquo;s not down). Actual data are stored in &lt;code&gt;.mat&lt;/code&gt; format. But the data in previous link are first extracted from &lt;code&gt;.mat&lt;/code&gt; format and then individually stored in &lt;code&gt;.csv&lt;/code&gt; format. Readers should first try to download the data from the original website. If that attempt fails, they should explore other options.&lt;/p&gt;

&lt;h2 id=&#34;enter-deep-learning&#34;&gt;Enter Deep Learning&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;In this section, we will show results of fault diagnosis task using deep learning on the same Case Western Reserve University bearing dataset. Due to the nondeterministic nature of operations used in deep learning and dependence of libraries like &lt;code&gt;Tensorflow&lt;/code&gt; on computer architecture, readers might obtain slightly different results than those in the notebooks. As a more reliable measure, we report average results of ten iterations. Our models are small enough to permit us to run those that many times in a reasonable amount of time. For reproducibility of our results, we also share the saved models of each notebook. All saved models can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/tree/master/notebooks/saved_models&#34;&gt;this link&lt;/a&gt;. A notebook describing the steps to use the saved models can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Using_saved_models_tensorflow.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Deep_learning_based_fault_diagnosis_using_CNN_on_raw_time_domain_data.ipynb&#34;&gt;Fault diagnosis using convolutional neural network (CNN) on raw time domain data&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.7%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Deep_learning_based_fault_diagnosis_using_CNN_on_continuous_wavelet_transform_of_time_data.ipynb&#34;&gt;CNN based fault diagnosis using continuous wavelet transform (CWT) of time domain data&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.1%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(This list will be updated gradually.)&lt;/p&gt;

&lt;h2 id=&#34;some-other-related-stuff&#34;&gt;Some other related stuff&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;Data-Driven Remaining Useful Life (RUL) Prediction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/&#34;&gt;Fault diagnosis of machines (A non-technical introduction)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/categories/blog/&#34;&gt;Blog articles by yours truly&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/matlab_intro.pdf&#34;&gt;A quick introduction to MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/transient_vibration_and_SRS_plots.pdf&#34;&gt;Transient vibration and shock response spectrum plots in MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;Simple examples on finding instantaneous frequency using Hilbert transform&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;MATLAB Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;why-have-i-used-only-jupyter-notebooks&#34;&gt;Why have I used only Jupyter notebooks?&lt;/h3&gt;

&lt;p&gt;These notebooks are for educational purposes only. Our experiments are relatively small scale and can be run in a reasonable amount of time in a notebook. I personally love the interactive nature of jupyter notebooks. We can see what we are doing. So the answer to the above question is: personal choice. I also don&amp;rsquo;t intend to deploy these, at least for the time being, in a production environment. Readers who wish to build deployment ready systems should bear in mind that they have to do many other things than just run an algorithm in a jupyter notebook.&lt;/p&gt;

&lt;p&gt;For attribution, cite this project as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{sahoo2016datadriven,
  author = {Sahoo, Biswajit},
  title = {Data-Driven Machinery Fault Diagnosis},
  url = {https://biswajitsahoo1111.github.io/cbm_codes_open/},
  year = {2016}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readers should cite original datasets separately.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
