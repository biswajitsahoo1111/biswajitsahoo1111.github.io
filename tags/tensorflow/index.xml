<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tensorflow | Biswajit Sahoo</title>
    <link>/tags/tensorflow/</link>
      <atom:link href="/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <description>Tensorflow</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Biswajit Sahoo</copyright><lastBuildDate>Sun, 17 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon.png</url>
      <title>Tensorflow</title>
      <link>/tags/tensorflow/</link>
    </image>
    
    <item>
      <title>Efficiently reading multiple files in Tensorflow 2</title>
      <link>/post/efficiently-reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/efficiently-reading-multiple-files-in-tensorflow-2/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Whether this method is efficient or not is contestable. Efficiency of a pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they run this method on their own system. The system on which we ran this notebook has 44 CPU cores. &lt;code&gt;Tensorflow&lt;/code&gt; version is 2.2.0 and it is &lt;code&gt;XLA&lt;/code&gt; enabled. We did not use any GPU. We achieved 20% improvement over naive method. For one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope that this method can be applied for other applications as well. Please note that for some weird reason, the speedup technique doesn’t work in &lt;code&gt;Google Colab&lt;/code&gt;. But it works in GPU enabled personal systems, that I have checked.&lt;/p&gt;
&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/0naikdujqvzosh4/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;This post is a sequel to &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;an older post&lt;/a&gt;. In the previous post, we discussed ways in which we can read multiple files in &lt;code&gt;Tensorflow 2&lt;/code&gt;. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are &lt;code&gt;spectrogram&lt;/code&gt; and normalizing (converting each value to a standard normal value). We have chosen these transformations just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this &lt;a href=&#34;https://www.tensorflow.org/guide/data_performance&#34;&gt;tensorflow guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on &lt;a href=&#34;#speedup&#34;&gt;section 4&lt;/a&gt; which is the main topic of this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#create_files&#34;&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generator&#34;&gt;Write a generator that reads data from the folder in chunks and transforms it.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Build data pipeline and train a CNN model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#speedup&#34;&gt;How to make the code run faster?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictions&#34;&gt;How to make predictions?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id = &#34;create_files&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.
* Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.
* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = np.sort(glob.glob(&amp;quot;./random_data/*&amp;quot;))
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





array([&amp;#39;./random_data/Fault_1_001.csv&amp;#39;, &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_003.csv&amp;#39;, &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_005.csv&amp;#39;, &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_007.csv&amp;#39;, &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_009.csv&amp;#39;, &amp;#39;./random_data/Fault_1_010.csv&amp;#39;],
      dtype=&amp;#39;&amp;lt;U29&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;generator&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;These are the few things that we want our generator to have.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It should run indefinitely, i.e., it is an infinite loop.&lt;/li&gt;
&lt;li&gt;Inside generator loop, read individual files using &lt;code&gt;pandas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Do transformations on data if required.&lt;/li&gt;
&lt;li&gt;Yield the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels. The generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
import pandas as pd
import re&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:    # This loop makes the generator an infinite loop
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                #########################################################################################################
                # Apply transformations. Comment this portion if you don&amp;#39;t have to do any.
                # Try to use Tensorflow transformations as much as possible. First compute a spectrogram.
                temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
                # After STFT transformation with given parameters, shape = (31,33)
                temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
                ##########################################################################################################
                # temp = tf.reshape(temp, (32,32,1)) # Uncomment this line if you have not done any transformation.
                data.append(temp)
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
            data = np.asarray(data).reshape(-1,31,33,1) 
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The generator works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;model&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-a-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training a CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;))[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;))[:10] # Showing first 10 files of Falut_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle files
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,31,33,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,31,33,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,31,33,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (31,33,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 29, 31, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 12, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  11
validation_steps =  2
steps =  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 64s 6s/step - loss: 1.6222 - accuracy: 0.1486 - val_loss: 1.6067 - val_accuracy: 0.1800
Epoch 2/5
11/11 [==============================] - 65s 6s/step - loss: 1.6088 - accuracy: 0.2200 - val_loss: 1.6078 - val_accuracy: 0.2000
Epoch 3/5
11/11 [==============================] - 65s 6s/step - loss: 1.6090 - accuracy: 0.2029 - val_loss: 1.6088 - val_accuracy: 0.2000
Epoch 4/5
11/11 [==============================] - 65s 6s/step - loss: 1.6003 - accuracy: 0.2886 - val_loss: 1.6075 - val_accuracy: 0.1800
Epoch 5/5
11/11 [==============================] - 66s 6s/step - loss: 1.5956 - accuracy: 0.3229 - val_loss: 1.6073 - val_accuracy: 0.2200





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fc3b818dc50&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 13s 3s/step - loss: 1.6098 - accuracy: 0.2000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6098381280899048
Test accuracy: 0.20000000298023224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;speedup&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-the-code-run-faster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make the code run faster?&lt;/h2&gt;
&lt;p&gt;If no transformations are used, just using &lt;code&gt;prefetch&lt;/code&gt; might improve performance. In deep learning usually GPUs are used for training. But all the data processing is done in CPU. In the naive approach, we will first process data in CPU, then send the processed data to GPU and after training finishes, we will prepare another batch of data. This approach is not efficient because GPU has to wait for data to get prepared. But using &lt;code&gt;prefetch&lt;/code&gt;, we prepare and keep ready batches of data while training continues. In this way, waiting time of GPU is minimized.&lt;/p&gt;
&lt;p&gt;When data transformations are used, out aim should always be to use parallel processing capabilities of &lt;code&gt;tensorflow&lt;/code&gt;. We can achieve this using &lt;code&gt;map&lt;/code&gt; function. Inside the &lt;code&gt;map&lt;/code&gt; function, all transformations are defined. Then we can &lt;code&gt;prefetch&lt;/code&gt; batches to further improve performance. The whole pipeline is as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. def transformation_function(...):
    # Define all transormations (STFT, Normalization, etc.)
    
2. def generator(...):
    
       # Read data
    
       # Call transformation_function using tf.data.Dataset.map so that it can parallelize operations.
    
       # Finally yield the processed data

3. Create tf.data.Dataset s.

4. Prefecth datasets.

5. Create model and train it.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use one extra library &lt;code&gt;tensorflow_datasets&lt;/code&gt; that will allow us to switch from &lt;code&gt;tf.dataset&lt;/code&gt; to &lt;code&gt;numpy&lt;/code&gt;. If &lt;code&gt;tensorflow_datasets&lt;/code&gt; is not installed in your system, use &lt;code&gt;pip install tensorflow-datasets&lt;/code&gt; to install it and then run following codes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow_datasets as tfds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_transformation_func(data):
  transformed_data = tf.math.abs(tf.signal.stft(data,frame_length = 64, frame_step = 32, fft_length = 64))
  transformed_data = tf.image.per_image_standardization(tf.reshape(transformed_data, shape = (-1,31,33,1))) # Normalization
  return transformed_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator_new(file_list, batch_size = 4):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size]
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                data.append(tf.reshape(temp.values, shape = (1,1024)))
                pattern = tf.constant(eval(&amp;quot;file[22:29]&amp;quot;))
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
                    
            data = np.asarray(data)
            labels = np.asarray(labels)
            first_dim = data.shape[0]
            # Create tensorflow dataset so that we can use `map` function that can do parallel computation.
            data_ds = tf.data.Dataset.from_tensor_slices(data)
            data_ds = data_ds.batch(batch_size = first_dim).map(data_transformation_func,
                                                                num_parallel_calls = tf.data.experimental.AUTOTUNE)
            # Convert the dataset to a generator and subsequently to numpy array
            data_ds = tfds.as_numpy(data_ds)   # This is where tensorflow-datasets library is used.
            data = np.asarray([data for data in data_ds]).reshape(first_dim,31,33,1)
            
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_045.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_032.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_025.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_013.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_087.csv&amp;#39;,
 &amp;#39;./random_data/Fault_5/Fault_5_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_4/Fault_4_019.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_034.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_044.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[0][22:29]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;Fault_3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 20
dataset_check = tf.data.Dataset.from_generator(tf_data_generator_new,args= [train_file_names, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset_check.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(20, 31, 33, 1)
tf.Tensor([2. 0. 0. 1. 2. 0. 4. 3. 2. 1. 1. 0. 3. 3. 2. 3. 1. 4. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([3. 1. 1. 3. 4. 4. 2. 3. 4. 3. 3. 0. 1. 2. 0. 3. 2. 2. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 3. 0. 2. 2. 4. 3. 0. 4. 1. 0. 0. 2. 0. 0. 1. 0. 3. 2. 1.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([4. 2. 2. 2. 0. 3. 4. 2. 0. 1. 2. 2. 3. 4. 0. 4. 2. 0. 4. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([1. 0. 4. 4. 0. 1. 0. 4. 0. 2. 1. 4. 3. 2. 1. 4. 4. 2. 4. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 2. 0. 1. 3. 2. 2. 2. 1. 3. 3. 4. 0. 1. 4. 1. 3. 2. 1. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 1. 2. 2. 4. 4. 1. 0. 2. 2. 1. 2. 3. 0. 0. 2. 2. 0. 3. 3.], shape=(20,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [train_file_names, batch_size], 
                                                  output_shapes = ((None,31,33,1),(None,)),
                                                  output_types = (tf.float32, tf.float32))

validation_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [validation_file_names, batch_size],
                                                       output_shapes = ((None,31,33,1),(None,)),
                                                       output_types = (tf.float32, tf.float32))

test_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [test_file_names, batch_size],
                                                 output_shapes = ((None,31,33,1),(None,)),
                                                 output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prefetch datasets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_dataset_new = train_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)
validation_dataset_new = validation_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset_new, validation_data = validation_dataset_new, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 45s 4s/step - loss: 1.5939 - accuracy: 0.2714 - val_loss: 1.6075 - val_accuracy: 0.2000
Epoch 2/5
11/11 [==============================] - 46s 4s/step - loss: 1.5890 - accuracy: 0.2886 - val_loss: 1.6082 - val_accuracy: 0.2600
Epoch 3/5
11/11 [==============================] - 44s 4s/step - loss: 1.5771 - accuracy: 0.3257 - val_loss: 1.6066 - val_accuracy: 0.2000
Epoch 4/5
11/11 [==============================] - 44s 4s/step - loss: 1.5710 - accuracy: 0.4400 - val_loss: 1.6057 - val_accuracy: 0.2200
Epoch 5/5
11/11 [==============================] - 45s 4s/step - loss: 1.5564 - accuracy: 0.3771 - val_loss: 1.6074 - val_accuracy: 0.1600





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fc398126090&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss_new, test_acc_new = model.evaluate(test_dataset_new, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 6s 2s/step - loss: 1.6097 - accuracy: 0.1700&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;predictions&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;In the generator used for prediction, we can also use &lt;code&gt;map&lt;/code&gt; function to parallelize data preprocessing. But in practice, inference is much faster. So we can make fast predictions using naive method also. We show the naive implementation below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)
            temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
            # After STFT transformation with given parameters, shape = (31,33)
            temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
            data.append(temp) 
        data = np.asarray(data).reshape(-1,31,33,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(5, 31, 33, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,31,33,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)





array([[0.13783312, 0.06810743, 0.18828638, 0.4399181 , 0.16585506],
       [0.2011155 , 0.0909321 , 0.12722781, 0.34147328, 0.23925126],
       [0.184051  , 0.11195082, 0.15630874, 0.41264012, 0.13504937],
       [0.17021744, 0.15275575, 0.17176864, 0.36582083, 0.13943738],
       [0.22107455, 0.13893652, 0.16182247, 0.22719847, 0.25096804],
       [0.16544239, 0.09297101, 0.19448881, 0.37793893, 0.16915883],
       [0.20981115, 0.09095117, 0.1454936 , 0.37553373, 0.17821036],
       [0.18948458, 0.08287238, 0.16043249, 0.31469837, 0.25251222],
       [0.14806318, 0.08988151, 0.18063019, 0.43348154, 0.14794365],
       [0.19300967, 0.17423573, 0.1853214 , 0.29504803, 0.15238515],
       [0.14796554, 0.10064519, 0.17332935, 0.46094754, 0.11711246],
       [0.1620164 , 0.10878453, 0.19735815, 0.28250632, 0.2493346 ],
       [0.17244144, 0.13593125, 0.18931074, 0.3498449 , 0.1524716 ],
       [0.16827711, 0.08276799, 0.16664039, 0.38747287, 0.19484173],
       [0.16345006, 0.1138956 , 0.17773166, 0.39695117, 0.14797151],
       [0.17923051, 0.12203053, 0.20120224, 0.23441198, 0.26312473],
       [0.1487248 , 0.09016878, 0.17162901, 0.43704256, 0.15243487],
       [0.16879848, 0.05954535, 0.14414911, 0.45952848, 0.16797861],
       [0.14453672, 0.11703113, 0.19364771, 0.4488474 , 0.09593706],
       [0.20345339, 0.1580022 , 0.17898531, 0.22838299, 0.23117617],
       [0.16221416, 0.05681619, 0.14693654, 0.39674726, 0.23728591],
       [0.200225  , 0.16417584, 0.18793206, 0.26401222, 0.18365487],
       [0.21399722, 0.13131607, 0.17154819, 0.21897295, 0.26416558],
       [0.1392046 , 0.05873166, 0.1671688 , 0.45915708, 0.17573787],
       [0.16581263, 0.08813614, 0.18449506, 0.3109786 , 0.25057748],
       [0.15438257, 0.11544537, 0.19926623, 0.37426034, 0.15664549],
       [0.15011945, 0.08316098, 0.1305757 , 0.5280127 , 0.10813113],
       [0.18201515, 0.12078299, 0.17261833, 0.29602036, 0.22856328],
       [0.17936407, 0.09680162, 0.17545709, 0.2755434 , 0.27283388],
       [0.19463587, 0.11394399, 0.17677711, 0.3607436 , 0.15389942],
       [0.1834404 , 0.07998151, 0.16563387, 0.38610289, 0.18484135],
       [0.20745523, 0.14513774, 0.18552025, 0.2872524 , 0.1746344 ],
       [0.18435965, 0.15455365, 0.19811183, 0.28113118, 0.18184367],
       [0.19053918, 0.13114992, 0.18859585, 0.28579548, 0.20391957],
       [0.1874934 , 0.13049673, 0.15486516, 0.4116317 , 0.11551296],
       [0.16247028, 0.09362978, 0.15978761, 0.41194388, 0.17216852],
       [0.16956635, 0.06130224, 0.13529454, 0.41197857, 0.2218583 ],
       [0.17781247, 0.14515027, 0.17571223, 0.34297863, 0.15834646],
       [0.18537633, 0.14020114, 0.17088792, 0.30255666, 0.20097792],
       [0.21270326, 0.13349937, 0.1523133 , 0.263564  , 0.23792005],
       [0.19857787, 0.07489564, 0.1436915 , 0.29431146, 0.2885235 ],
       [0.18926036, 0.11828965, 0.17655246, 0.24165332, 0.2742442 ],
       [0.18234053, 0.082383  , 0.16731356, 0.30818883, 0.25977406],
       [0.17025198, 0.08521349, 0.16441138, 0.41964525, 0.16047782],
       [0.17940679, 0.09788707, 0.15743247, 0.35294068, 0.21233296],
       [0.11456501, 0.05288012, 0.16385278, 0.5418783 , 0.12682377],
       [0.15863904, 0.06855461, 0.16147587, 0.40622538, 0.2051051 ],
       [0.19545631, 0.08327787, 0.13592716, 0.38202846, 0.20331012],
       [0.17398484, 0.14876288, 0.18257992, 0.33674046, 0.15793191],
       [0.21319063, 0.08506136, 0.15001011, 0.37536374, 0.17637412],
       [0.20356631, 0.21442604, 0.20090103, 0.22577564, 0.15533103],
       [0.18141419, 0.11649938, 0.18554828, 0.23423648, 0.2823017 ],
       [0.15753253, 0.10006633, 0.18498763, 0.36755162, 0.18986186],
       [0.18776654, 0.11064088, 0.20178466, 0.2612361 , 0.23857178],
       [0.20099026, 0.14279291, 0.15887792, 0.2843657 , 0.21297309]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3,
       4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,
       3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a final comment, read the &lt;strong&gt;note&lt;/strong&gt; at the beginning of this post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Doing Linear Algebra using Tensorflow 2</title>
      <link>/post/doing-linear-algebra-using-tensorflow-2/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/doing-linear-algebra-using-tensorflow-2/</guid>
      <description>


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Doing_Linear_Algebra_using_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Doing_Linear_Algebra_using_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/vtp81fo71uo9ctn/Doing_Linear_Algebra_using_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will explore the ways of doing linear algebra &lt;strong&gt;only&lt;/strong&gt; using &lt;code&gt;tensorflow&lt;/code&gt;. We will only import &lt;code&gt;tensorflow&lt;/code&gt; and nothing else. As we will see, we can do all the common linear algebra operations without using any other library. This post is very long as it covers almost all the functions that are there in the linear algebra library &lt;code&gt;tf.linalg&lt;/code&gt;. But this is not a copy of &lt;code&gt;tensorflow&lt;/code&gt; documentation. Rather, the &lt;code&gt;tensorflow&lt;/code&gt; documentation is a super set of what has been discussed here. This post also assumes that readers have a working knowledge of linear algebra. Most of the times, we will give examples to illustrate a function without going into the underlying theory. Interested readers should use the contents to browse relevant sections of their interest.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basics&#34;&gt;Basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating_tensors&#34;&gt;Creating tensors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating_a_sequence_of_numbers&#34;&gt;Creating a sequence of numbers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slicing&#34;&gt;Slicing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modifying_elements_of_a_matrix&#34;&gt;Modifying elements of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating_a_complex_matrix&#34;&gt;Creating a complex matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_matrix&#34;&gt;Transpose of a matrix&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_real_matrix&#34;&gt;Transpose of a real matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transpose_of_a_complex_matrix&#34;&gt;Transpose of a complex matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_common_matrices&#34;&gt;Some common matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#identity_matrix&#34;&gt;Identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagonal_matrix&#34;&gt;Diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tri-diagonal_matrix&#34;&gt;Tri-diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_of_all_zeros_and_ones&#34;&gt;Matrix of all zeros and ones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_matrices&#34;&gt;Random matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#random_uniform_matrix&#34;&gt;Random uniform matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_normal_matrix&#34;&gt;Random normal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#truncated_random_normal_matrix&#34;&gt;Truncated random normal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_poisson_matrix&#34;&gt;Random Poisson matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random_gamma_matrix&#34;&gt;Random gamma matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_special_matrices&#34;&gt;Some special matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sparse_matrices&#34;&gt;Sparse matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_multiplication&#34;&gt;Matrix multiplication&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_column_vectors&#34;&gt;Multiplying two column vectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inner_product&#34;&gt;Inner product&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outer_product&#34;&gt;Outer product&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_a_matrix_with_a_vector&#34;&gt;Multiplying a matrix with a vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_matrices&#34;&gt;Multiplying two matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplying_two_tri-diagonal_matrices&#34;&gt;Multiplying two tri-diagonal matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_common_operations_on_matrices&#34;&gt;Some common operations on matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trace&#34;&gt;Trace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#determinant&#34;&gt;Determinant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rank&#34;&gt;Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_inverse&#34;&gt;Matrix inverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extract_diagonals_of_a_matrix&#34;&gt;Extract diagonals of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extract_band_part_of_a_matrix&#34;&gt;Extract band part of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_factorizations&#34;&gt;Matrix factorizations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lu&#34;&gt;LU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cholesky&#34;&gt;Cholesky&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qr&#34;&gt;QR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#svd&#34;&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigenvalues_and_eigenvectors&#34;&gt;Eigenvalues and eigenvectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eigen-analysis_of_hermitian_matrices&#34;&gt;Eigen-analysis of Hermitian matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigen-analysis_of_other_matrices&#34;&gt;Eigen-analysis of other matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_linear_systems&#34;&gt;Solving linear systems&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using_lu_decomposition&#34;&gt;Using LU decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using_cholesky_decomposition&#34;&gt;Using Cholesky decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_tri-diagonal_systems&#34;&gt;Solving tri-diagonal systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving_least_squares_problems&#34;&gt;Solving least squares problems&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ordinary_least_squares&#34;&gt;Ordinary least squares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularized_least_squares&#34;&gt;Regularized least squares&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some_specialized_operations&#34;&gt;Some specialized operations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#norm&#34;&gt;Norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalizing_a_tensor&#34;&gt;Normalizing a tensor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#global_norm&#34;&gt;Global norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross_product_of_vectors&#34;&gt;Cross product of vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_square_root&#34;&gt;Matrix_square_root&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_exponential&#34;&gt;Matrix exponential&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_logarithm&#34;&gt;Matrix logarithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#log-determinant_of_a_matrix&#34;&gt;Log-determinant of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pseudo_inverse_of_a_matrix&#34;&gt;Pseudo inverse of a matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear_operators&#34;&gt;Linear operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#common_methods_on_linear_operators&#34;&gt;Common methods on linear operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#special_matrices_using_operators&#34;&gt;Special matrices using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#toeplitz_matrix&#34;&gt;Toeplitz matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#circulant_matrix&#34;&gt;Circulant matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block_diagonal_matrix&#34;&gt;Block diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block_lower_triangular_matrix&#34;&gt;Block lower triangular matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#householder_matrix&#34;&gt;Householder matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kronecker_matrix&#34;&gt;Kronecker matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#permutation_matrix&#34;&gt;Permutation matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common_matrices_using_operators&#34;&gt;Common matrices using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#identity_matrix&#34;&gt;Identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled_identity_matrix&#34;&gt;Scaled identity matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagonal_matrix&#34;&gt;Diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tri-diagonal_matrix&#34;&gt;Tri-diagonal matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lower_triangular_matrix&#34;&gt;Lower triangular matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_of_zeros&#34;&gt;Matrix of zeros&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix_operations_using_operators&#34;&gt;Matrix operations using operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#low-rank_update&#34;&gt;Low-rank update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operator_inversion&#34;&gt;Operator inversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operator_composition&#34;&gt;Operator composition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing we have to keep in mind is that while accessing a function, we have to always append the function by &lt;code&gt;tf.linalg&lt;/code&gt;. It is possible to remove the &lt;code&gt;tf&lt;/code&gt; part by importing the &lt;code&gt;linalg&lt;/code&gt; library from &lt;code&gt;tensorflow&lt;/code&gt;. But even then we have to append every function by &lt;code&gt;linalg&lt;/code&gt;. In this post, we will always use &lt;code&gt;tf.linalg&lt;/code&gt; followed by function name. This amounts to little more typing. But we will do this to remind ourselves that we are using &lt;code&gt;linalg&lt;/code&gt; library of &lt;code&gt;tensorflow&lt;/code&gt;. This might seem little awkward to seasoned users of &lt;code&gt;MATLAB&lt;/code&gt; or &lt;code&gt;Julia&lt;/code&gt; where you just need to type the function name to use it without having to write the library name all the time. Except that, linear algebra in &lt;code&gt;tensorflow&lt;/code&gt; seems quite natural.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In this post, we will show some of the ways in which we can handle matrix operations in &lt;code&gt;Tensorflow&lt;/code&gt;. We will mainly use 1D or 2D arrays in our examples. But matrix operations in Tensorflow are not limited to 2D arrays. In fact, the operations can be done on multidimensional arrays. If an array has more than 2 dimensions, the matrix operation is done on the &lt;strong&gt;last two&lt;/strong&gt; dimensions and the same operation is carried across other dimensions. For example, if our array has a shape of (3,5,5), it can be thought of as 3 matrices each of shape (5,5). When we call a matrix function on this array, the matrix function is applied to all 3 matrices of shape (5,5). This is also true for higher dimensional arrays.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;basics&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Tensorflow operates on &lt;code&gt;Tensors&lt;/code&gt;. &lt;code&gt;Tensors&lt;/code&gt; are characterized by their rank. Following table shows different types of tensors and their corresponding rank.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Tensors&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Scalars&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 0 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Vectors (1D array)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 1 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Matrices (2D array)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 2 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3D array&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Rank 3 Tensor&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a id = &#34;creating_tensors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;creating-tensors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating tensors&lt;/h3&gt;
&lt;p&gt;In this section, we will create &lt;code&gt;tensors&lt;/code&gt; of different rank, starting from scalars to multi-dimensional arrays. Though tensors can be both real or complex, we will mainly focus on real tensors.&lt;/p&gt;
&lt;p&gt;A scalar contains a single (real or complex) value.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant(5.0)
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the result is a &lt;code&gt;tf.Tensor&lt;/code&gt;. As scalars are rank 0 tensors, its shape is empty. Data type of the tensor is &lt;code&gt;float32&lt;/code&gt;. And corresponding numpy array is 5. We can get only the value of the tensor by calling &lt;code&gt;numpy&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, we can define 1D and 2D &lt;code&gt;tensors&lt;/code&gt;. While 1D &lt;code&gt;tensors&lt;/code&gt; are called vectors, 2D &lt;code&gt;tensors&lt;/code&gt; are called matrices.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([1, 3, 7, 9])    # Note the shape in result. Only one shape parameter is used for vectors.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 3, 7, 9], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([[1,2,3,4],
            [5,6,7,8]])     # Note the shape in result. There are two shape parameters (rows, columns).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=
array([[1, 2, 3, 4],
       [5, 6, 7, 8]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to define a 2D array is given below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant([1,2,3,4,5,6,7,8.0], shape = (2,4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
array([[1., 2., 3., 4.],
       [5., 6., 7., 8.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;creating_a_sequence_of_numbers&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-sequence-of-numbers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a sequence of numbers&lt;/h3&gt;
&lt;p&gt;There are two ways to generate sequence of numbers in &lt;code&gt;Tensorflow&lt;/code&gt;. Functions &lt;code&gt;tf.range&lt;/code&gt; and &lt;code&gt;tf.linspace&lt;/code&gt; can be used for that purpose. Sequences generated by these functions are equally spaced.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence = tf.range(start = 1,limit = 10, delta = 1)
sequence.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the last element (limit) is not included in the array. This is consistent with &lt;code&gt;Python&lt;/code&gt; behavior but in departure with &lt;code&gt;MATLAB&lt;/code&gt; and &lt;code&gt;Julia&lt;/code&gt; convention. It is also possible to set &lt;code&gt;delta&lt;/code&gt; to a fraction.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.range(start = 1, limit = 10, delta = 1.5).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1. , 2.5, 4. , 5.5, 7. , 8.5], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linspace(start = 1.0, stop = 10, num = 25)  # Start must be a `float`. See documentation for more details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(25,), dtype=float32, numpy=
array([ 1.   ,  1.375,  1.75 ,  2.125,  2.5  ,  2.875,  3.25 ,  3.625,
        4.   ,  4.375,  4.75 ,  5.125,  5.5  ,  5.875,  6.25 ,  6.625,
        7.   ,  7.375,  7.75 ,  8.125,  8.5  ,  8.875,  9.25 ,  9.625,
       10.   ], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though in this post we will mainly focus on matrices, it is easy to create higher dimensional arrays in &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.constant(tf.range(1,13), shape = (2,3,2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=
array([[[ 1,  2],
        [ 3,  4],
        [ 5,  6]],

       [[ 7,  8],
        [ 9, 10],
        [11, 12]]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;slicing&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;slicing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slicing&lt;/h3&gt;
&lt;p&gt;Slicing is similar to that of &lt;code&gt;numpy&lt;/code&gt; slicing. For vectors (rank 1 tensor with only one shape parameter), only one argument is passed that corresponds to the location of starting index and end index of sliced array. For matrices (rank 2 tensor with two shape parameters), two input arguments need to be passed. First one for rows and second one for columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector = tf.range(start = 1, limit = 10)
vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector[3:7].numpy() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([4, 5, 6, 7], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indexing in &lt;code&gt;tensorflow&lt;/code&gt; starts from zero. In the above example, start index is 3. So that corresponds to 4th element of the vector. And end index is not included. This is similar to &lt;code&gt;Python&lt;/code&gt; convention.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix = tf.constant(tf.range(20, dtype = tf.float32), shape = (4,5))
matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix[1:3, 2:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 7.,  8.],
       [12., 13.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix[:3,2:]      # Same behavior as numpy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 2.,  3.,  4.],
       [ 7.,  8.,  9.],
       [12., 13., 14.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;modifying_elements_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modifying-elements-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modifying elements of a matrix&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Tensors&lt;/code&gt; in &lt;code&gt;tensorflow&lt;/code&gt;, once created, can’t be modified. So the following code segment will result in an error.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = tf.constant([1,2,3,4])
&amp;gt;&amp;gt;&amp;gt; a[2] = 5  # Error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a way to modify values of a matrix. Instead of creating a &lt;code&gt;tensor&lt;/code&gt;, we create a &lt;code&gt;Variable&lt;/code&gt;. &lt;code&gt;Variables&lt;/code&gt; work just like &lt;code&gt;tensors&lt;/code&gt; with the added advantage that their values can be modified. So if we want to modify entries of our matrix at a later stage, we have to first create our matrix as a variable. Then we can do assignment using &lt;code&gt;assign&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;variable_mat = tf.Variable(tf.constant(tf.range(12, dtype = tf.float32), shape = (3,4)))
variable_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &amp;#39;Variable:0&amp;#39; shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  6.,  7.],
       [ 8.,  9., 10., 11.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;variable_mat[:2,2:4].assign(-1*tf.ones(shape = (2,2)))
variable_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &amp;#39;Variable:0&amp;#39; shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1., -1., -1.],
       [ 4.,  5., -1., -1.],
       [ 8.,  9., 10., 11.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;creating_a_complex_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-complex-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a complex matrix&lt;/h3&gt;
&lt;p&gt;To create a complex matrix, we have to first create the real part and imaginary part separately. Then both real and imaginary parts can be combined element wise to create a complex matrix. Elements of both real and imaginary part should be floats. This is the hard way of creating complex a complex matrix. We will discuss the simpler way next.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;real_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)
imag_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)
print(&amp;quot;Real part:&amp;quot;)
print(real_part)
print()
print(&amp;quot;Imaginary part:&amp;quot;)
print(imag_part)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Real part:
tf.Tensor(
[[4.0592647 2.7457805]
 [4.5665903 2.4078012]
 [2.9457803 3.0998607]], shape=(3, 2), dtype=float32)

Imaginary part:
tf.Tensor(
[[4.0819793 1.4122672]
 [3.3663173 1.0939579]
 [2.2030935 1.1165142]], shape=(3, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat = tf.dtypes.complex(real = real_part, imag = imag_part)
print(complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[4.0592647+4.0819793j 2.7457805+1.4122672j]
 [4.5665903+3.3663173j 2.4078012+1.0939579j]
 [2.9457803+2.2030935j 3.0998607+1.1165142j]], shape=(3, 2), dtype=complex64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a simpler way to create a complex matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat_2 = tf.constant([1+2j, 2+3j , 3+4j, 4+5j, 5+6j, 6+7j], shape = (2,3))
complex_mat_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3), dtype=complex128, numpy=
array([[1.+2.j, 2.+3.j, 3.+4.j],
       [4.+5.j, 5.+6.j, 6.+7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transpose of a matrix&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_real_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;transpose-of-a-real-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transpose of a real matrix&lt;/h4&gt;
&lt;p&gt;For real matrices &lt;code&gt;transpose&lt;/code&gt; just means changing the rows into columns and vice versa. There are three functions that achieve this.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.transpose&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.adjoint&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf.matrix_transpose&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For real matrices, all three functions give identical results.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.transpose(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.adjoint(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  5., 10., 15.],
       [ 1.,  6., 11., 16.],
       [ 2.,  7., 12., 17.],
       [ 3.,  8., 13., 18.],
       [ 4.,  9., 14., 19.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;transpose_of_a_complex_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-of-a-complex-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transpose of a complex matrix&lt;/h4&gt;
&lt;p&gt;Things are little different when we have a complex matrix. For complex matrices, we can take regular transpose or conjugate transpose if we want. Default is regular transpose. To take conjugate transpose, we have to set &lt;code&gt;conjugate = False&lt;/code&gt; in &lt;code&gt;tf.transpose&lt;/code&gt; and &lt;code&gt;tf.linalg.matrix_transpose&lt;/code&gt; or use &lt;code&gt;tf.linalg.adjoint&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;complex_mat_2.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.+2.j, 2.+3.j, 3.+4.j],
       [4.+5.j, 5.+6.j, 6.+7.j]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;transpose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = False) # Regular transpose
print(transpose_of_complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[1.+2.j 4.+5.j]
 [2.+3.j 5.+6.j]
 [3.+4.j 6.+7.j]], shape=(3, 2), dtype=complex128)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;conjugate_transpose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = True) # Conjugate transpose
print(conjugate_transpose_of_complex_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[1.-2.j 4.-5.j]
 [2.-3.j 5.-6.j]
 [3.-4.j 6.-7.j]], shape=(3, 2), dtype=complex128)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also do conjugate transpose by using function &lt;code&gt;linalg.adjoint&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.adjoint(complex_mat_2).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.-2.j, 4.-5.j],
       [2.-3.j, 5.-6.j],
       [3.-4.j, 6.-7.j]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to take transpose of a matrix is to use the function &lt;code&gt;linalg.matrix_transpose&lt;/code&gt;. In this function, we can set argument &lt;code&gt;conjugate&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt; depending on whether we want regular transpose or conjugate transpose. Default is &lt;code&gt;conjugate = False&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(complex_mat_2)   # Conjugate = False is the default&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=
array([[1.+2.j, 4.+5.j],
       [2.+3.j, 5.+6.j],
       [3.+4.j, 6.+7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matrix_transpose(complex_mat_2, conjugate = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=
array([[1.-2.j, 4.-5.j],
       [2.-3.j, 5.-6.j],
       [3.-4.j, 6.-7.j]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_common_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-common-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some common matrices&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;identity-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eye(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4,5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[1, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 3, 0, 0],
       [0, 0, 0, 4, 0],
       [0, 0, 0, 0, 5]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create diagonal matrix, we can also use &lt;code&gt;tf.linalg.tensor_diag&lt;/code&gt; with main diagonal as input.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tensor_diag(tf.constant([1,2,3,4,5.]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 3., 0., 0.],
       [0., 0., 0., 4., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create a matrix whose only nonzero entries are on its super-diagonals or sub-diagonals.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4], k = 1)   # Values in super-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[0, 1, 0, 0, 0],
       [0, 0, 2, 0, 0],
       [0, 0, 0, 3, 0],
       [0, 0, 0, 0, 4],
       [0, 0, 0, 0, 0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4], k = -1)  # Values in sub-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[0, 0, 0, 0, 0],
       [1, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 3, 0, 0],
       [0, 0, 0, 4, 0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag([1,2,3,4,5], k = 0, padding_value = -1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[ 1, -1, -1, -1, -1],
       [-1,  2, -1, -1, -1],
       [-1, -1,  3, -1, -1],
       [-1, -1, -1,  4, -1],
       [-1, -1, -1, -1,  5]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to create a diagonal matrix is by using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.zeros(shape = (5,5))
diag = tf.constant([1,2,3,4,5.])
tf.linalg.set_diag(input = mat, diagonal = diag, k = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 3., 0., 0.],
       [0., 0., 0., 4., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.set_diag(mat, tf.constant([1,2,3,4.]), k = 1)  # Set super-diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0., 1., 0., 0., 0.],
       [0., 0., 2., 0., 0.],
       [0., 0., 0., 3., 0.],
       [0., 0., 0., 0., 4.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[1,2,3,4,5],
                     [6,7,8,9,0.]])
tf.linalg.set_diag(mat, diags, k = (-1,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [6., 2., 0., 0., 0.],
       [0., 7., 3., 0., 0.],
       [0., 0., 8., 4., 0.],
       [0., 0., 0., 9., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next section, we will see a way to create tri-diagonal matrix using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;tri-diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tri-diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tri-diagonal matrix&lt;/h4&gt;
&lt;p&gt;Let’s create &lt;a href=&#34;http://www-math.mit.edu/~gs/&#34;&gt;Gilbert Strang’s&lt;/a&gt; favorite matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag(tf.repeat(2,repeats = 5)) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = -1) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[ 2, -1,  0,  0,  0],
       [-1,  2, -1,  0,  0],
       [ 0, -1,  2, -1,  0],
       [ 0,  0, -1,  2, -1],
       [ 0,  0,  0, -1,  2]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;tf.linalg.set_diag&lt;/code&gt;. While setting more that one diagonals using &lt;code&gt;set_diag&lt;/code&gt;, if &lt;code&gt;k = (-2,3)&lt;/code&gt;, we have to have 6 diagonals (2 sub-diagonals, 1 main diagonal, and 3 super-diagonals). First three rows of the input diagonals will correspond to super-diagonals and have to be appended at the right by zeros. Fourth row corresponds to main diagonal. Last two rows correspond to sub-diagonals and have to be appended at the left by zeros. This type of appending works when &lt;code&gt;align=&#34;LEFT_RIGHT&#34;&lt;/code&gt;. We chose this alignment strategy as it is consistent with tri-diagonal matrix convention, to be discussed in later sections.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1,-1,-1,-1, 0],
                     [ 2, 2, 2, 2, 2],
                     [ 0,-1,-1,-1,-1]], dtype = tf.float32)
mat = tf.zeros(shape = (5,5))
tf.linalg.set_diag(mat,diags, k = (-1,1), align = &amp;quot;LEFT_RIGHT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2., -1.,  0.,  0.,  0.],
       [-1.,  2., -1.,  0.,  0.],
       [ 0., -1.,  2., -1.,  0.],
       [ 0.,  0., -1.,  2., -1.],
       [ 0.,  0.,  0., -1.,  2.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is yet another simpler way to create a tri-diagonal matrix using a linear operator. We will see that technique in a later section.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_of_all_zeros_and_ones&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-of-all-zeros-and-ones&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix of all zeros and ones&lt;/h4&gt;
&lt;p&gt;Matrices of all 1s or all 0s are not in &lt;code&gt;linalg&lt;/code&gt; library. But those are available in core &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.zeros(shape = (3,5), dtype = tf.float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.ones(shape = (5,4), dtype = tf.int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Random matrices&lt;/h4&gt;
&lt;p&gt;Random matrices are also not part of &lt;code&gt;linalg&lt;/code&gt; library. Rather, they are part of &lt;code&gt;tf.random&lt;/code&gt; library. Using &lt;code&gt;Tensorflow&lt;/code&gt; we can create matrices whose entries come from normal, uniform, poisson, and gamma distributions.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;random_uniform_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;random-uniform-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random uniform matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.uniform(shape = (5,5), minval = 0, maxval = 5, seed= 32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[4.8578553 , 0.26324332, 1.7549878 , 4.434555  , 2.3975224 ],
       [3.219039  , 0.4039365 , 0.92039883, 2.9136662 , 4.9377174 ],
       [4.617196  , 3.6782126 , 4.0351195 , 4.8321657 , 4.206293  ],
       [2.3059547 , 4.922245  , 4.186061  , 2.1761923 , 0.88124394],
       [2.7422066 , 1.5948689 , 2.6099925 , 4.4901986 , 2.4033623 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Random matrix of integers
uniform_int = tf.random.uniform(shape = (5,5), minval= 10, maxval = 20, dtype = tf.int32, seed = 1234)
uniform_int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[19, 15, 13, 14, 10],
       [16, 18, 10, 15, 10],
       [12, 13, 19, 12, 16],
       [18, 11, 10, 18, 12],
       [17, 18, 14, 19, 10]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For further processing we usually require matrix entries to be floating point numbers. This can be achieved by using &lt;code&gt;tf.cast&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.dtypes.cast(uniform_int, dtype = tf.float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[19., 15., 13., 14., 10.],
       [16., 18., 10., 15., 10.],
       [12., 13., 19., 12., 16.],
       [18., 11., 10., 18., 12.],
       [17., 18., 14., 19., 10.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_normal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-normal-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random normal matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.normal(shape = (5,5), mean = 1, stddev= 3, seed = 253)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.5266892 , -5.114835  ,  4.4653835 , -1.013567  , -1.1874261 ],
       [ 5.503375  , -1.4568713 , -1.3270268 ,  0.2747649 ,  3.1374507 ],
       [ 4.211556  ,  4.618066  ,  1.2217634 ,  0.04707384,  1.4131291 ],
       [-2.7024255 ,  0.81293994, -3.11763   , -3.043394  ,  5.5663233 ],
       [ 1.4549919 ,  3.7368293 ,  1.2184538 ,  2.0713992 ,  0.19450545]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;truncated_random_normal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;truncated-random-normal-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Truncated random normal matrix&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;truncated_normal&lt;/code&gt; function gives values within two standard deviations of mean on both sides of normal curve.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.truncated_normal(shape = (5,5), mean = 0, stddev= 2, seed = 82) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2.3130474 ,  1.917585  ,  1.1134342 , -3.6221776 , -2.242488  ],
       [ 2.8108876 , -1.8440692 ,  1.7630143 , -0.4591654 , -0.20763761],
       [-0.4769438 ,  2.3582413 , -0.45690525, -0.4208855 , -1.8990422 ],
       [-2.2638845 ,  2.9536312 ,  0.9591611 ,  2.670887  ,  1.4793464 ],
       [-0.60492915,  3.6320126 ,  3.9752324 , -0.4684417 , -3.2791114 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are ways to create deterministic random numbers using &lt;code&gt;stateless_normal&lt;/code&gt;, &lt;code&gt;stateless_uniform&lt;/code&gt;, etc. To know more about random number generation in &lt;code&gt;Tensorflow&lt;/code&gt;, go to &lt;a href=&#34;https://www.tensorflow.org/guide/random_numbers&#34;&gt;this link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;random_poisson_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-poisson-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random Poisson matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.poisson(shape = (5,5), lam = 2, seed = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 1., 2., 3.],
       [0., 1., 2., 3., 4.],
       [2., 0., 2., 2., 2.],
       [2., 0., 2., 2., 3.],
       [1., 4., 2., 5., 4.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;random_gamma_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-gamma-matrix&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Random gamma matrix&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.random.gamma(shape = (5,5), alpha = 0.7, beta= 0.3, seed = 232)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0.78733766, 2.5200539 , 0.9812998 , 5.141082  , 1.9184761 ],
       [1.1069427 , 0.32923967, 0.13172682, 5.066955  , 2.8487072 ],
       [0.39204285, 0.53647757, 5.3083944 , 1.618826  , 0.41352856],
       [1.0327125 , 0.27330002, 0.34577194, 0.22123706, 0.77021873],
       [0.38616025, 9.153643  , 1.4737413 , 6.029133  , 0.05517024]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_special_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-special-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some special matrices&lt;/h4&gt;
&lt;p&gt;Special matrices like &lt;code&gt;toeplitz&lt;/code&gt;, &lt;code&gt;circulant&lt;/code&gt;, &lt;code&gt;Kronecker&lt;/code&gt;, etc can be created using linear operators. We will discuss this in the &lt;a href=&#34;#special_matrices_using_operators&#34;&gt;linear operator&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;sparse_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sparse-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sparse matrices&lt;/h3&gt;
&lt;p&gt;Sparse matrices are within &lt;code&gt;tf.sparse&lt;/code&gt; library. There are several functions specifically designed for sparse matrices. Full list of function in &lt;code&gt;tf.sparse&lt;/code&gt; library can be found at &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/sparse&#34;&gt;this link&lt;/a&gt;. In this section, we will see how sparse matrices are created. The first argument is set of indices (rows and columns), second argument is the values at those indices. Third argument is the &lt;code&gt;dense_shape&lt;/code&gt; of the sparse matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sparse_mat = tf.sparse.SparseTensor([[0,1],[1,3],[3,2]], [-5, -10, 7], dense_shape= (5,5))
sparse_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fdc6168d110&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the actual matrix, we have to convert the sparse matrix to a dense matrix. This is achieved using &lt;code&gt;to_dense&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.sparse.to_dense(sparse_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[  0,  -5,   0,   0,   0],
       [  0,   0,   0, -10,   0],
       [  0,   0,   0,   0,   0],
       [  0,   0,   7,   0,   0],
       [  0,   0,   0,   0,   0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should be noted that special algorithms exist to deal with sparse matrices. Those algorithms don’t require the sparse matrix to be converted into its dense equivalent. By converting a sparse matrix into a dense one, all its special properties are lost. Therefore, sparse matrices should not be converted into dense ones.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_multiplication&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-multiplication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix multiplication&lt;/h3&gt;
&lt;p&gt;To multiply two vectors, or two matrices, or a matrix with a vector in a linear algebra sense, we have to use &lt;code&gt;linalg.matmul&lt;/code&gt; function. Using &lt;code&gt;*&lt;/code&gt; operator in python does element wise multiplication with broadcasting wherever possible. So to multiply two matrices, we have to call &lt;code&gt;linalg.matmul&lt;/code&gt; function. Inputs to &lt;code&gt;linalg.matmul&lt;/code&gt; function are matrices. Therefore, while multiplying two arrays, we have to first convert them into vectors and then multiply. Also note that &lt;code&gt;linalg.matmul&lt;/code&gt; is same as &lt;code&gt;tf.matmul&lt;/code&gt;. Both are aliases.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_column_vectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;multiplying-two-column-vectors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two column vectors&lt;/h4&gt;
&lt;p&gt;Vectors in &lt;code&gt;tensorflow&lt;/code&gt; have only 1 shape parameter, where as a column vector (a matrix with one column) has two shape parameters. For example, a vector &lt;span class=&#34;math inline&#34;&gt;\([1,2,3]\)&lt;/span&gt; has shape &lt;span class=&#34;math inline&#34;&gt;\((3,)\)&lt;/span&gt;, but the column vector &lt;span class=&#34;math inline&#34;&gt;\([1,2,3]^T\)&lt;/span&gt; has shape &lt;span class=&#34;math inline&#34;&gt;\((3,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;inner_product&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;inner-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Inner product&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vector_1 = tf.constant([1., 2., 3.], shape = (3,1))
vector_2 = tf.constant([2., 3., 4.], shape = (3,1))
result = tf.matmul(a = vector_1, b = vector_2, transpose_a=True) # Inner product
result.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[20.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;outer_product&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outer-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Outer product&lt;/h5&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(a = vector_1, b = vector_2, transpose_b = True) # Outer product&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 2.,  3.,  4.],
       [ 4.,  6.,  8.],
       [ 6.,  9., 12.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_a_matrix_with_a_vector&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-a-matrix-with-a-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying a matrix with a vector&lt;/h4&gt;
&lt;p&gt;There are two ways in which we can achieve this. We can convert the vector into a column vector (matrix with 1 column) and then apply &lt;code&gt;tf.matmul&lt;/code&gt;, or we can use the inbuilt function &lt;code&gt;tf.linalg.matvec&lt;/code&gt; to multiply a matrix with a vector.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat_1 = tf.constant([1,2,3,4,5,6],shape = (2,3), dtype = tf.float32)
mat_1.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(a = mat_1, b = vector_1).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[14.],
       [32.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matvec(mat_1, tf.constant([1,2,3.]))    # Note the shape of input vector and result.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([14., 32.], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-two-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two matrices&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matmul(a = mat, b = mat, transpose_a=True).numpy() # Without `transpose_a` argument, result will be an error.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matmul(a = mat, b = mat, transpose_b=True).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;multiplying_two_tri-diagonal_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplying-two-tri-diagonal-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiplying two tri-diagonal matrices&lt;/h4&gt;
&lt;p&gt;If matrices have some sparse structure, usual matrix multiplication is not an efficient method for those type of matrices. Special algorithms are there that exploit the sparsity of the matrices.&lt;/p&gt;
&lt;p&gt;One such sparse matrix is tri-diagonal matrix. It has nonzero entries only on its super-diagonal, main diagonal, and sub-diagonal. To multiply a tri-diagonal matrix with another matrix, we can use &lt;code&gt;tf.linalg.tridiagonal_matmul&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diagonals = tf.constant([[-1,-1,-1,-1,0],
                         [ 2, 2, 2, 2, 2],
                         [ 0,-1,-1,-1,-1.]])
rhs = tf.constant([[1,2,3],
                   [2,1,3],
                   [4,5,6],
                   [7,8,9],
                   [2,5,4.]])
tf.linalg.tridiagonal_matmul(diagonals, rhs) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 0.,  3.,  3.],
       [-1., -5., -3.],
       [-1.,  1.,  0.],
       [ 8.,  6.,  8.],
       [-3.,  2., -1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can verify the result by dense matrix multiplication. However, note that this is only for verification. For large matrix multiplications involving tri-diagonal matrix, dense multiplication will be considerably slower.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tridiag_mat = tf.linalg.set_diag(tf.zeros(shape = (5,5)), diagonals, k = (-1,1), align = &amp;quot;LEFT_RIGHT&amp;quot;)
tf.matmul(tridiag_mat, rhs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 0.,  3.,  3.],
       [-1., -5., -3.],
       [-1.,  1.,  0.],
       [ 8.,  6.,  8.],
       [-3.,  2., -1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How to multiply two tri-diagonal matrices?&lt;/p&gt;
&lt;p&gt;In this case, we have to convert the right tri-diagonal matrix into a full matrix and then multiply it with the left one using only the diagonals of left tri-diagonal matrix. For example, we will multiply the previous tri-diagonal matrix with itself.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tridiagonal_matmul(diagonals, rhs = tridiag_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 5., -4.,  1.,  0.,  0.],
       [-4.,  6., -4.,  1.,  0.],
       [ 1., -4.,  6., -4.,  1.],
       [ 0.,  1., -4.,  6., -4.],
       [ 0.,  0.,  1., -4.,  5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tridiag_mat, tridiag_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 5., -4.,  1.,  0.,  0.],
       [-4.,  6., -4.,  1.,  0.],
       [ 1., -4.,  6., -4.,  1.],
       [ 0.,  1., -4.,  6., -4.],
       [ 0.,  0.,  1., -4.,  5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_common_operations_on_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-common-operations-on-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some common operations on matrices&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;trace&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;trace&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Trace&lt;/h4&gt;
&lt;p&gt;Computes the trace of a tensor. For non-square rank 2 tensors, trace of the main diagonal is computed.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[2,4,6],
                   [5,1,9.]])
tf.linalg.trace(mat).numpy()
              &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;determinant&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;determinant&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Determinant&lt;/h4&gt;
&lt;p&gt;Computes the determinant of the matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = -2*tf.linalg.diag([1,2,3.])
tf.linalg.det(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=-48.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;rank&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rank&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Rank&lt;/h4&gt;
&lt;p&gt;Computes the rank of a matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,4,5],
                 [3,2,5],
                 [2,1,3.]])
rank = tf.linalg.matrix_rank(A)
print(&amp;quot;Rank of A = &amp;quot;, rank.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rank of A =  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_inverse&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-inverse&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix inverse&lt;/h4&gt;
&lt;p&gt;Computes the matrix inverse if it exists. It uses &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition to calculate inverse. What happens if inverse doesn’t exist? Here is the answer taken directly from &lt;code&gt;tensorflow&lt;/code&gt; documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… If a matrix is not invertible there is no guarantee what the op does. It may detect the condition and raise an exception or it may simply return a garbage result.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having read the documentation, we will apply the function to an invertible matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[2, 2, 3],
                 [4,5,6],
                 [1,2,4.]])
A_inv = tf.linalg.inv(A)
print(A_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[ 1.5999998e+00 -3.9999992e-01 -6.0000008e-01]
 [-1.9999999e+00  9.9999994e-01  7.9472862e-08]
 [ 5.9999996e-01 -3.9999998e-01  3.9999998e-01]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,A_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 9.9999970e-01,  1.1920929e-07, -1.1920929e-07],
       [-5.9604645e-07,  1.0000002e+00,  0.0000000e+00],
       [-2.3841858e-07,  0.0000000e+00,  1.0000000e+00]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition result is already available from some prior computation, it can be used to compute the inverse using command &lt;code&gt;tf.linalg.lu_matrix_inverse&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lu, p = tf.linalg.lu(A)
A_inv_by_lu = tf.linalg.lu_matrix_inverse(lu,p)
A_inv_by_lu&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.5999998e+00, -3.9999992e-01, -6.0000008e-01],
       [-1.9999999e+00,  9.9999994e-01,  7.9472862e-08],
       [ 5.9999996e-01, -3.9999998e-01,  3.9999998e-01]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;extract_diagonals_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-diagonals-of-a-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Extract diagonals of a matrix&lt;/h4&gt;
&lt;p&gt;Diagonals of a matrix can be extracted using &lt;code&gt;tf.linalg.diag_part&lt;/code&gt; function. Diagonal entries are obtained by setting &lt;code&gt;k=0&lt;/code&gt; which is the default. By setting &lt;code&gt;k&lt;/code&gt; to any other value, either sub-diagonal or super-diagonal can be obtained.If two values are given to &lt;code&gt;k&lt;/code&gt;, the values correspond respectively to the lower limit and upper limit of the diagonal. And the result contains all diagonals within those limits. The result is not a matrix. It is an array of diagonals, appended if required. Sub-diagonals are appended at the right and super diagonals are appended at the left.&lt;/p&gt;
&lt;p&gt;Another function &lt;code&gt;tf.linalg.tensor_diag_part&lt;/code&gt; can be used to extract the main diagonal of the matrix. But it can extract only the main diagonal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.random.uniform(shape = (5,5), minval = 1, maxval = 20, dtype = tf.int32)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=
array([[ 1, 15, 19,  1,  1],
       [ 8,  5, 17, 14, 16],
       [12, 13, 12, 16, 19],
       [ 6,  8,  3,  5, 17],
       [12,  8,  3,  5,  8]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 1,  5, 12,  5,  8], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tensor_diag_part(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 1,  5, 12,  5,  8], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat, k = (-1,0)).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 1,  5, 12,  5,  8],
       [ 8, 13,  3,  5,  0]], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.diag_part(mat, k = [-2,1])  # 2 subdiagonals, main diagonal, and 1 super diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=int32, numpy=
array([[ 0, 15, 17, 16, 17],
       [ 1,  5, 12,  5,  8],
       [ 8, 13,  3,  5,  0],
       [12,  8,  3,  0,  0]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;extract_band_part_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-band-part-of-a-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Extract band part of a matrix&lt;/h4&gt;
&lt;p&gt;A band matrix is one that has nonzero values along its diagonal and a few sub-diagonals and super-diagonals. All other entries are zero. It is a sparse matrix. All of its nonzero entries are concentrated in a band along the diagonal. For example, tri-diagonal matrix is a banded matrix. It has lower bandwidth of 1 and upper bandwidth of 1. It is possible for a matrix to have different upper and lower bandwidths. It is still called a banded matrix.&lt;/p&gt;
&lt;p&gt;Banded matrices are useful because computations are significantly faster using these matrices as compared to dense matrices of same shape. If for some application, we want the band part of a matrix, we can use &lt;code&gt;linalg.band_part&lt;/code&gt; function to extract it. This function takes three arguments (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;num_lower&lt;/code&gt;, &lt;code&gt;num_upper&lt;/code&gt;). First argument is the tensor whose band part we want to extract. Second argument is the number of sub-diagonals to keep. If set to 0, no sub-diagonal is kept. &lt;code&gt;num_lower = -1&lt;/code&gt; keeps all the sub-diagonals. Similarly for &lt;code&gt;num_upper&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;matrix = tf.constant(tf.range(25, dtype=tf.float32), shape=(5,5))
matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.,  9.],
       [10., 11., 12., 13., 14.],
       [15., 16., 17., 18., 19.],
       [20., 21., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.band_part(matrix, num_lower = 2, num_upper = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  1.,  0.,  0.,  0.],
       [ 5.,  6.,  7.,  0.,  0.],
       [10., 11., 12., 13.,  0.],
       [ 0., 16., 17., 18., 19.],
       [ 0.,  0., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.band_part(matrix, num_lower = -1, num_upper = 0)  # Lower traiangular part&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 5.,  6.,  0.,  0.,  0.],
       [10., 11., 12.,  0.,  0.],
       [15., 16., 17., 18.,  0.],
       [20., 21., 22., 23., 24.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_factorizations&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-factorizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix factorizations&lt;/h2&gt;
&lt;p&gt;Some of the most common and widely used matrix factorizations are available in &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;lu&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;lu&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LU&lt;/h3&gt;
&lt;p&gt;Matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is factorized into a unit lower triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and an upper triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, such that &lt;span class=&#34;math display&#34;&gt;\[A=LU\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To reduce round-off errors, partial pivoting is used. In partial pivoting, the following factorization is done.
&lt;span class=&#34;math display&#34;&gt;\[PA = LU\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is called the permutation matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1, 4, 7, 8],
                [24, -5, -13, 9],
                [-7, 21, 8, 19],
                [0, 18, 6, 4]], dtype = tf.float32)
lu, p = tf.linalg.lu(A)  # As per documentation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;LU = &amp;quot;)
print(lu)
print()
print(&amp;quot;P = &amp;quot;)
print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LU = 
tf.Tensor(
[[ 24.          -5.         -13.           9.        ]
 [ -0.29166666  19.541666     4.2083335   21.625     ]
 [  0.04166667   0.21535183   6.635394     2.9680166 ]
 [  0.           0.9211088    0.32005137 -16.868896  ]], shape=(4, 4), dtype=float32)

P = 
tf.Tensor([1 2 0 3], shape=(4,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the above result mean? Well, both &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; matrices have been merged into one. And &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; contains permutation indices. In practice, we don’t have to reconstruct individual matrices &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, because &lt;code&gt;tensorflow&lt;/code&gt; has built-in functions for further analysis that uses the result of &lt;code&gt;tf.linalg.lu&lt;/code&gt; as given above. For the sake of demonstration, we will show how to reconstruct those matrices from above result. To reconstruct &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, we will use a linear operator that is discussed next. After constructing the matrices &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; will be &lt;span class=&#34;math display&#34;&gt;\[A = P^TLU\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))
U = tf.linalg.band_part(lu, 0, -1)
permu_operator = tf.linalg.LinearOperatorPermutation(p)
P = permu_operator.to_dense()
print(&amp;quot;L:&amp;quot;)
print(L)
print()
print(&amp;quot;U:&amp;quot;)
print(U)
print()
print(&amp;quot;P:&amp;quot;)
print(P)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;L:
tf.Tensor(
[[ 1.          0.          0.          0.        ]
 [-0.29166666  1.          0.          0.        ]
 [ 0.04166667  0.21535183  1.          0.        ]
 [ 0.          0.9211088   0.32005137  1.        ]], shape=(4, 4), dtype=float32)

U:
tf.Tensor(
[[ 24.         -5.        -13.          9.       ]
 [  0.         19.541666    4.2083335  21.625    ]
 [  0.          0.          6.635394    2.9680166]
 [  0.          0.          0.        -16.868896 ]], shape=(4, 4), dtype=float32)

P:
tf.Tensor(
[[0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(P, tf.matmul(L,U), transpose_a = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[  1.       ,   4.0000005,   7.       ,   8.       ],
       [ 24.       ,  -5.       , -13.       ,   9.       ],
       [ -7.       ,  21.       ,   8.       ,  19.       ],
       [  0.       ,  18.       ,   6.       ,   3.999998 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily reconstruct our original matrix from &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; factors using the function &lt;code&gt;tf.linalg.lu_reconstruct&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.lu_reconstruct(lu,p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[  1.       ,   4.0000005,   7.       ,   8.       ],
       [ 24.       ,  -5.       , -13.       ,   9.       ],
       [ -7.       ,  21.       ,   8.       ,  19.       ],
       [  0.       ,  18.       ,   6.       ,   3.999998 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;cholesky&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cholesky&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cholesky&lt;/h3&gt;
&lt;p&gt;It is defined for symmetric positive definite matrices. If A is a symmetric positive definite matrix, its Cholesky decomposition can be written as:
&lt;span class=&#34;math display&#34;&gt;\[ A = LL^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is a lower triangular matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,14]], dtype = tf.float32)
L = tf.linalg.cholesky(A)
L&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [1., 2., 0.],
       [1., 2., 3.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(L,L,transpose_b=True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  1.,  1.],
       [ 1.,  5.,  5.],
       [ 1.,  5., 14.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;qr&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;QR&lt;/h3&gt;
&lt;p&gt;Given a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(QR\)&lt;/span&gt; decomposition decomposes the matrix into an orthogonal matrix &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and an upper triangular matrix &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; such that product of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; gives back &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Columns of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; are an orthogonal basis for the column space of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (also known as range of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
Q,R = tf.linalg.qr(A)
print(&amp;quot;Q:&amp;quot;)
print(Q)
print()
print(&amp;quot;R:&amp;quot;)
print(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Q:
tf.Tensor(
[[-0.18257415  0.4079837 ]
 [-0.36514837 -0.44398218]
 [-0.5477225  -0.575977  ]
 [-0.73029673  0.5519779 ]], shape=(4, 2), dtype=float32)

R:
tf.Tensor(
[[-5.477226  -4.7469287]
 [ 0.         2.7778888]], shape=(2, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get full &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; matrices by setting &lt;code&gt;full_matrices = True&lt;/code&gt; in the argument.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Q_full, R_full = tf.linalg.qr(A, full_matrices = True)
print(&amp;quot;Q full:&amp;quot;)
print(Q_full)
print()
print(&amp;quot;R full:&amp;quot;)
print(R_full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Q full:
tf.Tensor(
[[-0.18257415  0.4079837  -0.17102492 -0.8780469 ]
 [-0.36514837 -0.44398218 -0.81774735  0.02891004]
 [-0.5477225  -0.575977    0.54808205 -0.2604928 ]
 [-0.73029673  0.5519779   0.04056833  0.4004264 ]], shape=(4, 4), dtype=float32)

R full:
tf.Tensor(
[[-5.477226  -4.7469287]
 [ 0.         2.7778888]
 [ 0.         0.       ]
 [ 0.         0.       ]], shape=(4, 2), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;svd&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;svd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SVD&lt;/h3&gt;
&lt;p&gt;Singular value decomposition (SVD) of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\in R^{m\times n}\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[A = U\Sigma V^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(U\in R^{m\times m}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\in R^{n\times n}\)&lt;/span&gt; are orthogonal matrices, commonly known as left and right singular vectors respectively. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma \in R^{m\times n}\)&lt;/span&gt; is a diagonal matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6],
                   [7,8,9.]])
s,u,v = tf.linalg.svd(mat)
print(&amp;quot;S:&amp;quot;)
print(s)
print()
print(&amp;quot;U:&amp;quot;)
print(u)
print()
print(&amp;quot;V:&amp;quot;)
print(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S:
tf.Tensor([18.604359   5.459675   2.4636664], shape=(3,), dtype=float32)

U:
tf.Tensor(
[[ 0.2936678   0.40458775  0.7340845 ]
 [ 0.48711583 -0.7956307   0.01849233]
 [ 0.34406567  0.418864   -0.67870086]
 [ 0.7470583   0.16683212  0.01195723]], shape=(4, 3), dtype=float32)

V:
tf.Tensor(
[[ 0.4678568   0.5231253   0.71235543]
 [ 0.6254436  -0.76545155  0.15134181]
 [ 0.62444425  0.3747316  -0.685307  ]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a truncated SVD. To get full SVD decomposition, we have to set &lt;code&gt;full_matrices = True&lt;/code&gt; in the argument.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;s_full,u_full,v_full = tf.linalg.svd(mat, full_matrices = True)
print(&amp;quot;S full:&amp;quot;)
print(s_full)
print()
print(&amp;quot;U full:&amp;quot;)
print(u_full)
print()
print(&amp;quot;V full:&amp;quot;)
print(v_full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S full:
tf.Tensor([18.604359   5.459675   2.4636664], shape=(3,), dtype=float32)

U full:
tf.Tensor(
[[ 0.2936678   0.40458775  0.7340845  -0.45955172]
 [ 0.48711583 -0.7956307   0.01849233 -0.35964906]
 [ 0.34406567  0.418864   -0.67870086 -0.4955166 ]
 [ 0.7470583   0.16683212  0.01195723  0.6433724 ]], shape=(4, 4), dtype=float32)

V full:
tf.Tensor(
[[ 0.4678568   0.5231253   0.71235543]
 [ 0.6254436  -0.76545155  0.15134181]
 [ 0.62444425  0.3747316  -0.685307  ]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If only singular values are of interest, it can be computed without computing singular vectors. In this way, computations can be much faster.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.svd(mat, compute_uv=False).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([18.604359 ,  5.459675 ,  2.4636664], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;eigenvalues_and_eigenvectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalues-and-eigenvectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Eigenvalues and eigenvectors&lt;/h2&gt;
&lt;p&gt;Symmetry is an important consideration while computing eigenvalues and eigenvectors. For symmetric matrices, different set of algorithms are used for eigen analysis that exploit the symmetry of the matrix. Therefore, two functions are available in &lt;code&gt;tensorflow&lt;/code&gt; for eigen analysis. &lt;code&gt;eig&lt;/code&gt; is used to compute eigenvalues and eigenvectors of a dense matrix without any special structure. &lt;code&gt;eigh&lt;/code&gt; is used for eigen analysis of &lt;code&gt;Hermitian&lt;/code&gt; matrices. If only eigenvalues are of interest, &lt;code&gt;eigvals&lt;/code&gt; and &lt;code&gt;eigvalsh&lt;/code&gt; can be used compute just eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;eigen-analysis_of_hermitian_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;eigen-analysis-of-hermitian-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigen-analysis of Hermitian matrices&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[3,1,1],
                 [1,2,1],
                 [1,1,2.]])
values, vectors = tf.linalg.eigh(A)
print(&amp;quot;Eigenvalues:&amp;quot;)
print(values.numpy())
print()
print(&amp;quot;Eigenvectors:&amp;quot;)
print(vectors.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Eigenvalues:
[1.        1.5857866 4.4142137]

Eigenvectors:
[[ 0.         -0.7071068   0.70710665]
 [-0.70710677  0.49999994  0.50000006]
 [ 0.7071068   0.4999999   0.5       ]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is an eigenvector.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.matvec(A, vectors[0,:]).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([-1.7881393e-07, -7.0710701e-01,  7.0710647e-01], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are accurate up to 5 decimal digits.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eigvalsh(A).numpy()    # Just eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1.       , 1.5857866, 4.4142137], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens if you pass a nonsymmetric matrix to &lt;code&gt;eigh&lt;/code&gt; by mistake?&lt;/p&gt;
&lt;p&gt;Well, while using &lt;code&gt;eigh&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; assumes the matrix to be symmetric. &lt;code&gt;Tensorflow&lt;/code&gt; doesn’t check whether the matrix is symmetric or not. It just takes the lower triangular part, assumes that the upper triangular part is same because of symmetry and performs the computations. So be prepared to get a wrong result!&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;eigen-analysis_of_other_matrices&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eigen-analysis-of-other-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eigen-analysis of other matrices&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,-5,3],
                 [2,4,-7],
                 [3,9,-2.]])
values, vectors = tf.linalg.eig(A)
print(&amp;quot;Eigenvalues:&amp;quot;)
print(values.numpy())
print()
print(&amp;quot;Eigenvectors:&amp;quot;)
print(vectors.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Eigenvalues:
[2.7560833 -7.9942424e-08j 0.12195918-7.5705280e+00j
 0.12195931+7.5705280e+00j]

Eigenvectors:
[[ 0.06142625+0.95019215j  0.16093381-0.34744066j  0.13818482+0.35709903j]
 [-0.01236177-0.19122148j  0.3560446 +0.53046155j  0.38952374-0.5063876j ]
 [ 0.01535368+0.23750281j -0.33046415+0.5796737j  -0.29238018-0.59978485j]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only eigenvalues.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.eigvals(A).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2.7560833 -7.9942424e-08j, 0.12195918-7.5705280e+00j,
       0.12195931+7.5705280e+00j], dtype=complex64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens when you pass a symmetric matrix to &lt;code&gt;eig&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;Nothing! We will still get the correct answer. &lt;code&gt;Tensorflow&lt;/code&gt; will use more operations to compute results when it could have been done using less computations.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;solving_linear_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-linear-systems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving linear systems&lt;/h2&gt;
&lt;p&gt;A linear system can be written as &lt;span class=&#34;math display&#34;&gt;\[ Ax = b\]&lt;/span&gt;
In general, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; can be square or rectangular. Right hand side &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a vector in this case. If we have to solve the linear system for multiple right hand side vectors involving same &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, the RHS can be replaced by a matrix whose columns are different RHS vectors.&lt;/p&gt;
&lt;p&gt;Depending on the structure of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (whether triangular, or tri-diagonal, or positive definite), suitable algorithm is chosen to solve the linear system. &lt;code&gt;Tensorflow&lt;/code&gt; has a function &lt;code&gt;tf.linalg.solve&lt;/code&gt; to solve linear systems. But this function doesn’t take into account the special structure of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
tf.linalg.solve(A,b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;using_lu_decomposition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;using-lu-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using LU decomposition&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition factors of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are known, those can be used to solve the linear system.
For example, solve:
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
  1 &amp;amp; 1 &amp;amp; 1\\
  1 &amp;amp; 5 &amp;amp; 5\\
  1 &amp;amp; 5 &amp;amp; 13
 \end{pmatrix} x= 
 \begin{pmatrix}
 3\\
 11\\
 20\\
 \end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
lu, p = tf.linalg.lu(A)   # Factoriztion result of LU
x_sol_lu = tf.linalg.lu_solve(lu,p,b)
x_sol_lu&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,x_sol_lu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[ 3.],
       [11.],
       [20.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have obtained factors &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, we can use &lt;code&gt;tf.linalg.triangular_solve&lt;/code&gt; to solve the linear system by solving following two triangular system. &lt;span class=&#34;math display&#34;&gt;\[Ly = b\]&lt;/span&gt;
and &lt;span class=&#34;math display&#34;&gt;\[Ux = y\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;L = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))
y = tf.linalg.triangular_solve(L,b)    # Solves Ly = b
x = tf.linalg.triangular_solve(lu,y, lower = False)  # Solves Ux = y
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.   ],
       [0.875],
       [1.125]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;using_cholesky_decomposition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-cholesky-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using Cholesky decomposition&lt;/h3&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is positive definite, Cholesky decomposition is an efficient method for solving the linear system. For positive definite &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, Cholesky decomposition requires fewer computations that &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition. This is because it exploits the symmetry of the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Once the Cholesky factor &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is found, we solve the linear system by solving two triangular systems. Solving triangular systems only requires &lt;span class=&#34;math inline&#34;&gt;\(O(n^2)\)&lt;/span&gt; operations.
&lt;span class=&#34;math display&#34;&gt;\[ LL^Tx = b\]&lt;/span&gt;
Two triangular systems are:
&lt;span class=&#34;math display&#34;&gt;\[ Ly = b\]&lt;/span&gt;
This gives us &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Using &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; we solve for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; using the following equation.
&lt;span class=&#34;math display&#34;&gt;\[L^Tx = y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;tensorflow&lt;/code&gt;, we solve the system using &lt;code&gt;tf.linalg.cholesky_solve&lt;/code&gt;. It takes cholesky factor &lt;span class=&#34;math inline&#34;&gt;\((L)\)&lt;/span&gt; and right hand side &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; as input.&lt;/p&gt;
&lt;p&gt;For example, solve:
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
  1 &amp;amp; 1 &amp;amp; 1\\
  1 &amp;amp; 5 &amp;amp; 5\\
  1 &amp;amp; 5 &amp;amp; 13
 \end{pmatrix} x= 
 \begin{pmatrix}
 3\\
 11\\
 20\\
 \end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,1,1],
                 [1,5,5],
                 [1,5,13]], dtype = tf.float32)
b = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)
L = tf.linalg.cholesky(A)
sol_chol = tf.linalg.cholesky_solve(L, b)
sol_chol&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.0000001],
       [0.8750001],
       [1.1249999]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A,sol_chol)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[ 3.      ],
       [11.      ],
       [19.999998]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_tri-diagonal_systems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-tri-diagonal-systems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving tri-diagonal systems&lt;/h3&gt;
&lt;p&gt;If matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is tri-diagonal, &lt;code&gt;tf.linalg.tridiagonal_solve&lt;/code&gt; can be used to solve the linear system efficiently. For example, we will solve the following tri-diagonal system.
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
2 &amp;amp; -1 &amp;amp; 0 &amp;amp;0 &amp;amp; 0\\
-1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; -1 &amp;amp; 2&amp;amp; -1&amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 2 &amp;amp; -1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 2
\end{pmatrix}x=
\begin{pmatrix}
3\\
4\\
-5\\
7\\
9\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1,-1,-1,-1, 0],
                     [ 2, 2, 2, 2, 2],
                     [ 0,-1,-1,-1,-1]], dtype = tf.float32)
b = tf.constant([3,4,-5,7,9.],shape = (5,1))
x = tf.linalg.tridiagonal_solve(diagonals = diags, rhs = b)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 6.5000005],
       [10.000001 ],
       [ 9.500001 ],
       [14.       ],
       [11.5      ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.tridiagonal_matmul(diagonals=diags, rhs = x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 3.      ],
       [ 4.000001],
       [-4.999999],
       [ 7.      ],
       [ 9.      ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;solving_least_squares_problems&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-least-squares-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving least squares problems&lt;/h2&gt;
&lt;p&gt;&lt;a id = &#34;ordinary_least_squares&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;ordinary-least-squares&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordinary least squares&lt;/h3&gt;
&lt;p&gt;Both over determined and under determined least squares problem can be solved using the command &lt;code&gt;tf.linalg.lstsq&lt;/code&gt;. In the underdetermined case, the output is the least norm solution. Least squares problem can be written as
&lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2^2\]&lt;/span&gt;
That is, we try to find an &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; such that the residual error is as small as possible.
For example, we will solve following two problems.
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
1 &amp;amp; 2\\
2 &amp;amp; 0.5\\
3 &amp;amp; 1\\
4 &amp;amp; 5\\
\end{pmatrix}x_{over}=
\begin{pmatrix}
3\\
4\\
5\\
6\\
\end{pmatrix}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
3 &amp;amp; 1 &amp;amp; 2 &amp;amp; 5\\
7 &amp;amp; 9 &amp;amp; 1 &amp;amp; 4
\end{pmatrix}x_{under}=
\begin{pmatrix}
7.2\\
-5.8\\
\end{pmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
A_under = tf.constant([[3,1,2,5],[7,9,1,4.]])
b_over = tf.constant([3,4,5,6.], shape = (4,1))
b_under = tf.constant([7.2,-5.8], shape = (2,1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_over = tf.linalg.lstsq(A_over, b_over)
x_over&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704103  ],
       [-0.04319588]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though it is not advisable, for this simple case, we will directly apply normal equation to get the solution.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.inv(tf.matmul(A_over,A_over, transpose_a = True)), tf.matmul(A_over,b_over, transpose_a = True))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704104  ],
       [-0.04319668]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_under = tf.linalg.lstsq(A_under, b_under)
x_under&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355565 ],
       [ 0.699703  ],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will computer the least norm solution for underdetermined case using the closed form solution. However, it should be remembered that it is not advisable to do so in practice for large systems.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(A_under,tf.matmul(tf.linalg.inv(tf.matmul(A_under, A_under, transpose_b = True)), b_under), transpose_a = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355561 ],
       [ 0.6997029 ],
       [ 1.4518325 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;regularized_least_squares&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularized-least-squares&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularized least squares&lt;/h3&gt;
&lt;p&gt;Only &lt;span class=&#34;math inline&#34;&gt;\(l_2\)&lt;/span&gt; regularization is supported. The following regularized problem is solved.
&lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2^2 + \lambda \|x\|_2^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a hyperparameter. Usually several values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; are tried over a logarithmic scale before choosing the best one.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_over_reg = tf.linalg.lstsq(A_over, b_over, l2_regularizer= 2.0)
x_over_reg.numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.3890449 ],
       [0.21348318]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_under_reg = tf.linalg.lstsq(A_under, b_under, l2_regularizer=2.)
x_under_reg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04763567],
       [-1.214508  ],
       [ 0.62748903],
       [ 1.299031  ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;some_specialized_operations&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-specialized-operations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some specialized operations&lt;/h2&gt;
&lt;p&gt;&lt;a id = &#34;norm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;norm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Norm&lt;/h3&gt;
&lt;p&gt;Norm can be defined for vectors as well as matrices. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; norm of vector is defined as
&lt;span class=&#34;math display&#34;&gt;\[\|x\|_p = (\Sigma_{i=1}^{n}|x_i|^p)^\frac{1}{p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Matrix is norm is defined as
&lt;span class=&#34;math display&#34;&gt;\[\|A\|_p= \max_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tensorflow&lt;/code&gt; supports all the usual vector and matrix norms that are used in practice. Using only &lt;code&gt;tensorflow&lt;/code&gt; we can calculate all norms except &lt;code&gt;infinity&lt;/code&gt; norm. To calculate &lt;code&gt;infinity&lt;/code&gt; norm we have to use &lt;code&gt;ord = np.inf&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3.]), ord = &amp;quot;euclidean&amp;quot;).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3.7416575&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3]), ord = 1).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fractional norms for &lt;strong&gt;vectors&lt;/strong&gt; are also supported.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(tf.constant([1,-2,3.]), ord = 0.75).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.46176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,8, 2,3],
                 [2,7,6,5],
                 [0,3,2,8.]])
mat_norm_2 = tf.linalg.norm(A, ord = 2, axis = [0,1])
print(&amp;quot;2 norm of matrix A = &amp;quot;, mat_norm_2.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_22/lib/python3.7/site-packages/tensorflow/python/ops/linalg_ops.py:721: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to tf.sets.difference().
2 norm of matrix A =  15.294547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2 norm of a matrix is equivalent to the largest singular value of the matrix. We will verify that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;vals,_ ,_ = tf.linalg.svd(A)
tf.math.reduce_max(vals).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15.294547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;normalizing_a_tensor&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normalizing-a-tensor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normalizing a tensor&lt;/h3&gt;
&lt;p&gt;Computes the norm and normalizes the tensor using that norm. By normalize we mean, divide the entries of the tensor by the norm. Here, we will consider a matrix. But the method can be extended to multi-dimensional tensor.&lt;/p&gt;
&lt;p&gt;If computed norm is a single number, all the entries of the matrix will be divided by that number. If norm is calculated along some axis, normalization happens along that axis using individual norms. Here are some examples.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[1., 8., 2., 3.],
       [2., 7., 6., 5.],
       [0., 3., 2., 8.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;normalized_mat, norm = tf.linalg.normalize(A, ord = 2, axis = [0,1])
print(&amp;quot;Normalized matrix: &amp;quot;)
print(normalized_mat.numpy())
print()
print(&amp;quot;Norm = &amp;quot;, norm.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix: 
[[0.06538278 0.5230622  0.13076556 0.19614834]
 [0.13076556 0.45767945 0.39229667 0.3269139 ]
 [0.         0.19614834 0.13076556 0.5230622 ]]

Norm =  [[15.294547]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will get the same normalized matrix by dividing the entries of the matrix by the norm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.06538278, 0.5230622 , 0.13076556, 0.19614834],
       [0.13076556, 0.45767945, 0.39229667, 0.3269139 ],
       [0.        , 0.19614834, 0.13076556, 0.5230622 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;norm_mat_by_col, norms_col = tf.linalg.normalize(A, ord = 2, axis = 0)
print(&amp;quot;Normalized matrix:&amp;quot;)
print(norm_mat_by_col)
print()
print(&amp;quot;Norms of columns of A:&amp;quot;)
print(norms_col)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix:
tf.Tensor(
[[0.4472136  0.724286   0.30151135 0.30304575]
 [0.8944272  0.63375026 0.904534   0.5050763 ]
 [0.         0.27160725 0.30151135 0.80812204]], shape=(3, 4), dtype=float32)

Norms of columns of A:
tf.Tensor([[ 2.236068  11.045361   6.6332498  9.899495 ]], shape=(1, 4), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(A[:,0], ord = 2).numpy()  # 2 Norm of first column of A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.236068&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norms_col&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.4472136 , 0.724286  , 0.30151135, 0.30304575],
       [0.8944272 , 0.63375026, 0.904534  , 0.5050763 ],
       [0.        , 0.27160725, 0.30151135, 0.80812204]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;norm_mat_by_row, norms_row = tf.linalg.normalize(A, ord = 2, axis = 1)
print(&amp;quot;Normalized matrix:&amp;quot;)
print(norm_mat_by_row)
print()
print(&amp;quot;Norms of rows:&amp;quot;)
print(norms_row)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Normalized matrix:
tf.Tensor(
[[0.11322771 0.9058217  0.22645542 0.33968312]
 [0.18731716 0.6556101  0.56195146 0.4682929 ]
 [0.         0.34188172 0.22792116 0.91168463]], shape=(3, 4), dtype=float32)

Norms of rows:
tf.Tensor(
[[ 8.83176 ]
 [10.677078]
 [ 8.774964]], shape=(3, 1), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm(A[0,:], ord = 2).numpy()    # 2 norm of first row of A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.83176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A/norms_row&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.11322771, 0.9058217 , 0.22645542, 0.33968312],
       [0.18731716, 0.6556101 , 0.56195146, 0.4682929 ],
       [0.        , 0.34188172, 0.22792116, 0.91168463]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;global_norm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;global-norm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global norm&lt;/h3&gt;
&lt;p&gt;Given two or more tensors, &lt;code&gt;tf.linalg.global_norm&lt;/code&gt; computes the 2 norm of a vector generated by resizing all the tensors to one dimensional arrays.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant([1, 2, 3.])
b = tf.constant([[4,5],
                 [6,7.]])
tf.linalg.global_norm([a,b]).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.83216&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.norm([1,2,3,4,5,6,7.], ord = 2).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;11.83216&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;cross_product_of_vectors&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-product-of-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross product of vectors&lt;/h3&gt;
&lt;p&gt;It is defined for 3-element vectors. For two vectors &lt;span class=&#34;math inline&#34;&gt;\(a = (a_1, a_2, a_3)^T\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b = (b_1, b_2, b_3)^T\)&lt;/span&gt;, cross product is defined as the determinant of following matrix
&lt;span class=&#34;math display&#34;&gt;\[\begin{pmatrix}
i &amp;amp; j &amp;amp; k\\
a_1 &amp;amp; a_2 &amp;amp; a_3\\
b_1 &amp;amp; b_2 &amp;amp; b_3
\end{pmatrix}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(i, j\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; are unit direction vectors along three perpendicular right handed system.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a = tf.constant([1,2,3])
b = tf.constant([2,3,4])
tf.linalg.cross(a,b).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([-1,  2, -1], dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First element in the output corresponds to the value along &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th direction. Similarly for other outputs.&lt;/p&gt;
&lt;p&gt;It is also possible to calculate cross product of more that one pair of vectors simultaneously.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;c = tf.random.normal(shape = (5,3))
d = tf.random.normal(shape = (5,3))
tf.linalg.cross(c,d).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 1.2073451 ,  0.64969957, -0.02847651],
       [-0.4979881 ,  1.3546165 , -0.591327  ],
       [-0.11196035,  2.9687622 ,  1.8474071 ],
       [-0.13938189, -3.3894374 , -2.557374  ],
       [-0.00778121, -3.5212047 , -3.4708867 ]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First row of output is the cross product of first rows of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Similarly for other rows.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;matrix_square_root&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-square-root&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix square root&lt;/h3&gt;
&lt;p&gt;Square root of a matrix is defined for invertible matrices whose real eigenvalues are positive.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6.]])
mat_root = tf.linalg.sqrtm(mat)
mat_root&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[2.1185937 , 0.35412252, 0.6189322 ],
       [0.30147368, 2.9409115 , 0.7257953 ],
       [0.6540313 , 0.33657336, 2.3132057 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(mat_root, mat_root)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[4.9999986, 2.0000007, 3.0000038],
       [2.0000005, 9.000003 , 4.0000057],
       [3.0000033, 2.000003 , 6.0000052]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_exponential&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-exponential&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix exponential&lt;/h3&gt;
&lt;p&gt;Exponential of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[ e^A = \sum_{n=0}^\infty \frac{A^n}{n!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, the sum is not taken to infinity. Rather, approximations are used to compute matrix exponential. &lt;code&gt;Tensorflow&lt;/code&gt; implementation is based on &lt;a href=&#34;https://epubs.siam.org/doi/10.1137/04061101X&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When the matrix has a full set of independent eigenvectors, the formula can be simplified to
&lt;span class=&#34;math display&#34;&gt;\[e^A = Se^{\Lambda}S^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is the eigenvector matrix and &lt;span class=&#34;math inline&#34;&gt;\(e^\Lambda\)&lt;/span&gt; is a diagonal matrix whose diagonal entries are exponentials of eigenvalues of the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[0,1],
                 [1,0.]])
tf.linalg.expm(A)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1.5430806, 1.1752012],
       [1.1752012, 1.5430806]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_logarithm&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-logarithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix logarithm&lt;/h3&gt;
&lt;p&gt;Computes logarithm of the matrix such that matrix exponential of the result gives back the original matrix. Refer to the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/linalg/logm&#34;&gt;documentation&lt;/a&gt; for further details. It is defined only for complex matrices.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,4],
                   [3,2,6.]], dtype = tf.complex64)
mat_log = tf.linalg.logm(mat)
mat_log&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=
array([[1.4031177 +0.j, 0.25731087+0.j, 0.53848237+0.j],
       [0.16580153+0.j, 2.1160111 +0.j, 0.54512537+0.j],
       [0.5994888 +0.j, 0.2268081 +0.j, 1.5622762 +0.j]], dtype=complex64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.expm(mat_log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=
array([[4.999999 +0.j, 1.9999989+0.j, 2.9999986+0.j],
       [1.9999995+0.j, 9.000006 +0.j, 4.0000005+0.j],
       [2.9999995+0.j, 2.000001 +0.j, 5.9999995+0.j]], dtype=complex64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;log-determinant_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-determinant-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log-determinant of a matrix&lt;/h3&gt;
&lt;p&gt;Computes the natural logarithm of the determinant of a matrix. There are two functions in &lt;code&gt;tensorflow&lt;/code&gt; to calculate this.
* If matrix is symmetric positive definite, use &lt;code&gt;tf.linalg.logdet&lt;/code&gt; (Uses Cholesky decomposition)
* For other matrices, use &lt;code&gt;tf.linalg.slogdet&lt;/code&gt; (Uses &lt;span class=&#34;math inline&#34;&gt;\(LU\)&lt;/span&gt; decomposition)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;slogdet&lt;/code&gt; computes the sign of the determinant as well as the log of the absolute value of the determinant.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[5,2,3],
                   [2,9,2],
                   [3,2,6.]])   # Symmetric positive definite
tf.linalg.logdet(mat).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.1298985&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.math.log(tf.linalg.det(mat)).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.1298985&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat_2 = tf.constant([[5,2,3],
                     [0,-2,2],
                     [0,0,6.]])
sign, log_abs_det = tf.linalg.slogdet(mat_2)
print(&amp;quot;Sign of determinant = &amp;quot;, sign.numpy())
print(&amp;quot;Log of absolute value of determinant = &amp;quot;, log_abs_det.numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sign of determinant =  -1.0
Log of absolute value of determinant =  4.0943446&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.math.log(tf.abs(tf.linalg.det(mat_2))).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4.0943446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;pseudo_inverse_of_a_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pseudo-inverse-of-a-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pseudo inverse of a matrix&lt;/h3&gt;
&lt;p&gt;While matrix inverse is defined only for square matrices, pseudo inverse is defined for matrices of any shape. It is also defined for singular matrices. Pseudo inverse can also be used to solve ordinary least squares problem. For the problem &lt;span class=&#34;math display&#34;&gt;\[arg\min_{x}\|Ax-b\|_2\]&lt;/span&gt;
the approximate least squares solution can be written as
&lt;span class=&#34;math display&#34;&gt;\[x_{ls} = A^{\dagger}b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, &lt;span class=&#34;math inline&#34;&gt;\(A^{\dagger}\)&lt;/span&gt; is the pseudo inverse of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.linalg.pinv(tf.constant([[2,0,0],[0,3,0],[5,0,0.]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[0.06896552, 0.        , 0.17241378],
       [0.        , 0.33333334, 0.        ],
       [0.        , 0.        , 0.        ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])
A_under = tf.constant([[3,1,2,5],[7,9,1,4.]])
b_over = tf.constant([3,4,5,6.], shape = (4,1))
b_under = tf.constant([7.2,-5.8], shape = (2,1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_ls_over = tf.linalg.lstsq(A_over,b_over)
x_ls_over&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.704103  ],
       [-0.04319588]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.pinv(A_over),b_over)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[ 1.7041038 ],
       [-0.04319668]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x_ls_under = tf.linalg.lstsq(A_under, b_under)
x_ls_under&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100358],
       [-1.3355565 ],
       [ 0.699703  ],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(tf.linalg.pinv(A_under), b_under)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[-0.04100376],
       [-1.3355565 ],
       [ 0.69970286],
       [ 1.4518324 ]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;linear_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-operators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear operators&lt;/h2&gt;
&lt;p&gt;Linear operators are a powerful way of defining matrices and associated operators without even doing actual computations. What does this mean? Do we ever get result of our computations using operators? Well, the computations are done only when we ask for the results. Before that the operators just act on each other (like chaining of operators) without doing any computation. We will show this by examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We will mainly use operators to form dense matrices. But the scope of applicability of operators is far bigger than that.&lt;/p&gt;
&lt;p&gt;To define a matrix as an operator, we use &lt;code&gt;tf.linalg.LinearOperatorFullMatrix&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],
                                                           [2,3,5],
                                                           [7,8,9.]]))
operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.ops.linalg.linear_operator_full_matrix.LinearOperatorFullMatrix at 0x7fdc6dadfd90&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get only the memory location. No result is shown. To see the actual matrix we have to call the method &lt;code&gt;to_dense&lt;/code&gt; on this operator. There are many methods that can be called on an operator. For the full list, refer the documentation.&lt;/p&gt;
&lt;p&gt;Before seeing the result, we will apply adjoint operator to our old operator and apply &lt;code&gt;to_dense&lt;/code&gt; to the adjoint operator. If everything works well, we should see the transpose of the matrix as result.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;adj_operator = tf.linalg.LinearOperatorAdjoint(operator)
adj_operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_22/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_adjoint.py:145: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.
Instructions for updating:
Do not call `graph_parents`.





&amp;lt;tensorflow.python.ops.linalg.linear_operator_adjoint.LinearOperatorAdjoint at 0x7fdc61140cd0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again no result. At this point we want to see the result. So we will apply &lt;code&gt;to_dense&lt;/code&gt; method to adj_operator.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;adj_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 7.],
       [2., 3., 8.],
       [3., 5., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compare it with our original matrix, we will also show the original matrix.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the adjoint operator gives the correct answer.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;common_methods_on_linear_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;common-methods-on-linear-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common methods on linear operators&lt;/h3&gt;
&lt;p&gt;There are many methods that can be called on the operator. Depending on the operator, the methods vary. In this section we will discuss some of the methods of &lt;code&gt;LinearOperatorFullMatrix&lt;/code&gt; operator. Some of the methods are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cond (To find condition number)&lt;/li&gt;
&lt;li&gt;determinant&lt;/li&gt;
&lt;li&gt;cholesky (To compute Cholesky factors of operator)&lt;/li&gt;
&lt;li&gt;eigvals (Compute eigenvalues only for self-adjoint (Hermitian) matrices)&lt;/li&gt;
&lt;li&gt;trace&lt;/li&gt;
&lt;li&gt;inverse&lt;/li&gt;
&lt;li&gt;solve (Solve linear system using operator)&lt;/li&gt;
&lt;li&gt;adjoint
and many others.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.cond()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=68.21983&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.trace()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=13.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.determinant()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:Using (possibly slow) default implementation of determinant.  Requires conversion to a dense matrix and O(N^3) operations.





&amp;lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.adjoint().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 7.],
       [2., 3., 8.],
       [3., 5., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.inverse().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.16666669],
       [ 2.8333333 , -2.        ,  0.16666669],
       [-0.83333325,  1.        , -0.16666669]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.matmul(operator.inverse().to_dense())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.0000000e+00,  0.0000000e+00,  0.0000000e+00],
       [-2.3841858e-07,  1.0000000e+00,  0.0000000e+00],
       [-2.3841858e-07,  0.0000000e+00,  1.0000000e+00]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;special_matrices_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;special-matrices-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Special matrices using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;toeplitz_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;toeplitz-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Toeplitz matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;col = tf.constant([1,2,3,4,5.])
row = tf.constant([1,6,7,8,9.])
toeplitz_operator = tf.linalg.LinearOperatorToeplitz(col = col, row = row )
toeplitz_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 6., 7., 8., 9.],
       [2., 1., 6., 7., 8.],
       [3., 2., 1., 6., 7.],
       [4., 3., 2., 1., 6.],
       [5., 4., 3., 2., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;circulant_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;circulant-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Circulant matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;kernel = [1,2,3,4,5]
spectrum = tf.signal.fft(tf.cast(kernel, dtype = tf.complex64))
circ_operator = tf.linalg.LinearOperatorCirculant(spectrum = spectrum, input_output_dtype = tf.float32)
circ_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1.0000002 , 4.9999995 , 3.9999993 , 3.        , 2.0000005 ],
       [2.0000005 , 0.99999934, 4.9999995 , 3.9999993 , 2.9999998 ],
       [3.0000002 , 2.        , 0.9999998 , 4.9999995 , 3.9999998 ],
       [3.9999993 , 2.9999993 , 2.        , 1.        , 4.9999995 ],
       [4.9999995 , 3.9999993 , 3.        , 2.0000005 , 1.0000002 ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;circ_operator.convolution_kernel()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5,), dtype=float32, numpy=
array([1.       , 2.0000002, 3.       , 3.9999993, 4.9999995],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;block_diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;block-diagonal-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Block diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1 = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],
                                                             [4,5,6],
                                                             [7,8,9]], dtype = tf.float32))
operator_2 = tf.linalg.LinearOperatorFullMatrix(-1*tf.constant([[9,8],
                                                           [7,6]], dtype = tf.float32))
blk_diag_operator = tf.linalg.LinearOperatorBlockDiag([operator_1,operator_2])
blk_diag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.,  2.,  3.,  0.,  0.],
       [ 4.,  5.,  6.,  0.,  0.],
       [ 7.,  8.,  9.,  0.,  0.],
       [ 0.,  0.,  0., -9., -8.],
       [ 0.,  0.,  0., -7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;block_lower_triangular_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;block-lower-triangular-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Block lower triangular matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_3 = tf.linalg.LinearOperatorFullMatrix(tf.constant(tf.repeat(6.,repeats = 6), shape = (2,3)))
blk_lower = tf.linalg.LinearOperatorBlockLowerTriangular([[operator_1], [operator_3, operator_2]])
blk_lower.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 1.,  2.,  3.,  0.,  0.],
       [ 4.,  5.,  6.,  0.,  0.],
       [ 7.,  8.,  9.,  0.,  0.],
       [ 6.,  6.,  6., -9., -8.],
       [ 6.,  6.,  6., -7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;householder_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;householder-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Householder matrix&lt;/h4&gt;
&lt;p&gt;Householder matrix can be used to triangularize a matrix using orthogonal matrices (the process is called orthogonal triangularization). But we will not pursue that point here. We will only show the method using only a column vector. Given a vector &lt;span class=&#34;math inline&#34;&gt;\(v = [1, 4, 7]^T\)&lt;/span&gt;, Householder transform can transform the vector into &lt;span class=&#34;math inline&#34;&gt;\(v = \|v\|_2[1,0,0]^T\)&lt;/span&gt;. It is achieved by multiplying the vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; by an orthogonal Householder matrix &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[H\begin{pmatrix}
v_1\\
v_2\\
v_3\end{pmatrix}=\|v\|_2\begin{pmatrix}
1\\
0\\
0\end{pmatrix}\]&lt;/span&gt;
This process can be repeated with other columns of a matrix to transform it into an upper triangular one. For more details, have a look at the references.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;first_column_of_operator_1 = operator_1.to_dense()[:,0]
norm = tf.linalg.norm(first_column_of_operator_1)
vec = first_column_of_operator_1 - norm*tf.linalg.eye(3)[:,0]    # Whether to take positive or neagtive sign? See references.
householder = tf.linalg.LinearOperatorHouseholder(reflection_axis = vec)
householder.matmul(tf.reshape(first_column_of_operator_1, shape = (3,1))).numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[8.1240387e+00],
       [4.7683716e-07],
       [4.7683716e-07]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.matmul(householder.to_dense(), tf.reshape(first_column_of_operator_1, shape = (3,1)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[8.1240387e+00],
       [4.7683716e-07],
       [7.1525574e-07]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;kronecker_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kronecker-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Kronecker matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [4., 5., 6.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_2.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-9., -8.],
       [-7., -6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;kron_operator = tf.linalg.LinearOperatorKronecker([operator_1,operator_2])
kron_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(6, 6), dtype=float32, numpy=
array([[ -9.,  -8., -18., -16., -27., -24.],
       [ -7.,  -6., -14., -12., -21., -18.],
       [-36., -32., -45., -40., -54., -48.],
       [-28., -24., -35., -30., -42., -36.],
       [-63., -56., -72., -64., -81., -72.],
       [-49., -42., -56., -48., -63., -54.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;permutation_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Permutation matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;perm_operator = tf.linalg.LinearOperatorPermutation(tf.constant([2,4,0,3,1]))
perm_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[0., 0., 1., 0., 0.],
       [0., 0., 0., 0., 1.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;common_matrices_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-matrices-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common matrices using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;identity-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;iden_operator = tf.linalg.LinearOperatorIdentity(num_rows = 5)
iden_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;scaled_identity_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scaled-identity-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Scaled identity matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scaled_iden_operator = tf.linalg.LinearOperatorScaledIdentity(num_rows = 5, multiplier = 5.)
scaled_iden_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[5., 0., 0., 0., 0.],
       [0., 5., 0., 0., 0.],
       [0., 0., 5., 0., 0.],
       [0., 0., 0., 5., 0.],
       [0., 0., 0., 0., 5.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scaled_iden_operator_2 = tf.linalg.LinearOperatorScaledIdentity(num_rows = 3, multiplier = tf.constant([-5,7]))
scaled_iden_operator_2.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=
array([[[-5,  0,  0],
        [ 0, -5,  0],
        [ 0,  0, -5]],

       [[ 7,  0,  0],
        [ 0,  7,  0],
        [ 0,  0,  7]]], dtype=int32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonal-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diag_operator = tf.linalg.LinearOperatorDiag(tf.constant([1,2,3,4.]))
diag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 0.],
       [0., 2., 0., 0.],
       [0., 0., 3., 0.],
       [0., 0., 0., 4.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;tri-diagonal_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tri-diagonal-matrix-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tri-diagonal matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;diags = tf.constant([[-1, -1, -1, -1, 0],
                         [ 2,  2,  2,  2, 2],
                         [ 0, -1, -1, -1, -1.]])
tridiag_operator = tf.linalg.LinearOperatorTridiag(diagonals = diags)
tridiag_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=
array([[ 2., -1.,  0.,  0.,  0.],
       [-1.,  2., -1.,  0.,  0.],
       [ 0., -1.,  2., -1.,  0.],
       [ 0.,  0., -1.,  2., -1.],
       [ 0.,  0.,  0., -1.,  2.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;lower_triangular_matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lower-triangular-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Lower triangular matrix&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mat = tf.constant([[2,4,7,8],
                   [1,2,3,4],
                   [5,8,9,6],
                   [4,2,3,1]], dtype = tf.float32)
lower_tri_operator = tf.linalg.LinearOperatorLowerTriangular(mat)
lower_tri_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[2., 0., 0., 0.],
       [1., 2., 0., 0.],
       [5., 8., 9., 0.],
       [4., 2., 3., 1.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_of_zeros&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-of-zeros&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix of zeros&lt;/h4&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;zeros_operator = tf.linalg.LinearOperatorZeros(num_rows = 4, num_columns = 5, is_square = False,  is_self_adjoint=False)
zeros_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;matrix_operations_using_operators&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-operations-using-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrix operations using operators&lt;/h3&gt;
&lt;p&gt;&lt;a id = &#34;low-rank_update&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;low-rank-update&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Low-rank update&lt;/h4&gt;
&lt;p&gt;When a low rank matrix is added to a given matrix, the resulting matrix is called a low-rank update of the original matrix. Let’s suppose our original matrix was &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and we add a rank 1 update to it. The resulting matrix is &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. So
&lt;span class=&#34;math display&#34;&gt;\[B = A + uv^T\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; are column vectors. It should be noted that low-rank matrix update doesn’t always increase the rank of the original matrix. For example, if rank of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; was 2, updating it with a rank 1 matrix will not always make its rank 3. Here is an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = tf.constant([[1,2,3],
                 [2,4,6],
                 [3,4,5]], dtype = tf.float32)
u = tf.constant([[5],
                 [6],
                 [7]], dtype = tf.float32)
v = tf.constant([[7],
                 [8],
                 [9]], dtype = tf.float32)
B = A + tf.matmul(u,v, transpose_b=True)
print(&amp;quot;Rank of A = &amp;quot;, tf.linalg.matrix_rank(A).numpy())
print(&amp;quot;Rank of B = &amp;quot;, tf.linalg.matrix_rank(B).numpy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rank of A =  2
Rank of B =  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Why is it useful?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It turns out that this low rank update appears in many applications. One of the applications is in least squares. Imagine that you have solved the least squares problem using the available data. And now you get some new data. The problem is to get the new least squares fit. Well, you can start form scratch by including the new data to your old data and then fit the model on the whole data. But this is a wasteful approach. A better alternative is to use the new data to modify your old fit. If you do some mathematics, you will arrive at matrix update equation. We will not do the math here. Interested readers can check references.&lt;/p&gt;
&lt;p&gt;Computations can be much faster if we use low-rank matrix update equation. In tensorflow it is done using &lt;code&gt;tf.linalg.OperatorLowRankUpdate&lt;/code&gt; operator. Though the operator can handle more than rank 1 update, we will use it only for rank 1 update.&lt;/p&gt;
&lt;p&gt;As an example, let’s suppose we want to compute the inverse of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. We can do so by modifying the inverse of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; that we have previously computed. The result is the famous Sherman–Morrison-Woodbury formula.
&lt;span class=&#34;math display&#34;&gt;\[B^{-1} = (A+uv^T)^{-1} = A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Provided that the denominator is not equal to zero. Note that denominator is a scalar for rank 1 update. This equation show that we can compute new inverse from the old inverse by using matrix-vector and vector-vector multiplications.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [2., 3., 5.],
       [7., 8., 9.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update = tf.linalg.LinearOperatorLowRankUpdate(operator,u = u, diag_update = None, v = v)
low_rank_update.inverse().to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.625     ],
       [ 2.8333333 , -2.        , -0.24999982],
       [-0.83333325,  1.        , -0.25000012]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using Sherman–Morrison-Woodbury formula.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_inv = operator.inverse()
second_factor_numer = tf.matmul(operator_inv.matmul(u), tf.matmul(v,operator_inv.to_dense(), transpose_a=True))
second_factor_denom = 1 + tf.matmul(v,operator_inv.matmul(u), transpose_a = True)
update_inv = operator.inverse().to_dense() - (1/second_factor_denom)*second_factor_numer
print(update_inv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[-2.1666667   1.          0.6250001 ]
 [ 2.8333333  -2.         -0.25      ]
 [-0.83333325  1.         -0.25000006]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[36., 42., 48.],
       [44., 51., 59.],
       [56., 64., 72.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator.to_dense() + tf.matmul(u, v, transpose_b = True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[36., 42., 48.],
       [44., 51., 59.],
       [56., 64., 72.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Along with inverse, other methods can be applied to &lt;code&gt;LinearOperatorLowRankUpdate&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;low_rank_update.diag_part().numpy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([36., 51., 72.], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;operator_inversion&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operator-inversion&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Operator inversion&lt;/h4&gt;
&lt;p&gt;Computes inverse operator of a given operator.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;inv_operator = tf.linalg.LinearOperatorInversion(operator = operator)
inv_operator.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-2.1666667 ,  1.        ,  0.16666669],
       [ 2.8333333 , -2.        ,  0.16666669],
       [-0.83333325,  1.        , -0.16666669]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;operator_composition&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operator-composition&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Operator composition&lt;/h4&gt;
&lt;p&gt;Like composition of function, this operator applies one operator over another. In terms of matrices, it just means matrix multiplication. But the result of composition is another operator. That new operator can be used for further analysis.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;operator_1 = tf.linalg.LinearOperatorFullMatrix([[1,2],
                                                 [2,5],
                                                 [7,-3.]])
operator_2 = tf.linalg.LinearOperatorFullMatrix([[2,-1,3],
                                                 [-1,4,5.]])
operator_comp = tf.linalg.LinearOperatorComposition([operator_1,operator_2])
operator_comp.to_dense()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[  0.,   7.,  13.],
       [ -1.,  18.,  31.],
       [ 17., -19.,   6.]], dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;conclusion&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As we have seen, using only &lt;code&gt;tensorflow&lt;/code&gt; we can do quite a bit of linear algebra. In this post, we have only glossed over some of the functionalities. Clever use of the functions and operators will enable us to do much more than what has been covered here. At times, it might feel a little verbose. But the flexibility that it offers will make the exploration a rewarding experience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/linalg&#34;&gt;Tensorflow documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Datta, Biswa Nath. Numerical linear algebra and applications. Vol. 116. Siam, 2010.&lt;/li&gt;
&lt;li&gt;(The Book) Golub, Gene H., and Charles F. Van Loan. Matrix computations. Vol. 3. JHU press, 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple files in Tensorflow 2</title>
      <link>/post/reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/reading-multiple-files-in-tensorflow-2/</guid>
      <description>


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/o4aevvuqr39kq20/Reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will read multiple &lt;code&gt;.csv&lt;/code&gt; files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 &lt;code&gt;.csv&lt;/code&gt; files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of &lt;code&gt;.csv&lt;/code&gt; files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and &lt;code&gt;.csv&lt;/code&gt; files will be saved in it. Subsequently files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following codes. Jupyter notebook of this post can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. &lt;code&gt;Tensorflow&lt;/code&gt; is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/li&gt;
&lt;li&gt;Write a generator that reads data from the folder in chunks and preprocesses it.&lt;/li&gt;
&lt;li&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.
* Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.
* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





[&amp;#39;./random_data/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;Generator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. &lt;code&gt;yield&lt;/code&gt; keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For our application, we will use a generator that runs indefinitely.&lt;/p&gt;
&lt;p&gt;The following generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;. &lt;code&gt;batch_size&lt;/code&gt; determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there is no need for generators. In case our data size is huge, we can process chunks of it.&lt;/p&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re            # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  # This loop is used to run the generator indefinitely.
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
                data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
                pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
                for j in range(len(label_classes)):
                    if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                        labels.append(j)  
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the generator change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the generator loop.&lt;/p&gt;
&lt;p&gt;Now we will check whether the generator works as intended or not. We will set &lt;code&gt;batch_size&lt;/code&gt; to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use &lt;code&gt;np.random.shuffle(file_list)&lt;/code&gt; to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;generated_data = data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in generated_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)  # Just to see the lables
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;Now we will create a &lt;code&gt;tensorflow dataset&lt;/code&gt; using the generator. &lt;code&gt;Tensorflow&lt;/code&gt; datasets can conveniently be used to train &lt;code&gt;tensorflow&lt;/code&gt; models.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;tensorflow dataset&lt;/code&gt; can be created form numpy arrays or from generators.Here, we will create it using a generator. Use of the previously created generator as it is in &lt;code&gt;tensorflow datasets&lt;/code&gt; doesn’t work (Readers can verify this). This happens because of the inability of regular expression to compare a “string” with a “byte string”. “byte strings” are generated by default in tensorflow. As a way around, we will make modifications to the earlier generator and use it with tensorflow datasets. Note that we will only modified three lines. Modified lines are accompanied by commented texts beside it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) # This line has changed.
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
                data.append(temp.values.reshape(32,32,1)) 
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  # This line has changed
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()):  # This line has changed.
                        labels.append(j)
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test whether modified generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_data = tf_data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in check_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the new generator created by using a few &lt;code&gt;tensorflow&lt;/code&gt; commands works just fine as our previous generator. This new generator can now be integrated with a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,32,32,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether &lt;code&gt;dataset&lt;/code&gt; works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in dataset:
    print(data.shape, labels.shape)
    print(labels)
    print()
    num = num + 1
    if num &amp;gt; 7: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Falut_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle data (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,32,32,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,32,32,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,32,32,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  35
validation_steps =  5
steps =  10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
35/35 [==============================] - 1s 40ms/step - loss: 1.6268 - accuracy: 0.2029 - val_loss: 1.6111 - val_accuracy: 0.2000
Epoch 2/10
35/35 [==============================] - 1s 36ms/step - loss: 1.6101 - accuracy: 0.2114 - val_loss: 1.6079 - val_accuracy: 0.2600
Epoch 3/10
35/35 [==============================] - 1s 35ms/step - loss: 1.6066 - accuracy: 0.2343 - val_loss: 1.6076 - val_accuracy: 0.2000
Epoch 4/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5993 - accuracy: 0.2143 - val_loss: 1.6085 - val_accuracy: 0.2400
Epoch 5/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5861 - accuracy: 0.2657 - val_loss: 1.6243 - val_accuracy: 0.2000
Epoch 6/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5620 - accuracy: 0.3514 - val_loss: 1.6363 - val_accuracy: 0.2000
Epoch 7/10
35/35 [==============================] - 1s 36ms/step - loss: 1.5370 - accuracy: 0.2857 - val_loss: 1.6171 - val_accuracy: 0.2600
Epoch 8/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5015 - accuracy: 0.4057 - val_loss: 1.6577 - val_accuracy: 0.2000
Epoch 9/10
35/35 [==============================] - 1s 35ms/step - loss: 1.4415 - accuracy: 0.5086 - val_loss: 1.6484 - val_accuracy: 0.1400
Epoch 10/10
35/35 [==============================] - 1s 36ms/step - loss: 1.3363 - accuracy: 0.6143 - val_loss: 1.6672 - val_accuracy: 0.2200





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fcab40f6150&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 25ms/step - loss: 1.6974 - accuracy: 0.1500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6973648071289062
Test accuracy: 0.15000000596046448&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But often times, for test set, we don’t have access to true labels. Rather we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitray but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just call &lt;code&gt;model.predict()&lt;/code&gt; on this data and then use &lt;code&gt;np.argmax()&lt;/code&gt; to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a generator to read files from the prediciton set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,32,32,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(5, 32, 32, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,32,32,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)





array([[0.28138927, 0.3383776 , 0.17806269, 0.18918239, 0.01298801],
       [0.16730548, 0.20139892, 0.32996896, 0.16305783, 0.13826886],
       [0.08079846, 0.35669118, 0.4091237 , 0.13286887, 0.02051783],
       [0.01697877, 0.79075295, 0.17063092, 0.01676028, 0.00487713],
       [0.19006915, 0.02615157, 0.39364284, 0.09650648, 0.29362988],
       [0.05416911, 0.682985  , 0.19086388, 0.0668761 , 0.00510592],
       [0.21325852, 0.27782622, 0.10314588, 0.39539766, 0.01037181],
       [0.23633875, 0.3308002 , 0.30727112, 0.09573858, 0.02985144],
       [0.06442448, 0.34153524, 0.47356713, 0.08497778, 0.03549532],
       [0.37901744, 0.32311487, 0.12875995, 0.16359715, 0.00551067],
       [0.12227482, 0.49774405, 0.26021793, 0.1060346 , 0.01372868],
       [0.07139122, 0.17324339, 0.5490784 , 0.10136751, 0.10491937],
       [0.18757634, 0.2833261 , 0.3367256 , 0.14390293, 0.04846917],
       [0.23564269, 0.2800771 , 0.19150141, 0.2686058 , 0.02417296],
       [0.4835618 , 0.03908279, 0.09785527, 0.31918615, 0.06031401],
       [0.03285189, 0.5866938 , 0.3362034 , 0.0313101 , 0.01294078],
       [0.31367007, 0.05583594, 0.24806198, 0.2707511 , 0.1116809 ],
       [0.11204866, 0.05982558, 0.44611645, 0.16678827, 0.21522103],
       [0.04504926, 0.7100154 , 0.16532828, 0.0747861 , 0.00482096],
       [0.22441828, 0.01738338, 0.36729604, 0.0961706 , 0.29473177],
       [0.22392808, 0.23958267, 0.11669649, 0.41423568, 0.00555711],
       [0.11768451, 0.16422512, 0.49695587, 0.13158153, 0.08955302],
       [0.04941175, 0.31670955, 0.46190843, 0.12606393, 0.04590632],
       [0.19507076, 0.03239974, 0.3885634 , 0.14447391, 0.23949222],
       [0.3530666 , 0.08613478, 0.11636773, 0.4088019 , 0.03562902],
       [0.12874755, 0.3140329 , 0.3858064 , 0.1278494 , 0.0435637 ],
       [0.3001929 , 0.02791574, 0.11502622, 0.5044482 , 0.05241694],
       [0.0929171 , 0.1467541 , 0.6005069 , 0.06660035, 0.09322156],
       [0.10712272, 0.5518521 , 0.2632791 , 0.06340495, 0.01434106],
       [0.27723876, 0.25847596, 0.18952209, 0.25228631, 0.02247689],
       [0.12578863, 0.44461673, 0.25048074, 0.14304985, 0.03606399],
       [0.09593316, 0.06914104, 0.49921316, 0.1389045 , 0.19680816],
       [0.22185169, 0.0878747 , 0.33703303, 0.23808932, 0.11515129],
       [0.0850782 , 0.06328611, 0.57307494, 0.08615369, 0.19240707],
       [0.41479778, 0.07033634, 0.22154689, 0.2007963 , 0.09252268],
       [0.22052608, 0.10761442, 0.33570328, 0.25846007, 0.07769614],
       [0.03679338, 0.4369671 , 0.42453632, 0.07080499, 0.03089818],
       [0.17414902, 0.3666445 , 0.26953018, 0.16861232, 0.02106389],
       [0.04334973, 0.04427214, 0.5819794 , 0.02825493, 0.30214384],
       [0.23099631, 0.31964707, 0.31392127, 0.11803907, 0.01739628],
       [0.03072637, 0.6739159 , 0.25826213, 0.0309101 , 0.00618558],
       [0.20030826, 0.05058228, 0.42536664, 0.14415787, 0.17958501],
       [0.25894472, 0.0410106 , 0.25135538, 0.15487678, 0.29381245],
       [0.31544876, 0.05200702, 0.20838396, 0.31984535, 0.10431487],
       [0.10788545, 0.31769663, 0.44471365, 0.08522549, 0.04447879],
       [0.01864015, 0.35556656, 0.551683  , 0.02805553, 0.04605483],
       [0.20043266, 0.1211144 , 0.26670808, 0.33885604, 0.07288874],
       [0.29432756, 0.19128233, 0.19503927, 0.2826192 , 0.03673161],
       [0.2151616 , 0.05391361, 0.34218988, 0.11304423, 0.27569073],
       [0.241943  , 0.05663572, 0.23858468, 0.36390153, 0.09893499],
       [0.24665013, 0.22702417, 0.33673155, 0.11996701, 0.06962712],
       [0.05448309, 0.33466634, 0.49283266, 0.07876839, 0.03924957],
       [0.3060696 , 0.03565398, 0.33453086, 0.12989788, 0.19384763],
       [0.1417291 , 0.40642622, 0.20021752, 0.22896914, 0.02265806],
       [0.10395318, 0.20624556, 0.46823606, 0.12000521, 0.10156006]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 2, 2, 1, 2, 1, 3, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 3, 2,
       2, 2, 3, 2, 3, 2, 1, 0, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 4, 3,
       2, 2, 3, 0, 2, 3, 2, 2, 2, 1, 2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are randomly generated. So we should not be surprised by this result. Also note that the for each new data, softmax outputs are close to each other. This means that the network is not very sure about the classification result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a generator, and trained a model using that generator. The above code can be tweaked slightly to read any type of files other than &lt;code&gt;.csv&lt;/code&gt;. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: If along with reading, one has to perform complex transformations on extracted data (say, doing spectrogram on each segment of data, etc.), the naive approach presented in this blog may turn out to be slow. But there are ways to make these computations faster. One such speedup technique can be found at &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last modified: 27th April, 2020.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
