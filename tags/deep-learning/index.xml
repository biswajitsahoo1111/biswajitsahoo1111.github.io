<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Biswajit Sahoo</title>
    <link>/tags/deep-learning/</link>
      <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Biswajit Sahoo</copyright><lastBuildDate>Thu, 09 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon.png</url>
      <title>Deep Learning</title>
      <link>/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Reading multiple files in Tensorflow 2</title>
      <link>/post/reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/reading-multiple-files-in-tensorflow-2/</guid>
      <description>


&lt;center&gt;
(&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;Jupyter notebook for this post can be found here&lt;/a&gt;)
&lt;/center&gt;
&lt;p&gt;In this post, we will read multiple &lt;code&gt;.csv&lt;/code&gt; files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 &lt;code&gt;.csv&lt;/code&gt; files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of &lt;code&gt;.csv&lt;/code&gt; files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and &lt;code&gt;.csv&lt;/code&gt; files will be saved in it. Subsequently files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following codes. Jupyter notebook of this post can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. &lt;code&gt;Tensorflow&lt;/code&gt; is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/li&gt;
&lt;li&gt;Write a generator that reads data from the folder in chunks and preprocesses it.&lt;/li&gt;
&lt;li&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h3&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...
[&amp;#39;./random_data\\Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data\Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data has been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h3&gt;
&lt;p&gt;Generator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. &lt;code&gt;yield&lt;/code&gt; keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For out application, we will use a generator that runs indefinitely.&lt;/p&gt;
&lt;p&gt;The following generator takes a list of file names as first input. The second argument is &lt;code&gt;batch_size&lt;/code&gt;. &lt;code&gt;batch_size&lt;/code&gt; determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there is no need for generators. In case our data size is huge, we can process chunks of it.&lt;/p&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re            # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  # This loop is used to run the generator indefinitely.
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
                data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
                pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
                for j in range(len(label_classes)):
                    if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                        labels.append(j)  
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the generator change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other.&lt;/p&gt;
&lt;p&gt;Now we will check whether the generator works as intended or not. We will set &lt;code&gt;batch_size&lt;/code&gt; to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use &lt;code&gt;np.random.shuffle(file_list)&lt;/code&gt; to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;generated_data = data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in generated_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)  # Just to see the lables
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;Now we will create a &lt;code&gt;tensorflow dataset&lt;/code&gt; using the generator. &lt;code&gt;Tensorflow&lt;/code&gt; datasets can conveniently be used to train &lt;code&gt;tensorflow&lt;/code&gt; models.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;tensorflow dataset&lt;/code&gt; can be created form numpy arrays or from generators.Here, we will create it using a generator. Use of the previously created generator as it is in &lt;code&gt;tensorflow datasets&lt;/code&gt; doesn’t work (Readers can verify this). This happens because of the inability of regular expression to compare a “string” with a “byte string”. “byte strings” are generated by default in tensorflow. As a way around, we will make modifications to the earlier generator and use it with tensorflow datasets. Note that we will only modified three lines. Modified lines are accompanied by commented texts beside it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) # This line has changed.
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
                data.append(temp.values.reshape(32,32,1)) 
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  # This line has changed
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()):  # This line has changed.
                        labels.append(j)
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test whether modified generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_data = tf_data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in check_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the new generator created by using a few &lt;code&gt;tensorflow&lt;/code&gt; commands works just fine as our previous generator. This new generator can now be integrated with a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,32,32,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether &lt;code&gt;dataset&lt;/code&gt; works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in dataset:
    print(data.shape, labels.shape)
    print(labels)
    print()
    num = num + 1
    if num &amp;gt; 7: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-cnn-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Building data pipeline and training CNN model&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names, test_file_names = train_test_split(files, test_size = 0.2, random_state = 321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names, validation_file_names = train_test_split(train_file_names, test_size = 0.15, random_state = 232)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 340
Number of validation_files: 60
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,32,32,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,32,32,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,32,32,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 340 files in training set. Batch_size is 10. So if the generator runs 34 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 34. Similarly, &lt;code&gt;validation_steps = 6&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = 34
validation_steps = 6
steps = 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train for 34 steps, validate for 6 steps
Epoch 1/10
34/34 [==============================] - 2s 56ms/step - loss: 1.6156 - accuracy: 0.1735 - val_loss: 1.6097 - val_accuracy: 0.2000
Epoch 2/10
34/34 [==============================] - 1s 40ms/step - loss: 1.6094 - accuracy: 0.2059 - val_loss: 1.6108 - val_accuracy: 0.0833
Epoch 3/10
34/34 [==============================] - 1s 39ms/step - loss: 1.6091 - accuracy: 0.2265 - val_loss: 1.6115 - val_accuracy: 0.0833
Epoch 4/10
34/34 [==============================] - 1s 38ms/step - loss: 1.6089 - accuracy: 0.2265 - val_loss: 1.6123 - val_accuracy: 0.0833
Epoch 5/10
34/34 [==============================] - 1s 38ms/step - loss: 1.6087 - accuracy: 0.2265 - val_loss: 1.6134 - val_accuracy: 0.0833
Epoch 6/10
34/34 [==============================] - 1s 39ms/step - loss: 1.6084 - accuracy: 0.2265 - val_loss: 1.6140 - val_accuracy: 0.0833
Epoch 7/10
34/34 [==============================] - 1s 40ms/step - loss: 1.6084 - accuracy: 0.2265 - val_loss: 1.6150 - val_accuracy: 0.0833
Epoch 8/10
34/34 [==============================] - 1s 40ms/step - loss: 1.6082 - accuracy: 0.2265 - val_loss: 1.6159 - val_accuracy: 0.0833
Epoch 9/10
34/34 [==============================] - 1s 39ms/step - loss: 1.6080 - accuracy: 0.2265 - val_loss: 1.6166 - val_accuracy: 0.0833
Epoch 10/10
34/34 [==============================] - 1s 39ms/step - loss: 1.6079 - accuracy: 0.2265 - val_loss: 1.6171 - val_accuracy: 0.0833
&amp;lt;tensorflow.python.keras.callbacks.History at 0x65ec0f0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 31ms/step - loss: 1.6133 - accuracy: 0.1800&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6132649421691894
Test accuracy: 0.18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly. This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a generator, and trained a model using that generator. The above code can be tweaked slightly to read any type of files other than &lt;code&gt;.csv&lt;/code&gt;. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Python Generators</title>
      <link>/post/using-python-generators/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/using-python-generators/</guid>
      <description>


&lt;center&gt;
(&lt;a href=&#34;https://gist.github.com/biswajitsahoo1111/33cea59f24de6c19d1a513b42d28674d&#34;&gt;Jupyter notebook for this post can be found here&lt;/a&gt;)
&lt;/center&gt;
&lt;center&gt;
(&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;Check out this post for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;&lt;/a&gt;)
&lt;/center&gt;
&lt;p&gt;In this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful to the beginner.&lt;/p&gt;
&lt;p&gt;While functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It’s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.&lt;/p&gt;
&lt;p&gt;First let’s create artificial data that we will extract later batch by batch.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
data = np.random.randint(100,150, size = (10,2,2))
labels = np.random.permutation(10)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[149 136]
  [149 121]]

 [[127 132]
  [126 105]]

 [[113 108]
  [107 131]]

 [[127 107]
  [107 143]]

 [[105 132]
  [105 143]]

 [[145 103]
  [128 123]]

 [[103 107]
  [131 108]]

 [[130 104]
  [122 104]]

 [[130 121]
  [128 102]]

 [[102 108]
  [141 105]]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;labels:&amp;quot;, labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;labels: [1 7 6 0 4 3 9 5 2 8]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def my_gen(data, labels, batch_size = 2):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(labels):
            i = 0
            idx = np.random.permutation(len(labels))
            data, labels = data[idx], labels[idx]
            continue
        else:
            X = data[i*batch_size:(i+1)*batch_size,:]
            y = labels[i*batch_size:(i+1)*batch_size]
            i += 1
            yield X,y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found &lt;a href=&#34;https://stackoverflow.com/questions/42727412/efficient-way-to-partially-read-large-numpy-file&#34;&gt;here&lt;/a&gt;. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using “np.shuffle()”.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_data = my_gen(data,labels)
for i in range(10):
    X,y = next(get_data)
    print(X,y)
    print(X.shape, y.shape)
    print(&amp;quot;=========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[149 136]
  [149 121]]

 [[127 132]
  [126 105]]] [1 7]
(2, 2, 2) (2,)
=========================
[[[113 108]
  [107 131]]

 [[127 107]
  [107 143]]] [6 0]
(2, 2, 2) (2,)
=========================
[[[105 132]
  [105 143]]

 [[145 103]
  [128 123]]] [4 3]
(2, 2, 2) (2,)
=========================
[[[103 107]
  [131 108]]

 [[130 104]
  [122 104]]] [9 5]
(2, 2, 2) (2,)
=========================
[[[130 121]
  [128 102]]

 [[102 108]
  [141 105]]] [2 8]
(2, 2, 2) (2,)
=========================
[[[130 104]
  [122 104]]

 [[149 136]
  [149 121]]] [5 1]
(2, 2, 2) (2,)
=========================
[[[127 107]
  [107 143]]

 [[105 132]
  [105 143]]] [0 4]
(2, 2, 2) (2,)
=========================
[[[127 132]
  [126 105]]

 [[103 107]
  [131 108]]] [7 9]
(2, 2, 2) (2,)
=========================
[[[145 103]
  [128 123]]

 [[130 121]
  [128 102]]] [3 2]
(2, 2, 2) (2,)
=========================
[[[102 108]
  [141 105]]

 [[113 108]
  [107 131]]] [8 6]
(2, 2, 2) (2,)
=========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
import math

class my_new_gen(tf.keras.utils.Sequence):
    def __init__(self, data, labels, batch_size= 2 ):
        self.x, self.y = data, labels
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])

    def __len__(self):
        return math.floor(self.x.shape[0] / self.batch_size)

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x = self.x[inds]
        batch_y = self.y[inds]
        return batch_x, batch_y
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we must add &lt;code&gt;len&lt;/code&gt; method and &lt;code&gt;getitem&lt;/code&gt; method within the class and if we want to shuffle data between epochs, we have to add &lt;code&gt;on_epoch_end()&lt;/code&gt; method. &lt;code&gt;len&lt;/code&gt; finds out the number of batches possible in an epoch and &lt;code&gt;getitem&lt;/code&gt; extracts batches one by one. When one epoch is complete, &lt;code&gt;on_epoch_end()&lt;/code&gt; shuffles the data and the process continues. We will test it with an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_new_data = my_new_gen(data, labels)

for i in range(10):
    if i == 5:
        get_new_data.on_epoch_end()
        i = 0
    elif i &amp;gt; 5:
        i = i-5
    dat,labs = get_new_data.__getitem__(i)
    print(dat,labs)
    print(dat.shape, labs.shape)
    print(&amp;quot;===========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[149 136]
  [149 121]]

 [[127 132]
  [126 105]]] [1 7]
(2, 2, 2) (2,)
===========================
[[[113 108]
  [107 131]]

 [[127 107]
  [107 143]]] [6 0]
(2, 2, 2) (2,)
===========================
[[[105 132]
  [105 143]]

 [[145 103]
  [128 123]]] [4 3]
(2, 2, 2) (2,)
===========================
[[[103 107]
  [131 108]]

 [[130 104]
  [122 104]]] [9 5]
(2, 2, 2) (2,)
===========================
[[[130 121]
  [128 102]]

 [[102 108]
  [141 105]]] [2 8]
(2, 2, 2) (2,)
===========================
[[[127 132]
  [126 105]]

 [[105 132]
  [105 143]]] [7 4]
(2, 2, 2) (2,)
===========================
[[[113 108]
  [107 131]]

 [[149 136]
  [149 121]]] [6 1]
(2, 2, 2) (2,)
===========================
[[[102 108]
  [141 105]]

 [[103 107]
  [131 108]]] [8 9]
(2, 2, 2) (2,)
===========================
[[[130 121]
  [128 102]]

 [[127 107]
  [107 143]]] [2 0]
(2, 2, 2) (2,)
===========================
[[[145 103]
  [128 123]]

 [[130 104]
  [122 104]]] [3 5]
(2, 2, 2) (2,)
===========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have also used generators to train MNIST example. The code can be found &lt;a href=&#34;https://gist.github.com/biswajitsahoo1111/33cea59f24de6c19d1a513b42d28674d&#34;&gt;here&lt;/a&gt;. The example might seem bit stretched as we don’t need generators for small data sets like MNIST. The aim of the example is just to show different implementation using generators.&lt;/p&gt;
&lt;p&gt;Perhaps the most detailed blog about using generators for deep learning is &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;this one&lt;/a&gt;. I also found &lt;a href=&#34;https://github.com/keras-team/keras/issues/9707#issuecomment-374609666&#34;&gt;these comments&lt;/a&gt; helpful.&lt;/p&gt;
&lt;p&gt;Update: With the release of &lt;code&gt;tensorflow-2.0&lt;/code&gt;, it is much easier to use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API for handling large datasets. Generators can still be used for training using &lt;code&gt;tf.keras&lt;/code&gt;. As a final note, use generators if it is absolutely essential to do so. Otherwise, use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API. Check out &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;this post&lt;/a&gt; for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data-Driven Machinery Fault Diagnosis</title>
      <link>/project/personal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/personal-project/</guid>
      <description>

&lt;p&gt;&lt;center&gt;(&lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;Visit this page for an updated list of results&lt;/a&gt;)&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction-to-the-problem&#34;&gt;Introduction to the problem&lt;/h2&gt;

&lt;p&gt;Condition based maintenance is the process of doing maintenance only when it is required. Adoption of this maintenance strategy leads to significant monetary gains as it precludes periodic maintenance and reduces unplanned downtime. Another term commonly used for condition based maintenance is predictive maintenance. As the name suggests, in this method we predict in advance when to perform maintenance. Maintenance is required, if fault has already occurred or is imminent. This leads us to the problem of fault diagnosis and prognosis.&lt;/p&gt;

&lt;p&gt;In fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognosis, our aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model Based Approaches&lt;/li&gt;
&lt;li&gt;Data-Driven Approaches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In model based approach, a complete model of the system is formulated and it is then used for fault diagnosis and prognosis. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modelling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.&lt;/p&gt;

&lt;p&gt;In data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognosis. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognosis tasks.&lt;/p&gt;

&lt;h2 id=&#34;aim-of-the-project&#34;&gt;Aim of the project&lt;/h2&gt;

&lt;p&gt;In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.&lt;/p&gt;

&lt;p&gt;This is an ongoing project and modifications and additions of new techniques will be done over time. Python, R, and MATLAB are popular programming languages that are used for machine learning applications. We will use those for our demonstrations. &lt;strong&gt;This page contains results on fault diagnosis only. Results on fault prognosis will be summarized in a separate webpage.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;results-using-case-western-reserve-university-bearing-data-https-csegroups-case-edu-bearingdatacenter-pages-welcome-case-western-reserve-university-bearing-data-center-website&#34;&gt;Results using &lt;a href=&#34;https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-western-reserve-university-bearing-data-center-website&#34;&gt;Case Western Reserve University Bearing Data&lt;/a&gt;&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time.pdf&#34;&gt;SVM on time domain
features (10 classes, sampling frequency: 48k)&lt;/a&gt;(Overall accuracy: &lt;strong&gt;96.4%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru.pdf&#34;&gt;SVM on wavelet packet energy features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;99.3%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Visualizing High Dimensional Data Using Dimensionality Reduction Techniques&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Python Code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/dimensionality_reduction_projection.pdf&#34;&gt;R Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru.pdf&#34;&gt;SVM on wavelet packet entropy features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;99.2%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru.pdf&#34;&gt;SVM on time and wavelet packet features (12 classes, sampling frequency: 12k)&lt;/a&gt; (&lt;strong&gt;Achieves 100% test accuracy in one case&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression.pdf&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features (10 classes, sampling frequency: 48k)&lt;/a&gt;(Overall accuracy: &lt;strong&gt;93.8%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k.pdf&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98.8%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k.pdf&#34;&gt;LDA on wavelet packet energy features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;88.3%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k.pdf&#34;&gt;LDA on wavelet packet energy features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k.pdf&#34;&gt;QDA on wavelet packet energy features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;97.7%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k.pdf&#34;&gt;QDA on wavelet packet energy features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k.pdf&#34;&gt;kNN on wavelet packet energy features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;89%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k.pdf&#34;&gt;kNN on wavelet packet energy features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98.8%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k.pdf&#34;&gt;Decision tree on wavelet packet features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;91.3%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k.pdf&#34;&gt;Decision tree on wavelet packet features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98.3%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k.pdf&#34;&gt;Bagging on wavelet packet features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;96.8%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k.pdf&#34;&gt;Bagging on wavelet packet features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k.pdf&#34;&gt;Boosting on wavelet packet features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98.6%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k.pdf&#34;&gt;Boosting on wavelet packet features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k.pdf&#34;&gt;Random forest on wavelet packet features (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k.pdf&#34;&gt;Random forest on wavelet packet features (12 classes, sampling frequency: 12k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;enter-deep-learning&#34;&gt;Enter Deep Learning&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Deep_Learning_CWRU_Blog.ipynb&#34;&gt;Fault diagnosis using convolutional neural network (CNN) (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;96.2%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/CWRU_CNN_Wavelet_Git_Final.ipynb&#34;&gt;CNN based fault diagnosis using continuous wavelet transform (CWT) (10 classes, sampling frequency: 48k)&lt;/a&gt; (Overall accuracy: &lt;strong&gt;98.3%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(This list will be updated gradually.)&lt;/p&gt;

&lt;h2 id=&#34;some-other-related-stuff&#34;&gt;Some other related stuff&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.awsar-dst.in/assets/winner_article_2018/30_PhD.pdf&#34;&gt;Fault diagnosis of machines (A non-technical introduction)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/matlab_intro.pdf&#34;&gt;A quick introduction to MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/transient_vibration_and_SRS_plots.pdf&#34;&gt;Transient vibration and shock response spectrum plots in MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;Simple examples on finding instantaneous frequency using Hilbert transform&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;MATLAB Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;Readers who use the processed datasets of this page &lt;strong&gt;must&lt;/strong&gt; cite the original data source as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{casewesternbearingdata,
  url = {https://csegroups.case.edu/bearingdatacenter/home},
  note = {This data come from Case Western Reserve University Bearing Data Center Website}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For attribution, readers &lt;strong&gt;may&lt;/strong&gt; cite this project as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{sahoo2016datadriven,
  author = {Sahoo, Biswajit},
  title = {Data-Driven Machinery Fault Diagnosis},
  url = {https://biswajitsahoo1111.github.io/cbm_codes_open/},
  year = {2016}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
