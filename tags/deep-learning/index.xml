<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Biswajit Sahoo</title>
    <link>https://biswajitsahoo1111.github.io/tags/deep-learning/</link>
      <atom:link href="https://biswajitsahoo1111.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Biswajit Sahoo</copyright><lastBuildDate>Thu, 01 Apr 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://biswajitsahoo1111.github.io/img/icon.png</url>
      <title>Deep Learning</title>
      <link>https://biswajitsahoo1111.github.io/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>IndexedSlices in Tensorflow</title>
      <link>https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Tensorflow_IndexedSlices.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Tensorflow_IndexedSlices.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/paze6mubda9bvv4/Tensorflow_IndexedSlices.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will discuss about &lt;code&gt;IndexedSlices&lt;/code&gt; class of &lt;code&gt;Tensorflow&lt;/code&gt;. We will try to answer the following questions in this blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#q1&#34;&gt;What are &lt;code&gt;IndexedSlices&lt;/code&gt;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q2&#34;&gt;Where do we get it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q3&#34;&gt;How to convert from IndexedSlices to tensors?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id = &#34;q1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;what-are-indexedslices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are &lt;code&gt;IndexedSlices&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;According to &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/IndexedSlices&#34;&gt;&lt;code&gt;Tensorflow&lt;/code&gt; documentation&lt;/a&gt;, &lt;code&gt;IndexedSlices&lt;/code&gt; are sparse representation of a set of tensor slices at a given index. At an high level it appears to be some kind of sparse representation. Let’s try to understand it with examples.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;q2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-do-we-get-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where do we get it?&lt;/h2&gt;
&lt;p&gt;We get &lt;code&gt;IndexedSlices&lt;/code&gt; while taking gradients of an &lt;code&gt;Embedding&lt;/code&gt; layer. Embedding matrices can be huge (depending on vocabulary size). But each batch only contains a small fraction of tokens. So while computing the gradient of loss with respect to embedding layer, in each pass we have to only consider the corresponding token embeedings of the present batch. Naturally a sparse tensor seems to be a better option to record those gradients. &lt;code&gt;Tensorflow&lt;/code&gt; does that using &lt;code&gt;IndexedSlices&lt;/code&gt;. We will show that below using a contrived example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow version: &amp;quot;, tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.models.Sequential([
    # Vocab size: 10, Embedding dimension: 4, Input_shape size: (batch_size, num_words). As usual, batch_size is omitted.
    tf.keras.layers.Embedding(10, 4, input_shape = (5,)),
    tf.keras.layers.Flatten(), 
    tf.keras.layers.Dense(1)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 5, 4)              40        
_________________________________________________________________
flatten (Flatten)            (None, 20)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 21        
=================================================================
Total params: 61
Trainable params: 61
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data = tf.random.uniform(shape = (1, 5), minval = 0, maxval = 10, dtype = tf.int32) # Batch size is 1.
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[6, 1, 1, 4, 8]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.variables  # Is a list of 3 tensors. 1 from Embedding layer and 2 from Dense layer (Kernel and bias)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;tf.Variable &amp;#39;embedding/embeddings:0&amp;#39; shape=(10, 4) dtype=float32, numpy=
 array([[ 4.10897247e-02, -2.48962641e-03,  1.26880072e-02,
          3.39310430e-02],
        [ 3.28579657e-02,  3.90318781e-03,  2.81411521e-02,
          3.09719704e-02],
        [ 1.16247907e-02, -1.41257644e-02, -3.36343870e-02,
         -4.41543460e-02],
        [-4.67238426e-02,  2.42819674e-02, -4.26802635e-02,
         -2.59207971e-02],
        [ 2.28367783e-02, -2.09717881e-02,  1.05572566e-02,
          3.33249308e-02],
        [-3.37148309e-02, -4.61939685e-02, -2.61853095e-02,
         -4.10162285e-03],
        [-3.59787717e-02,  2.78765075e-02, -3.16200405e-02,
          4.54976298e-02],
        [-4.67344411e-02, -1.30221620e-02,  1.52915232e-02,
          2.22466923e-02],
        [-1.03901625e-02,  2.40740217e-02, -1.24427900e-02,
          4.47194651e-03],
        [-3.57637033e-02,  4.28059734e-02, -2.59280205e-05,
          4.09286283e-02]], dtype=float32)&amp;gt;,
 &amp;lt;tf.Variable &amp;#39;dense/kernel:0&amp;#39; shape=(20, 1) dtype=float32, numpy=
 array([[ 0.42870212],
        [ 0.04779923],
        [ 0.4126016 ],
        [-0.13294601],
        [-0.3175783 ],
        [-0.46080017],
        [-0.23412797],
        [ 0.30137837],
        [-0.5197849 ],
        [-0.10935467],
        [ 0.5087845 ],
        [-0.06930307],
        [ 0.10028934],
        [-0.11278141],
        [-0.21269777],
        [-0.0214209 ],
        [ 0.12959635],
        [-0.13330323],
        [-0.23972857],
        [ 0.23718971]], dtype=float32)&amp;gt;,
 &amp;lt;tf.Variable &amp;#39;dense/bias:0&amp;#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)
loss_object = tf.keras.losses.MeanSquaredError()
target = tf.constant([2.5], shape = (1,1))
for _ in range(2):   # Let&amp;#39;s run gradient descent for two batches of the same input data. (It&amp;#39;s a contrived examples)
    with tf.GradientTape() as tape:
        output = model(data) # Output has shape: (batch_size, 1). Here batch_size is 1. So output shape is (1,1)
        loss_value = loss_object(target, output)  # Calculating some random loss.
    grads = tape.gradient(loss_value, model.trainable_variables)

    # Gradient descent step
    optimizer.apply_gradients(zip(grads, model.trainable_variables))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;len(grads)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;grads[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tensorflow.python.framework.indexed_slices.IndexedSlices at 0x16c04d4a970&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(grads[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IndexedSlices(indices=tf.Tensor([6 1 1 4 8], shape=(5,), dtype=int32), values=tf.Tensor(
[[-0.9495101  -0.14344962 -0.91739434  0.25398374]
 [ 0.69607455  1.0615798   0.5085497  -0.7338184 ]
 [ 1.1639317   0.24842012 -1.2103697   0.12384857]
 [-0.25895947  0.2856651   0.47968888  0.01028775]
 [-0.28760925  0.28005898  0.56933826 -0.5540699 ]], shape=(5, 4), dtype=float32), dense_shape=tf.Tensor([10  4], shape=(2,), dtype=int32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An &lt;code&gt;IndexedSlices&lt;/code&gt; object has 3 main entries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;indices&lt;/li&gt;
&lt;li&gt;values, and&lt;/li&gt;
&lt;li&gt;dense_shape&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id = &#34;q3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-convert-indexedslices-to-tensors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to convert &lt;code&gt;IndexedSlices&lt;/code&gt; to &lt;code&gt;Tensors&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;Before we do the conversion, let’s answer a relevant question: Why do we have to do the conversion from &lt;code&gt;IndexedSlices&lt;/code&gt; to tensors given that &lt;code&gt;Tensorflow&lt;/code&gt; can do a gradient descent step automatically through the &lt;code&gt;IndexedSlices&lt;/code&gt;? In the last section, we could run 2 gradient descent steps without worrying about &lt;code&gt;IndexedSlices&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But the problem occurs if we want to do some processing on gradient values. One such processing is &lt;code&gt;gradient clipping&lt;/code&gt;. In &lt;code&gt;gradient clipping&lt;/code&gt;, if sum of norm of gradients exceed a given value, gradients are rescaled to decrease their magnitude. Therefore, to do any gradient clipping, we have to access the gradient tensors. This is precisely where we would like to convert IndexedSlices to tensors. Having an embedding layer is common in deep learning models and applying gradient clipping to gradient values is also a common practice. We will show two approaches to do the conversion.&lt;/p&gt;
&lt;div id=&#34;easiest-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Easiest approach&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tf.convert_to_tensor(grads[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: shape=(10, 4), dtype=float32, numpy=
array([[ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 1.8600063 ,  1.31      , -0.70182   , -0.60996985],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.25895947,  0.2856651 ,  0.47968888,  0.01028775],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.9495101 , -0.14344962, -0.91739434,  0.25398374],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.28760925,  0.28005898,  0.56933826, -0.5540699 ],
       [ 0.        ,  0.        ,  0.        ,  0.        ]],
      dtype=float32)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-did-just-happen-in-the-last-step&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What did just happen in the last step?&lt;/h3&gt;
&lt;p&gt;Though the last approach is a single line elegant solution, it hides many things. How actually is the conversion done? The code below shows the steps in which we can manually do the conversion.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_grad = tf.zeros_like(model.variables[0]).numpy()   # Create a dense tensor of all zeros
for i, ind in enumerate(grads[0].indices):
    check_grad[ind] = check_grad[ind] + grads[0].values[i]
check_grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 1.8600063 ,  1.31      , -0.70182   , -0.60996985],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.25895947,  0.2856651 ,  0.47968888,  0.01028775],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.9495101 , -0.14344962, -0.91739434,  0.25398374],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [-0.28760925,  0.28005898,  0.56933826, -0.5540699 ],
       [ 0.        ,  0.        ,  0.        ,  0.        ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This brings us to the end of this blog. I hope this blog has demystified a few things about &lt;code&gt;IndexedSlices&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation for this post&lt;/strong&gt;: While writing TF 2 code for &lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Attention Mechanisms chapter of D2L book&lt;/a&gt;, the author encountered an error involving &lt;code&gt;IndexedSlices&lt;/code&gt;. After spending a good deal of time hopelessly trying to figure out what’s going on, the author finally found that the error was occurring because of an user defined gradient clipping function that didn’t handle &lt;code&gt;IndexedSlices&lt;/code&gt; properly. The model involved embedding layers as it was dealing with machine translation task. Therefore, I thought of writing this blog with the hope that it would be of help to readers who are struggling to figure out what &lt;code&gt;IndexedSlices&lt;/code&gt; are.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book</title>
      <link>https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;
&lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;amp;repo=D2L_Attention_Mechanisms_in_TF&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;
&lt;/iframe&gt;
&lt;/td&gt;
&lt;!-----
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  -----&gt;
&lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
&lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download code (.zip)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;amp;repo=D2L_Attention_Mechanisms_in_TF&amp;amp;type=fork&amp;amp;count=true&amp;amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;
&lt;/iframe&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This post contains &lt;code&gt;Tensorflow 2&lt;/code&gt; code for Attention Mechanisms chapter of &lt;a href=&#34;http://d2l.ai/&#34;&gt;Dive into Deep Learning (D2L)&lt;/a&gt; book. The chapter has 7 sections and code for each section can be found at the following links. We have given only code implementations. For theory, readers should refer the book.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_1_Visualization_of_attention.ipynb&#34;&gt;10.1. Attention Cues&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_2_Attention_based_regression.ipynb&#34;&gt;10.2. Attention Pooling: Nadaraya-Watson Kernel Regression&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_3_Attention_scoring_functions.ipynb&#34;&gt;10.3. Attention Scoring Functions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_4_Bahdanau_attention.ipynb&#34;&gt;10.4. Bahdanau Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_5_Multi-head_attention.ipynb&#34;&gt;10.5. Multi-Head Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_6_Self-attention_and_positional_encoding.ipynb&#34;&gt;10.6. Self-Attention and Positional Encoding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/10_7_Transformer.ipynb&#34;&gt;10.7. Transformer&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;additional-sections&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional sections:&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF/blob/master/additional_sections/9_7_Sequence_to_Sequence_Learning.ipynb&#34;&gt;9.7. Sequence to Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-run-these-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to run these code:&lt;/h3&gt;
&lt;p&gt;The best way (in our opinion) is to either clone the repo (or download the zipped repo) and then run each notebook from the cloned (or extracted) folder. All the notebooks will run without any issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We claim no originality for the code. Credit goes to the authors of this excellent &lt;a href=&#34;http://d2l.ai/&#34;&gt;book&lt;/a&gt;. However, all errors and omissions are my own and readers are encouraged to bring it to my notice. Finally, no TF code was available (to the best of my knowledge) for &lt;code&gt;Attention Mechanisms&lt;/code&gt; chapter when &lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;this repo&lt;/a&gt; was first made public.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple files in Tensorflow 2 using Sequence</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Reading_mulitple_files_using_tensorflow_sequence.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Reading_mulitple_files_using_tensorflow_sequence.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/weog8ubsu9qcugv/Reading_mulitple_files_using_tensorflow_sequence.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will read multiple csv files using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;Tensroflow Sequence&lt;/a&gt;. In an &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;earlier post&lt;/a&gt; we had demonstrated the procedure for reading multiple csv files using a custom generator. Though generators are convenient for handling chunks of data from a large dataset, they have limited portability and scalability (&lt;a href=&#34;https://www.tensorflow.org/guide/data#consuming_python_generators&#34;&gt;see the caution here&lt;/a&gt;). Therefore, Tensorflow prefers &lt;code&gt;Sequence&lt;/code&gt; over generators.&lt;/p&gt;
&lt;p&gt;Sequence is similar to &lt;code&gt;Tensorflow Dataset&lt;/code&gt; but provides flexibility for batched data preparation in a custom manner. &lt;code&gt;Tensorflow Dataset&lt;/code&gt; provides a wide range of functionalities to handle different data types. But for some specific applications we might have to make some custom modifications for which built-in methods are not available in &lt;code&gt;tensorflow dataset&lt;/code&gt;. In that case, we can use &lt;code&gt;Sequence&lt;/code&gt; to create our own dataset equivalent. In this post, we will show how to use sequence to read multiple csv files. The method we will discuss is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 .csv files. But the method can be easily extended to huge datasets involving thousands of csv files.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. Tensorflow is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#sec_1&#34;&gt;Create 500 “.csv” files and save it in the folder “random_data” in current directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_2&#34;&gt;Write a sequence object that reads data from the folder in chunks and preprocesses it.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_3&#34;&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_4&#34;&gt;Make prediction on new data for which labels are not known.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id=&#34;sec_1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 .csv files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each .csv file will have one column of data with 1024 entries.&lt;/li&gt;
&lt;li&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …,“Fault_1_100.csv”. Similarly for other classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...

[&amp;#39;./random_data\\Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data\\Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data\Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, create a custom &lt;code&gt;Sequence&lt;/code&gt; object, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-custom-sequence-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a custom Sequence object&lt;/h2&gt;
&lt;p&gt;According to &lt;code&gt;Tensorflow&lt;/code&gt; &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;documentation&lt;/a&gt;, every Sequence must implement following two methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__getitem__&lt;/code&gt; method: Iterates over the dataset chunk by chunk. It must return a complete batch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__len__&lt;/code&gt; method: Returns the length of the Sequence (total number of batches that we can extract from the full dataset).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we create a custom sequence object, we have complete control over the arguments we want to pass it. The following sequence object takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re    # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow version: &amp;quot;, tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class CustomSequence(tf.keras.utils.Sequence):  # It inherits from `tf.keras.utils.Sequence` class
  def __init__(self, filenames, batch_size):  # Two input arguments to the class.
        self.filenames= filenames
        self.batch_size = batch_size

  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))

  def __getitem__(self, idx):  # idx is index that runs from 0 to length of sequence
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size] # Select a chunk of file names
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]

        for file in batch_x:   # In this loop read the files in the chunk that was selected previously
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
            data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
            pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
            for j in range(len(label_classes)):
                if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                    labels.append(j)  
        data = np.asarray(data).reshape(-1,32,32,1)
        labels = np.asarray(labels)
        return data, labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the &lt;code&gt;__getitem__&lt;/code&gt; method change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the &lt;code&gt;__getitem__&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Now we will check whether the dataset works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence = CustomSequence(filenames = files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sequence.__len__()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;len(list(sequence))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sequence is not an infinite loop. Let’s check again. This is yet another way to check the length of the sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;counter = 0
for _,_ in sequence:
    counter = counter + 1
print(counter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for num, (data, labels) in enumerate(sequence):
    print(data.shape, labels.shape)
    print(labels)
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;We can pass this sequence directly to &lt;code&gt;model.fit()&lt;/code&gt; to train our deep learning model. Now that sequence works fine, we will use it to train a simple deep learning model. The focus of this post is not on the model itself. So we will use a simplest model. If readers want a different model, they can do so by just replacing our model with theirs.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feed-the-chunks-of-data-to-a-cnn-model-and-train-it-for-several-epochs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Feed the chunks of data to a CNN model and train it for several epochs&lt;/h2&gt;
&lt;p&gt;But before we build the model and train it, we will first move our files to different folders depending on their fault type. We do so as it will be convenient later to create a training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders one each for a given fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data\\Fault_1&amp;#39;,
 &amp;#39;./random_data\\Fault_2&amp;#39;,
 &amp;#39;./random_data\\Fault_3&amp;#39;,
 &amp;#39;./random_data\\Fault_4&amp;#39;,
 &amp;#39;./random_data\\Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1\\Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1\\Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3\\Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3\\Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle training files (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create sequences for training, validation, and test set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_sequence = CustomSequence(filenames = train_file_names, batch_size = batch_size)
val_sequence = CustomSequence(filenames = validation_file_names, batch_size = batch_size)
test_sequence = CustomSequence(filenames = test_file_names, batch_size = batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers
model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit the model using sequence.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_sequence, validation_data = val_sequence, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
35/35 [==============================] - 2s 38ms/step - loss: 1.6305 - accuracy: 0.1788 - val_loss: 1.6095 - val_accuracy: 0.1800
Epoch 2/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6086 - accuracy: 0.2212 - val_loss: 1.6093 - val_accuracy: 0.2200
Epoch 3/10
35/35 [==============================] - 1s 16ms/step - loss: 1.6089 - accuracy: 0.2057 - val_loss: 1.6094 - val_accuracy: 0.2200
Epoch 4/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6092 - accuracy: 0.1987 - val_loss: 1.6100 - val_accuracy: 0.2000
Epoch 5/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6105 - accuracy: 0.1173 - val_loss: 1.6095 - val_accuracy: 0.2000
Epoch 6/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6069 - accuracy: 0.2062 - val_loss: 1.6098 - val_accuracy: 0.2000
Epoch 7/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6070 - accuracy: 0.2332 - val_loss: 1.6097 - val_accuracy: 0.2200
Epoch 8/10
35/35 [==============================] - 1s 17ms/step - loss: 1.6067 - accuracy: 0.2255 - val_loss: 1.6100 - val_accuracy: 0.2200
Epoch 9/10
35/35 [==============================] - 1s 19ms/step - loss: 1.6044 - accuracy: 0.2510 - val_loss: 1.6090 - val_accuracy: 0.2400
Epoch 10/10
35/35 [==============================] - 1s 20ms/step - loss: 1.5998 - accuracy: 0.3180 - val_loss: 1.6094 - val_accuracy: 0.2400





&amp;lt;tensorflow.python.keras.callbacks.History at 0x24f86841b80&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_sequence, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 14ms/step - loss: 1.6097 - accuracy: 0.2000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.609706997871399
Test accuracy: 0.20000000298023224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_4&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But oftentimes, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just make prediction on this data by calling &lt;code&gt;model.evaluate()&lt;/code&gt; and then use np.argmax() to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a separate &lt;code&gt;Sequence&lt;/code&gt; to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...

[&amp;#39;./random_data/prediction_set\\file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set\\file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prediction sequence will be slightly different from our previous custom dataset class. We only need to return data in this case.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class PredictionSequence(tf.keras.utils.Sequence):
  def __init__(self, filenames, batch_size):
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))
  def __getitem__(self, idx):
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) 
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,32,32,1)
        return data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator sequence works or not. First we will check its length.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_seq = PredictionSequence(filenames = prediction_files, batch_size=10)
print(prediction_seq.__len__())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data in prediction_seq:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(5, 32, 32, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;predictions = model.predict(prediction_seq)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)

array([[0.1673972 , 0.24178548, 0.21493195, 0.14968236, 0.22620301],
       [0.17025462, 0.23611873, 0.21584883, 0.15583281, 0.22194503],
       [0.16997598, 0.23636353, 0.2160589 , 0.15556051, 0.22204101],
       [0.17808239, 0.23929793, 0.20060928, 0.15217312, 0.22983731],
       [0.16749388, 0.24041025, 0.21612632, 0.15114665, 0.22482291],
       [0.16648369, 0.24155213, 0.21663304, 0.14991023, 0.22542089],
       [0.16956756, 0.24306948, 0.21019061, 0.14832966, 0.2288426 ],
       [0.16461462, 0.23314428, 0.22771336, 0.15880677, 0.21572094],
       [0.16099085, 0.24614859, 0.22101355, 0.14492905, 0.22691788],
       [0.16029194, 0.23579895, 0.2323411 , 0.15559815, 0.21596995],
       [0.15644614, 0.24322459, 0.23156258, 0.14755438, 0.22121225],
       [0.16045825, 0.23727809, 0.23065342, 0.1540593 , 0.2175509 ],
       [0.18453611, 0.2363899 , 0.19367108, 0.1549771 , 0.23042585],
       [0.16539477, 0.23163731, 0.22783667, 0.16050129, 0.21463   ],
       [0.16019677, 0.2387398 , 0.22967993, 0.15250796, 0.21887548],
       [0.164585  , 0.241522  , 0.2197408 , 0.14987943, 0.22427279],
       [0.1423634 , 0.2492775 , 0.25001773, 0.13962528, 0.21871613],
       [0.1685916 , 0.23507524, 0.21948369, 0.15692794, 0.21992151],
       [0.1899035 , 0.23893598, 0.18335396, 0.15139207, 0.23641457],
       [0.16065492, 0.2413149 , 0.2264044 , 0.14987133, 0.22175437],
       [0.17529766, 0.24322936, 0.20103565, 0.14799424, 0.23244311],
       [0.16809338, 0.24051304, 0.2150641 , 0.15104856, 0.22528093],
       [0.17161693, 0.23075785, 0.21867743, 0.16179037, 0.21715748],
       [0.16884513, 0.23852658, 0.21579023, 0.15319362, 0.22364433],
       [0.17458686, 0.2428582 , 0.20250764, 0.14843197, 0.23161522],
       [0.1646341 , 0.2297526 , 0.23079726, 0.16249931, 0.21231675],
       [0.15729222, 0.23233345, 0.24062563, 0.15889668, 0.210852  ],
       [0.14776482, 0.23704691, 0.2526417 , 0.15245639, 0.21009028],
       [0.16680573, 0.23577859, 0.22168827, 0.15609236, 0.21963505],
       [0.17260087, 0.24030833, 0.20811835, 0.15126604, 0.22770639],
       [0.13550763, 0.24409433, 0.26797664, 0.1427981 , 0.20962337],
       [0.15663186, 0.23662733, 0.23769669, 0.15432158, 0.21472257],
       [0.1549199 , 0.23210286, 0.24487424, 0.15878738, 0.20931558],
       [0.1547824 , 0.24484529, 0.23276137, 0.14575545, 0.22185549],
       [0.1587394 , 0.23657197, 0.23419838, 0.15462619, 0.21586415],
       [0.16478752, 0.24151964, 0.21941346, 0.14988995, 0.22438939],
       [0.16816008, 0.23736185, 0.21800458, 0.15443383, 0.22203967],
       [0.16076393, 0.24149327, 0.22604792, 0.14969479, 0.22200012],
       [0.17530249, 0.23095778, 0.21273129, 0.16161412, 0.21939443],
       [0.15688972, 0.2358581 , 0.23799387, 0.15515216, 0.21410616],
       [0.16600947, 0.23382592, 0.22481105, 0.15816282, 0.21719065],
       [0.14932008, 0.24364983, 0.24340245, 0.14621353, 0.21741422],
       [0.17864552, 0.22986974, 0.20857714, 0.16281399, 0.22009356],
       [0.1659024 , 0.22970615, 0.2287755 , 0.1626553 , 0.21296065],
       [0.17181188, 0.24148299, 0.2082077 , 0.15000671, 0.22849073],
       [0.17638905, 0.23389527, 0.20834343, 0.15828574, 0.22308654],
       [0.15240492, 0.24294852, 0.23874305, 0.14735675, 0.21854681],
       [0.18557529, 0.2407352 , 0.18793964, 0.14989276, 0.23585702],
       [0.13773988, 0.2396555 , 0.2682667 , 0.14754502, 0.20679286],
       [0.15072912, 0.24018031, 0.2443891 , 0.1498653 , 0.21483612],
       [0.15102535, 0.24379413, 0.24028388, 0.14632827, 0.2185683 ],
       [0.17316325, 0.23940398, 0.20811678, 0.1522415 , 0.22707452],
       [0.14266515, 0.24046816, 0.25842705, 0.14801402, 0.21042569],
       [0.16398384, 0.23465969, 0.22732665, 0.1571278 , 0.21690205],
       [0.16170275, 0.23673837, 0.22910713, 0.15473619, 0.2177155 ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use np.argmax() command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,
       1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1], dtype=int64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that our data are randomly generated. So we should not be surprised by this result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a custom sequence, trained a model using that sequence, and made predictions on new data. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-this-method-efficient-will-it-work-at-a-reasobable-speed-if-we-have-many-complex-preprocessing-steps-to-do-before-training-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is this method efficient? Will it work at a reasobable speed if we have many complex preprocessing steps to do before training the model?&lt;/h2&gt;
&lt;p&gt;In this blog, we have mentioned nothing about the ways to speed up the data loading process. Tensorflow prefers sequences over generators as sequences are a safer way to do multiprocessing (&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence&#34;&gt;see this&lt;/a&gt;). Sequences can be passed to &lt;code&gt;model.fit()&lt;/code&gt; along with parameters like &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt; (&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&#34;&gt;see this&lt;/a&gt;). To efficiently load data, we have to choose suitable values for &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt;. The next obvious question is: How do we choose suitable values for the parameters for our particular system architecture? The best approach, that this author can suggest, is to try different values and choose the ones that work best for your system architecture. Even when we are using complex preprocessing steps, suitable choice of above parameters will, hopefully, speedup our training process.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple csv files in PyTorch</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-csv-files-in-pytorch/</link>
      <pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-csv-files-in-pytorch/</guid>
      <description>


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/qxz8zmctpopulck/Reading_multiple_csv_files_in_PyTorch.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In many engineering applications data are usually stored in CSV (Comma Separated Values) files. In big data applications, it’s not uncommon to obtain thousands of csv files. As the number of files increases, at some point, we can no longer load the whole dataset into computer’s memory. In deep learning applications it is increasingly common to come across datasets that don’t fit in the computer’s memory. In that case, we have to devise a way so as to be able to read chunks of data at a time so that the model can be trained using the whole dataset.&lt;/p&gt;
&lt;p&gt;There are many ways to achieve this objective. In this post, we will adopt an approach that allows us to read csv files in chunks and preprocess those files in whatever way we like. Then we can pass the processed data to train any deep learning model. Though we will use csv files in this post, the method is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 csv files. But the method can be easily extended to huge datasets involving thousands of csv files.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use PyTorch to run our deep learning model. For efficiency in data loading, we will use PyTorch dataloaders.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#sec_1&#34;&gt;Create 500 “.csv” files and save it in the folder “random_data” in current working directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_2&#34;&gt;Create a custom dataloader.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_3&#34;&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sec_4&#34;&gt;Make prediction on new data for which labels are not known.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id=&#34;sec_1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 .csv files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each .csv file will have one column of data with 1024 entries.&lt;/li&gt;
&lt;li&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...

[&amp;#39;./random_data/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, create a custom dataloader, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-custom-dataloader&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a custom dataloader&lt;/h2&gt;
&lt;p&gt;We have to first create a &lt;code&gt;Dataset&lt;/code&gt; class. Then we can pass the dataset to the dataloader. Every dataset class must implement the &lt;code&gt;__len__&lt;/code&gt; method that determines the length of the dataset and &lt;code&gt;__getitem__&lt;/code&gt; method that iterates over the dataset item by item. In our case, item would mean the processed version of a chunk of data.&lt;/p&gt;
&lt;p&gt;The following dataset class takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Fault_5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re   
import torch
from torch.utils.data import Dataset
print(&amp;quot;PyTorch Version: &amp;quot;, torch.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;PyTorch Version:  1.7.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class CustomDataset(Dataset):
  def __init__(self, filenames, batch_size):
    # `filenames` is a list of strings the contains all file names.
    # `batch_size` is the determines the number of files that we want to read in a chunk.
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))   # Number of chunks.
  def __getitem__(self, idx): #idx means index of the chunk.
    # In this method, we do all the preprocessing.
    # First read data from files in a chunk. Preprocess it. Extract labels. Then return data and labels.
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]   # This extracts one batch of file names from the list `filenames`.
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
            data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
            pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
            for j in range(len(label_classes)):
                if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                    labels.append(j)  
        data = np.asarray(data).reshape(-1,1,32,32) # Because of Pytorch&amp;#39;s channel first convention
        labels = np.asarray(labels)

        # The following condition is actually needed in Pytorch. Otherwise, for our particular example, the iterator will be an infinite loop.
        # Readers can verify this by removing this condition.
        if idx == self.__len__():  
          raise IndexError

        return data, labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the &lt;code&gt;__getitem__&lt;/code&gt; method change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the &lt;code&gt;__getitem__&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Now we will check whether the dataset works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_dataset = CustomDataset(filenames = files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_dataset.__len__()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i, (data, labels) in enumerate(check_dataset):
  print(data.shape, labels.shape)
  print(labels)
  if i == 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]
(10, 1, 32, 32) (10,)
[0 0 0 0 0 0 0 0 0 0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;To train a deep learning model, we need to create a data loader from the dataset. Dataloaders offer multi-worker, multi-processing capabilities without requiring us to right codes for that. So let’s first create a dataloader from the dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from torch.utils.data import DataLoader&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dataloader = DataLoader(check_dataset,batch_size = None, shuffle = True) # Here we select batch size to be None as we have already batched our data in dataset.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether dataloader works on not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i, (data,labels) in enumerate(dataloader):
    print(data.shape, labels.shape)
    print(labels)  # Just to see the labels.
    if i == 3: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
torch.Size([10, 1, 32, 32]) torch.Size([10])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that dataloader works, we will use it to train a simple deep learning model. The focus of this post is not on the model itself. So we will use a simplest model. If readers want a different model, they can do so by just replacing our model with theirs.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;sec_3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feed-chunks-of-data-to-a-cnn-model-and-train-it-for-several-epochs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Feed chunks of data to a CNN model and train it for several epochs&lt;/h2&gt;
&lt;p&gt;But before we build the model and train it, we will first move our files to different folders depending on their fault type. We do so as it will be convenient later to create a training, validation, and test set from the data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders one each for a given fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle training files (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create the datasets and dataloaders for training, validation, and test set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = CustomDataset(filenames = train_file_names, batch_size = batch_size)
val_dataset = CustomDataset(filenames = validation_file_names, batch_size = batch_size)
test_dataset = CustomDataset(filenames = test_file_names, batch_size = batch_size)

train_dataloader = DataLoader(train_dataset, batch_size = None, shuffle = True)
val_dataloader = DataLoader(val_dataset, batch_size = None)  # Shuffle is False by default.
test_dataloader = DataLoader(test_dataset, batch_size = None)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model. We will build one of the simplest models. Readers are free to choose a different model of their choice. If &lt;code&gt;torchsummary&lt;/code&gt; is not installed, use &lt;code&gt;pip install torchsummary&lt;/code&gt; to install it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear, ReLU, Softmax
from torchsummary import summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dev = torch.device(&amp;quot;cuda&amp;quot;) if torch.cuda.is_available() else torch.device(&amp;quot;cpu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = Sequential(
        Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3),
        ReLU(),
        MaxPool2d(2),
        Conv2d(16,32,3),
        ReLU(),
        MaxPool2d(2),
        Flatten(),
        Linear(in_features = 1152, out_features=16),
        ReLU(),
        Linear(16, 5),
        Softmax(dim = 1)
)
model.to(dev)
summary(model,input_size = (1,32,32), device = dev.type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 30, 30]             160
              ReLU-2           [-1, 16, 30, 30]               0
         MaxPool2d-3           [-1, 16, 15, 15]               0
            Conv2d-4           [-1, 32, 13, 13]           4,640
              ReLU-5           [-1, 32, 13, 13]               0
         MaxPool2d-6             [-1, 32, 6, 6]               0
           Flatten-7                 [-1, 1152]               0
            Linear-8                   [-1, 16]          18,448
              ReLU-9                   [-1, 16]               0
           Linear-10                    [-1, 5]              85
          Softmax-11                    [-1, 5]               0
================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.35
Params size (MB): 0.09
Estimated Total Size (MB): 0.44
----------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = model.float()   # We will make all model parameters floats.
epochs = 10
for epoch in range(epochs):
    running_loss_train = 0.0
    running_loss_val = 0.0
    correct_train = 0.0
    correct_val = 0.0
    num_labels_train = 0.0
    num_labels_val = 0.0

    # Training loop
    for inputs, labels in train_dataloader:
        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.
        optimizer.zero_grad()
        outputs = model(inputs.float())
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss_train = running_loss_train + loss.item()
        correct_train = correct_train + (torch.argmax(outputs,dim = 1) == labels).float().sum()
        num_labels_train = num_labels_train + len(labels)

    # Validation loop
    for inputs, labels in val_dataloader:
        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.
        outputs = model(inputs.float())
        loss = criterion(outputs, labels)
        running_loss_val = running_loss_val + loss.item()
        correct_val = correct_val + (torch.argmax(outputs, dim = 1) == labels).float().sum()
        num_labels_val = num_labels_val + len(labels)

    train_accuracy = correct_train/num_labels_train
    val_accuracy = correct_val/num_labels_val
    print(&amp;quot;Epoch:{}, Train_loss: {:.4f}, Train_accuracy: {:.4f}, Val_loss: {:.4f}, Val_accuracy: {:.4f}&amp;quot;.\
        format(epoch, running_loss_train/len(train_dataloader), train_accuracy, running_loss_val/len(val_dataloader), val_accuracy))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:0, Train_loss: 1.6112, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:1, Train_loss: 1.6096, Train_accuracy: 0.2000, Val_loss: 1.6097, Val_accuracy: 0.2000
Epoch:2, Train_loss: 1.6095, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:3, Train_loss: 1.6090, Train_accuracy: 0.2000, Val_loss: 1.6097, Val_accuracy: 0.2000
Epoch:4, Train_loss: 1.6084, Train_accuracy: 0.2000, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:5, Train_loss: 1.6075, Train_accuracy: 0.2000, Val_loss: 1.6099, Val_accuracy: 0.2000
Epoch:6, Train_loss: 1.6084, Train_accuracy: 0.2057, Val_loss: 1.6096, Val_accuracy: 0.2000
Epoch:7, Train_loss: 1.6045, Train_accuracy: 0.2000, Val_loss: 1.6101, Val_accuracy: 0.2000
Epoch:8, Train_loss: 1.6051, Train_accuracy: 0.2000, Val_loss: 1.6100, Val_accuracy: 0.2000
Epoch:9, Train_loss: 1.6001, Train_accuracy: 0.2057, Val_loss: 1.6126, Val_accuracy: 0.2200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we make any comments on training accuracy and validation accuracy, we should keep in mind that our original dataset contains only random numbers. So it would be better if we don’t interpret the results here.&lt;/p&gt;
&lt;p&gt;Compute test score.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;running_loss_test = 0.0
correct_test = 0.0
num_labels_test = 0.0
for inputs, labels in test_dataloader:
    inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # Pytorch expects categorical targets as LongTensor.
    outputs = model(inputs.float())
    loss = criterion(outputs.to(dev), labels.to(dev))
    running_loss_test = running_loss_test + loss.item()
    correct_test = correct_test + (torch.argmax(outputs, dim = 1) == labels).float().sum()
    num_labels_test = num_labels_test + len(labels)

test_accuracy = correct_test/num_labels_test
print(&amp;quot;Test_loss: {:.4f}, Test_accuracy: {:.4f}&amp;quot;.format(running_loss_test/len(test_dataloader), test_accuracy))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test_loss: 1.6130, Test_accuracy: 0.2100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id=&#34;sec_4&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But oftentimes, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just make prediction on this data and then use &lt;code&gt;np.argmax()&lt;/code&gt; or &lt;code&gt;torch.argmax()&lt;/code&gt; to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a separate dataset to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...

[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The prediction dataset will be slightly different from our previous custom dataset class. We only need to return data in this case.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class PredictionDataset(Dataset):
  def __init__(self, filenames, batch_size):
        self.filenames= filenames
        self.batch_size = batch_size
  def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))
  def __getitem__(self, idx):
        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        data = []
        labels = []
        label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
        for file in batch_x:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) 
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,1,32,32) 
        
        
        if idx == self.__len__():  
          raise IndexError

        return data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the dataset and dataloader work or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_dataset = PredictionDataset(prediction_files, batch_size = 10)
prediction_dataloader = DataLoader(prediction_dataset,batch_size = None, shuffle = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data in prediction_dataloader:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([10, 1, 32, 32])
torch.Size([5, 1, 32, 32])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make predictions.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;preds = []
for data in prediction_dataloader:
    data = data.to(dev)
    preds.append(model(data.float()))
preds = torch.cat(preds)
preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([[0.3369, 0.0306, 0.2113, 0.3813, 0.0399],
        [0.3747, 0.0238, 0.2525, 0.3083, 0.0407],
        [0.3462, 0.0353, 0.2434, 0.3220, 0.0531],
        [0.3387, 0.0365, 0.2338, 0.3394, 0.0516],
        [0.3619, 0.0246, 0.2021, 0.3783, 0.0331],
        [0.3302, 0.0431, 0.2429, 0.3209, 0.0629],
        [0.4018, 0.0178, 0.2334, 0.3161, 0.0308],
        [0.3479, 0.0335, 0.2398, 0.3288, 0.0501],
        [0.3279, 0.0465, 0.2430, 0.3162, 0.0665],
        [0.3299, 0.0396, 0.2703, 0.2957, 0.0645],
        [0.3538, 0.0263, 0.2408, 0.3382, 0.0409],
        [0.3306, 0.0415, 0.2074, 0.3691, 0.0514],
        [0.3377, 0.0326, 0.2195, 0.3661, 0.0441],
        [0.3326, 0.0387, 0.2409, 0.3324, 0.0554],
        [0.3321, 0.0376, 0.2649, 0.3054, 0.0600],
        [0.3368, 0.0385, 0.2396, 0.3286, 0.0565],
        [0.3596, 0.0239, 0.2409, 0.3377, 0.0379],
        [0.3905, 0.0235, 0.2188, 0.3307, 0.0365],
        [0.3396, 0.0298, 0.2374, 0.3488, 0.0444],
        [0.3534, 0.0248, 0.2546, 0.3260, 0.0412],
        [0.3356, 0.0336, 0.2101, 0.3772, 0.0435],
        [0.3255, 0.0501, 0.2172, 0.3441, 0.0632],
        [0.3375, 0.0318, 0.2428, 0.3409, 0.0471],
        [0.3309, 0.0345, 0.2925, 0.2799, 0.0621],
        [0.3575, 0.0294, 0.2304, 0.3385, 0.0443],
        [0.3312, 0.0428, 0.2192, 0.3513, 0.0556],
        [0.3382, 0.0355, 0.2282, 0.3489, 0.0493],
        [0.3400, 0.0287, 0.2491, 0.3374, 0.0448],
        [0.3407, 0.0410, 0.2238, 0.3386, 0.0559],
        [0.3529, 0.0316, 0.2259, 0.3444, 0.0452],
        [0.3413, 0.0346, 0.2100, 0.3699, 0.0442],
        [0.3432, 0.0274, 0.2159, 0.3754, 0.0380],
        [0.3319, 0.0403, 0.2334, 0.3386, 0.0559],
        [0.3323, 0.0377, 0.2615, 0.3090, 0.0595],
        [0.3351, 0.0355, 0.2241, 0.3571, 0.0482],
        [0.3420, 0.0367, 0.2103, 0.3636, 0.0474],
        [0.3271, 0.0416, 0.1838, 0.4018, 0.0456],
        [0.3345, 0.0272, 0.1773, 0.4302, 0.0308],
        [0.3489, 0.0246, 0.2447, 0.3428, 0.0389],
        [0.3360, 0.0338, 0.1997, 0.3890, 0.0414],
        [0.3340, 0.0365, 0.2511, 0.3235, 0.0550],
        [0.3622, 0.0207, 0.2225, 0.3632, 0.0314],
        [0.3674, 0.0261, 0.2247, 0.3425, 0.0393],
        [0.3371, 0.0320, 0.2391, 0.3452, 0.0465],
        [0.3620, 0.0271, 0.2215, 0.3501, 0.0393],
        [0.3303, 0.0415, 0.2512, 0.3150, 0.0620],
        [0.3315, 0.0402, 0.2165, 0.3600, 0.0517],
        [0.3358, 0.0389, 0.2567, 0.3086, 0.0600],
        [0.3580, 0.0276, 0.2110, 0.3652, 0.0382],
        [0.3577, 0.0290, 0.2221, 0.3494, 0.0418],
        [0.3450, 0.0297, 0.2698, 0.3044, 0.0512],
        [0.3370, 0.0325, 0.2324, 0.3520, 0.0461],
        [0.3694, 0.0259, 0.2063, 0.3626, 0.0358],
        [0.3308, 0.0414, 0.2087, 0.3674, 0.0517],
        [0.3410, 0.0344, 0.2436, 0.3302, 0.0508]], device=&amp;#39;cuda:0&amp;#39;,
       grad_fn=&amp;lt;CatBackward&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; or &lt;code&gt;torch.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;torch.argmax(preds, dim = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tensor([3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 3, 3, 0,
        0, 3, 3, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 0, 3, 0, 3, 0, 3, 0, 0, 3, 0,
        3, 0, 0, 3, 0, 3, 0], device=&amp;#39;cuda:0&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that our data are randomly generated. So we should not be surprised by this result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a custom dataloader, trained a model using that dataloader, and made predictions on new data. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;Also note that we have not used the multi-worker and multi-processing capabilities of dataloader. To further speedup the dataloading process, readers should take advantage of the multiprocessing capabilities of dataloader. The best way to choose optimum multiprocessing and multi-worker parameters is to try a few ones and see which set of parameters work best for the system under consideration.&lt;/p&gt;
&lt;p&gt;As a final note, please keep in mind that the approach we have discussed in only one of many different ways in which we can read multiple files. I have chosen this approach as it seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope this blog would be of help to reader. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: It is very likely that this blog might not have used some of the best practices of PyTorch. This is because the author has a superficial knowledge of PyTorch and is not aware of its best practices. The author (un)fortunately prefers &lt;code&gt;Tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Efficiently reading multiple files in Tensorflow 2</title>
      <link>https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Whether this method is efficient or not is contestable. Efficiency of a data input pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they use this method in their own problems. For the simple (and small) problem considered in this post, we got no perceivable performance improvement. But for one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope this method can be of help to others as well. The system on which we ran this notebook has 44 CPU cores. &lt;code&gt;Tensorflow&lt;/code&gt; version was 2.4.0 and we did not use any GPU. Please note that for some weird reason, the speedup technique doesn’t work in &lt;code&gt;Google Colab&lt;/code&gt;. But it works in GPU enabled personal systems, that I have checked.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Along with the method described in this post, readers should also try using &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34;&gt;Tensorflow Sequence&lt;/a&gt; and see if it improves input pipeline efficiecy. Define all the complex transformations inside &lt;code&gt;__getitem__&lt;/code&gt; method of seqeuence class and then suitably choose &lt;code&gt;max_queue_size&lt;/code&gt;, &lt;code&gt;workers&lt;/code&gt;, and &lt;code&gt;use_multiprocessing&lt;/code&gt; in &lt;code&gt;model.fit()&lt;/code&gt; to improve pipeline efficiency.&lt;/p&gt;
&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
&lt;td&gt;
&lt;a target=&#34;_blank&#34; href=&#34;https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/0naikdujqvzosh4/Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;This post is a sequel to &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;an older post&lt;/a&gt;. In the previous post, we discussed ways in which we can read multiple files in &lt;code&gt;Tensorflow 2&lt;/code&gt;. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are &lt;code&gt;spectrogram&lt;/code&gt; and normalizing (converting each value to a standard normal value). We have chosen these transformations just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this &lt;a href=&#34;https://www.tensorflow.org/guide/data_performance&#34;&gt;tensorflow guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on &lt;a href=&#34;#speedup&#34;&gt;section 4&lt;/a&gt; which is the main topic of this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#create_files&#34;&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generator&#34;&gt;Write a generator that reads data from the folder in chunks and transforms it.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Build data pipeline and train a CNN model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#speedup&#34;&gt;How to make the code run faster?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictions&#34;&gt;How to make predictions?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id = &#34;create_files&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.
* Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.
* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = np.sort(glob.glob(&amp;quot;./random_data/*&amp;quot;))
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





array([&amp;#39;./random_data/Fault_1_001.csv&amp;#39;, &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_003.csv&amp;#39;, &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_005.csv&amp;#39;, &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_007.csv&amp;#39;, &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1_009.csv&amp;#39;, &amp;#39;./random_data/Fault_1_010.csv&amp;#39;],
      dtype=&amp;#39;&amp;lt;U29&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;generator&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;These are the few things that we want our generator to have.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It should run indefinitely, i.e., it is an infinite loop.&lt;/li&gt;
&lt;li&gt;Inside generator loop, read individual files using &lt;code&gt;pandas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Do transformations on data if required.&lt;/li&gt;
&lt;li&gt;Yield the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels. The generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(&amp;quot;Tensorflow Version: &amp;quot;, tf.__version__)
import pandas as pd
import re&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensorflow Version:  2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:    # This loop makes the generator an infinite loop
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                #########################################################################################################
                # Apply transformations. Comment this portion if you don&amp;#39;t have to do any.
                # Try to use Tensorflow transformations as much as possible. First compute a spectrogram.
                temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
                # After STFT transformation with given parameters, shape = (31,33)
                temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
                ##########################################################################################################
                # temp = tf.reshape(temp, (32,32,1)) # Uncomment this line if you have not done any transformation.
                data.append(temp)
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
            data = np.asarray(data).reshape(-1,31,33,1) 
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)
(15, 31, 33, 1)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The generator works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;model&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-a-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training a CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;))[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.sort(glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;))[:10] # Showing first 10 files of Falut_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
       &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;], dtype=&amp;#39;&amp;lt;U37&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle files
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,31,33,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,31,33,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,31,33,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (31,33,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 29, 31, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 12, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  11
validation_steps =  2
steps =  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 2s 202ms/step - loss: 1.6211 - accuracy: 0.1585 - val_loss: 1.6088 - val_accuracy: 0.1400
Epoch 2/5
11/11 [==============================] - 2s 164ms/step - loss: 1.6080 - accuracy: 0.2110 - val_loss: 1.6097 - val_accuracy: 0.2200
Epoch 3/5
11/11 [==============================] - 2s 164ms/step - loss: 1.6084 - accuracy: 0.1907 - val_loss: 1.6093 - val_accuracy: 0.1200
Epoch 4/5
11/11 [==============================] - 2s 163ms/step - loss: 1.6038 - accuracy: 0.2405 - val_loss: 1.6101 - val_accuracy: 0.1800
Epoch 5/5
11/11 [==============================] - 2s 162ms/step - loss: 1.6025 - accuracy: 0.2750 - val_loss: 1.6101 - val_accuracy: 0.1400





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7f28cc0cfd60&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 0s 101ms/step - loss: 1.6099 - accuracy: 0.1900&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6099034547805786
Test accuracy: 0.1899999976158142&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;p&gt;&lt;a id = &#34;speedup&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-the-code-run-faster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make the code run faster?&lt;/h2&gt;
&lt;p&gt;If no transformations are used, just using &lt;code&gt;prefetch&lt;/code&gt; might improve performance. In deep learning usually GPUs are used for training. But all the data processing is done in CPU. In the naive approach, we will first process data in CPU, then send the processed data to GPU and after training finishes, we will prepare another batch of data. This approach is not efficient because GPU has to wait for data to get prepared. But using &lt;code&gt;prefetch&lt;/code&gt;, we prepare and keep ready batches of data while training continues. In this way, waiting time of GPU is minimized.&lt;/p&gt;
&lt;p&gt;When data transformations are used, out aim should always be to use parallel processing capabilities of &lt;code&gt;tensorflow&lt;/code&gt;. We can achieve this using &lt;code&gt;map&lt;/code&gt; function. Inside the &lt;code&gt;map&lt;/code&gt; function, all transformations are defined. Then we can &lt;code&gt;prefetch&lt;/code&gt; batches to further improve performance. The whole pipeline is as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. def transformation_function(...):
    # Define all transormations (STFT, Normalization, etc.)
    
2. def generator(...):
    
       # Read data
    
       # Call transformation_function using tf.data.Dataset.map so that it can parallelize operations.
    
       # Finally yield the processed data

3. Create tf.data.Dataset s.

4. Prefecth datasets.

5. Create model and train it.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use one extra library &lt;code&gt;tensorflow_datasets&lt;/code&gt; that will allow us to switch from &lt;code&gt;tf.dataset&lt;/code&gt; to &lt;code&gt;numpy&lt;/code&gt;. If &lt;code&gt;tensorflow_datasets&lt;/code&gt; is not installed in your system, use &lt;code&gt;pip install tensorflow-datasets&lt;/code&gt; to install it and then run following codes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow_datasets as tfds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_transformation_func(data):
  transformed_data = tf.math.abs(tf.signal.stft(data,frame_length = 64, frame_step = 32, fft_length = 64))
  transformed_data = tf.image.per_image_standardization(tf.reshape(transformed_data, shape = (-1,31,33,1))) # Normalization
  return transformed_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator_new(file_list, batch_size = 4):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size]
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) 
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)    # Read data
                data.append(tf.reshape(temp.values, shape = (1,1024)))
                pattern = tf.constant(eval(&amp;quot;file[22:29]&amp;quot;))
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()): 
                        labels.append(j)
                    
            data = np.asarray(data)
            labels = np.asarray(labels)
            first_dim = data.shape[0]
            # Create tensorflow dataset so that we can use `map` function that can do parallel computation.
            data_ds = tf.data.Dataset.from_tensor_slices(data)
            data_ds = data_ds.batch(batch_size = first_dim).map(data_transformation_func,
                                                                num_parallel_calls = tf.data.experimental.AUTOTUNE)
            # Convert the dataset to a generator and subsequently to numpy array
            data_ds = tfds.as_numpy(data_ds)   # This is where tensorflow-datasets library is used.
            data = np.asarray([data for data in data_ds]).reshape(first_dim,31,33,1)
            
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_045.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_032.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_025.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_013.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_087.csv&amp;#39;,
 &amp;#39;./random_data/Fault_5/Fault_5_053.csv&amp;#39;,
 &amp;#39;./random_data/Fault_4/Fault_4_019.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_034.csv&amp;#39;,
 &amp;#39;./random_data/Fault_2/Fault_2_044.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names[0][22:29]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;Fault_3&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 20
dataset_check = tf.data.Dataset.from_generator(tf_data_generator_new,args= [train_file_names, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,31,33,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for data, labels in dataset_check.take(7):
  print(data.shape)
  print(labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(20, 31, 33, 1)
tf.Tensor([2. 0. 0. 1. 2. 0. 4. 3. 2. 1. 1. 0. 3. 3. 2. 3. 1. 4. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([3. 1. 1. 3. 4. 4. 2. 3. 4. 3. 3. 0. 1. 2. 0. 3. 2. 2. 2. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 3. 0. 2. 2. 4. 3. 0. 4. 1. 0. 0. 2. 0. 0. 1. 0. 3. 2. 1.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([4. 2. 2. 2. 0. 3. 4. 2. 0. 1. 2. 2. 3. 4. 0. 4. 2. 0. 4. 4.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([1. 0. 4. 4. 0. 1. 0. 4. 0. 2. 1. 4. 3. 2. 1. 4. 4. 2. 4. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 2. 0. 1. 3. 2. 2. 2. 1. 3. 3. 4. 0. 1. 4. 1. 3. 2. 1. 3.], shape=(20,), dtype=float32)
(20, 31, 33, 1)
tf.Tensor([2. 1. 2. 2. 4. 4. 1. 0. 2. 2. 1. 2. 3. 0. 0. 2. 2. 0. 3. 3.], shape=(20,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 32
train_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [train_file_names, batch_size], 
                                                  output_shapes = ((None,31,33,1),(None,)),
                                                  output_types = (tf.float32, tf.float32))

validation_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [validation_file_names, batch_size],
                                                       output_shapes = ((None,31,33,1),(None,)),
                                                       output_types = (tf.float32, tf.float32))

test_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [test_file_names, batch_size],
                                                 output_shapes = ((None,31,33,1),(None,)),
                                                 output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prefetch datasets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_dataset_new = train_dataset_new.prefetch(buffer_size = tf.data.AUTOTUNE)
validation_dataset_new = validation_dataset_new.prefetch(buffer_size = tf.data.AUTOTUNE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset_new, validation_data = validation_dataset_new, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/5
11/11 [==============================] - 3s 226ms/step - loss: 1.6027 - accuracy: 0.1989 - val_loss: 1.6112 - val_accuracy: 0.1600
Epoch 2/5
11/11 [==============================] - 2s 214ms/step - loss: 1.5986 - accuracy: 0.2520 - val_loss: 1.6104 - val_accuracy: 0.2400
Epoch 3/5
11/11 [==============================] - 2s 200ms/step - loss: 1.5954 - accuracy: 0.3161 - val_loss: 1.6122 - val_accuracy: 0.1800
Epoch 4/5
11/11 [==============================] - 2s 209ms/step - loss: 1.5892 - accuracy: 0.3650 - val_loss: 1.6101 - val_accuracy: 0.1600
Epoch 5/5
11/11 [==============================] - 2s 196ms/step - loss: 1.5816 - accuracy: 0.2972 - val_loss: 1.6148 - val_accuracy: 0.1600





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7f2888147940&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss_new, test_acc_new = model.evaluate(test_dataset_new, steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4/4 [==============================] - 1s 139ms/step - loss: 1.6089 - accuracy: 0.2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id = &#34;predictions&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;In the generator used for prediction, we can also use &lt;code&gt;map&lt;/code&gt; function to parallelize data preprocessing. But in practice, inference is much faster. So we can make fast predictions using naive method also. We show the naive implementation below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)).astype(np.float32)
            temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))
            # After STFT transformation with given parameters, shape = (31,33)
            temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization
            data.append(temp) 
        data = np.asarray(data).reshape(-1,31,33,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(10, 31, 33, 1)
(5, 31, 33, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,31,33,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)


array([[0.13616945, 0.22521223, 0.29032916, 0.11108191, 0.23720728],
       [0.13136739, 0.1767847 , 0.2776762 , 0.11521462, 0.29895717],
       [0.12264969, 0.17929009, 0.2746509 , 0.11005757, 0.3133517 ],
       [0.12557542, 0.14570946, 0.20385396, 0.15842426, 0.3664368 ],
       [0.13804483, 0.13592169, 0.24367407, 0.13454145, 0.347818  ],
       [0.1430458 , 0.18991745, 0.28873193, 0.11874774, 0.25955713],
       [0.13853352, 0.17857482, 0.31380644, 0.10534842, 0.26373684],
       [0.10823276, 0.22618511, 0.32452983, 0.0847131 , 0.25633916],
       [0.1383514 , 0.16129029, 0.25447774, 0.13601685, 0.30986372],
       [0.13583152, 0.1730804 , 0.25602627, 0.12829432, 0.30676743],
       [0.12959503, 0.1772274 , 0.30786148, 0.10242429, 0.28289178],
       [0.13454609, 0.16846487, 0.2601272 , 0.12760776, 0.30925405],
       [0.14779252, 0.14842029, 0.2833778 , 0.11843931, 0.3019701 ],
       [0.11896624, 0.21513633, 0.25535005, 0.11902266, 0.29152465],
       [0.13734229, 0.13935044, 0.2748529 , 0.11947   , 0.3289844 ],
       [0.15610486, 0.20303495, 0.30530566, 0.11581586, 0.21973862],
       [0.13397609, 0.15995616, 0.28893223, 0.11423217, 0.30290335],
       [0.12130069, 0.22576565, 0.2828214 , 0.10909654, 0.26101568],
       [0.15606783, 0.14581656, 0.25918248, 0.1374906 , 0.3014426 ],
       [0.13284522, 0.15171063, 0.23527463, 0.13531938, 0.34485018],
       [0.15039593, 0.18859874, 0.2730181 , 0.13246077, 0.25552657],
       [0.13006356, 0.23040107, 0.31713945, 0.09858881, 0.2238071 ],
       [0.14617579, 0.1553615 , 0.24506982, 0.14371207, 0.30968076],
       [0.15057516, 0.18175651, 0.26442483, 0.13344486, 0.26979864],
       [0.12942658, 0.17502321, 0.27020454, 0.11938909, 0.3059566 ],
       [0.1362109 , 0.18799251, 0.2874499 , 0.11758988, 0.27075684],
       [0.12986937, 0.20687391, 0.28071418, 0.11440149, 0.26814106],
       [0.13393444, 0.1739722 , 0.27028513, 0.12331051, 0.2984978 ],
       [0.12112842, 0.13956769, 0.22920072, 0.12982692, 0.3802763 ],
       [0.11119145, 0.23633307, 0.32426968, 0.08549411, 0.2427116 ],
       [0.13327955, 0.18379854, 0.2872899 , 0.11320265, 0.28242943],
       [0.12073855, 0.20085782, 0.2646106 , 0.11651796, 0.29727504],
       [0.11189438, 0.20137395, 0.27387396, 0.10702953, 0.30582818],
       [0.18017001, 0.16150263, 0.28068233, 0.1368001 , 0.2408449 ],
       [0.10944357, 0.17276171, 0.25993338, 0.10688126, 0.35098007],
       [0.13728923, 0.1559456 , 0.25643092, 0.13189963, 0.31843463],
       [0.15782082, 0.1793215 , 0.28856605, 0.12700985, 0.24728177],
       [0.13353582, 0.20542818, 0.32362464, 0.0972899 , 0.2401215 ],
       [0.1327661 , 0.19204186, 0.29048327, 0.11032179, 0.274387  ],
       [0.15101205, 0.16577183, 0.28014943, 0.12510163, 0.27796513],
       [0.10811005, 0.24937892, 0.2825413 , 0.09674085, 0.26322895],
       [0.12502007, 0.17934126, 0.23135227, 0.1389524 , 0.32533407],
       [0.15938489, 0.12479166, 0.2140554 , 0.16871263, 0.33305547],
       [0.13133633, 0.15853986, 0.2776162 , 0.11680949, 0.31569818],
       [0.13070984, 0.20629251, 0.32593974, 0.09547149, 0.24158639],
       [0.12578759, 0.13497958, 0.2329479 , 0.13350599, 0.37277895],
       [0.11535928, 0.18584532, 0.25516343, 0.11577264, 0.3278593 ],
       [0.12250951, 0.16624808, 0.22112629, 0.14213458, 0.34798154],
       [0.11505162, 0.22170952, 0.29335403, 0.09799453, 0.27189022],
       [0.15128227, 0.18352163, 0.26057395, 0.1377367 , 0.2668855 ],
       [0.10571367, 0.14169416, 0.2291365 , 0.12079947, 0.40265626],
       [0.11637849, 0.2011716 , 0.28354362, 0.10431051, 0.29459572],
       [0.12311503, 0.1520483 , 0.26735488, 0.11320169, 0.34428006],
       [0.13812302, 0.23593263, 0.27522495, 0.12361279, 0.22710668],
       [0.13761182, 0.17356406, 0.23145248, 0.14515112, 0.3122205 ]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 4, 2, 2,
       4, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4,
       2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a final comment, read the &lt;strong&gt;note&lt;/strong&gt; at the beginning of this post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading multiple files in Tensorflow 2</title>
      <link>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/</guid>
      <description>
&lt;script src=&#34;https://biswajitsahoo1111.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;center&#34;&gt;
&lt;td&gt;
&lt;a href=&#34;https://colab.research.google.com/github/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/colab_logo_32px.png&#34; /&gt;
Run in Google Colab&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb&#34;&gt;
&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;
View source on GitHub&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.dropbox.com/s/o4aevvuqr39kq20/Reading_multiple_files_in_Tensorflow_2.ipynb?dl=1&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download notebook&lt;/a&gt;
&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;In this post, we will read multiple &lt;code&gt;.csv&lt;/code&gt; files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 &lt;code&gt;.csv&lt;/code&gt; files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of &lt;code&gt;.csv&lt;/code&gt; files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.&lt;/p&gt;
&lt;p&gt;This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and &lt;code&gt;.csv&lt;/code&gt; files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;Tensorflow 2&lt;/code&gt; to run our deep learning model. &lt;code&gt;Tensorflow&lt;/code&gt; is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create 500 &lt;code&gt;&#34;.csv&#34;&lt;/code&gt; files and save it in the folder “random_data” in current directory.&lt;/li&gt;
&lt;li&gt;Write a generator that reads data from the folder in chunks and preprocesses it.&lt;/li&gt;
&lt;li&gt;Feed the chunks of data to a CNN model and train it for several epochs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;create-500-.csv-files-of-random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Create 500 &lt;code&gt;.csv&lt;/code&gt; files of random data&lt;/h2&gt;
&lt;p&gt;As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each &lt;code&gt;.csv&lt;/code&gt; file will have one column of data with 1024 entries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”
category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import os
import glob
np.random.seed(1111)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First create a function that will generate random files.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_random_csv_files(fault_classes, number_of_files_in_each_class):
    os.mkdir(&amp;quot;./random_data/&amp;quot;)  # Make a directory to save created files.
    for fault_class in fault_classes:
        for i in range(number_of_files_in_each_class):
            data = np.random.rand(1024,)
            file_name = &amp;quot;./random_data/&amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot;_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
            np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
        print(str(eval(&amp;quot;number_of_files_in_each_class&amp;quot;)) + &amp;quot; &amp;quot; + eval(&amp;quot;fault_class&amp;quot;) + &amp;quot; files&amp;quot;  + &amp;quot; created.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now use the function to create 100 files each for five fault types.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_random_csv_files([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;], number_of_files_in_each_class = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 Fault_1 files created.
100 Fault_2 files created.
100 Fault_3 files created.
100 Fault_4 files created.
100 Fault_5 files created.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;files = glob.glob(&amp;quot;./random_data/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  500
Showing first 10 files...





[&amp;#39;./random_data/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract labels from file name, extract the part of the file name that corresponds to fault type.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;./random_data/Fault_1_001.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(files[0][14:21])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fault_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-generator-that-reads-data-in-chunks-and-preprocesses-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Write a generator that reads data in chunks and preprocesses it&lt;/h2&gt;
&lt;p&gt;Generator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. &lt;code&gt;yield&lt;/code&gt; keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For our application, we will use a generator that runs indefinitely.&lt;/p&gt;
&lt;p&gt;The following generator takes a list of file names as first argument. The second argument is &lt;code&gt;batch_size&lt;/code&gt;. &lt;code&gt;batch_size&lt;/code&gt; determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there is no need for generators. In case our data size is huge, we can process chunks of it.&lt;/p&gt;
&lt;p&gt;As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Fault_4&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Fault_5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The generator will &lt;code&gt;yield&lt;/code&gt; both data and labels.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import re            # To match regular expression for extracting labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  # This loop is used to run the generator indefinitely.
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;)) # Change this line to read any other type of file
                data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel
                pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)      # Pattern extracted from file_name
                for j in range(len(label_classes)):
                    if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes
                        labels.append(j)  
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read any other file format, inside the generator change the line that reads files. This will enable us to read different file formats, be it &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.npz&lt;/code&gt; or any other. Preprocessing of data, different from what we have done in this blog, can be done within the generator loop.&lt;/p&gt;
&lt;p&gt;Now we will check whether the generator works as intended or not. We will set &lt;code&gt;batch_size&lt;/code&gt; to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use &lt;code&gt;np.random.shuffle(file_list)&lt;/code&gt; to shuffle files.&lt;/p&gt;
&lt;p&gt;In the demonstration, we will read files from an ordered list. This will help us check any errors in the code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;generated_data = data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in generated_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)  # Just to see the labels
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.&lt;/p&gt;
&lt;p&gt;Now we will create a &lt;code&gt;tensorflow dataset&lt;/code&gt; using the generator. &lt;code&gt;Tensorflow&lt;/code&gt; datasets can conveniently be used to train &lt;code&gt;tensorflow&lt;/code&gt; models.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;tensorflow dataset&lt;/code&gt; can be created form numpy arrays or from generators.Here, we will create it using a generator. Use of the previously created generator as it is in &lt;code&gt;tensorflow datasets&lt;/code&gt; doesn’t work (Readers can verify this). This happens because of the inability of regular expression to compare a “string” with a “byte string”. “byte strings” are generated by default in tensorflow. As a way around, we will make modifications to the earlier generator and use it with tensorflow datasets. Note that we will only modified three lines. Modified lines are accompanied by commented texts beside it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
print(tf.__version__)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def tf_data_generator(file_list, batch_size = 20):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(file_list):  
            i = 0
            np.random.shuffle(file_list)
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
            data = []
            labels = []
            label_classes = tf.constant([&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]) # This line has changed.
            for file in file_chunk:
                temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
                data.append(temp.values.reshape(32,32,1)) 
                pattern = tf.constant(eval(&amp;quot;file[14:21]&amp;quot;))  # This line has changed
                for j in range(len(label_classes)):
                    if re.match(pattern.numpy(), label_classes[j].numpy()):  # This line has changed.
                        labels.append(j)
            data = np.asarray(data).reshape(-1,32,32,1)
            labels = np.asarray(labels)
            yield data, labels
            i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test whether modified generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;check_data = tf_data_generator(files, batch_size = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in check_data:
    print(data.shape, labels.shape)
    print(labels, &amp;quot;&amp;lt;--Labels&amp;quot;)
    print()
    num = num + 1
    if num &amp;gt; 5: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels

(10, 32, 32, 1) (10,)
[0 0 0 0 0 0 0 0 0 0] &amp;lt;--Labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the new generator created by using a few &lt;code&gt;tensorflow&lt;/code&gt; commands works just fine as our previous generator. This new generator can now be integrated with a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 15
dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),
                                                output_shapes = ((None,32,32,1),(None,)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether &lt;code&gt;dataset&lt;/code&gt; works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;num = 0
for data, labels in dataset:
    print(data.shape, labels.shape)
    print(labels)
    print()
    num = num + 1
    if num &amp;gt; 7: break&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)

(15, 32, 32, 1) (15,)
tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the &lt;code&gt;tf_data_generator&lt;/code&gt; create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-data-pipeline-and-training-cnn-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Building data pipeline and training CNN model&lt;/h2&gt;
&lt;p&gt;Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shutil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create five different folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_folders = [&amp;quot;Fault_1&amp;quot;, &amp;quot;Fault_2&amp;quot;, &amp;quot;Fault_3&amp;quot;, &amp;quot;Fault_4&amp;quot;, &amp;quot;Fault_5&amp;quot;]
for folder_name in fault_folders:
    os.mkdir(os.path.join(&amp;quot;./random_data&amp;quot;, folder_name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Move files into those folders.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for file in files:
    pattern = &amp;quot;^&amp;quot; + eval(&amp;quot;file[14:21]&amp;quot;)
    for j in range(len(fault_folders)):
        if re.match(pattern, fault_folders[j]):
            dest = os.path.join(&amp;quot;./random_data/&amp;quot;,eval(&amp;quot;fault_folders[j]&amp;quot;))
            shutil.move(file, dest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1&amp;#39;,
 &amp;#39;./random_data/Fault_2&amp;#39;,
 &amp;#39;./random_data/Fault_3&amp;#39;,
 &amp;#39;./random_data/Fault_4&amp;#39;,
 &amp;#39;./random_data/Fault_5&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)[:10] # Showing first 10 files of Fault_1 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_1/Fault_1_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_1/Fault_1_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)[:10] # Showing first 10 files of Fault_3 folder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;./random_data/Fault_3/Fault_3_001.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_002.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_003.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_004.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_005.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_006.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_007.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_008.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_009.csv&amp;#39;,
 &amp;#39;./random_data/Fault_3/Fault_3_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_files = glob.glob(&amp;quot;./random_data/Fault_1/*&amp;quot;)
fault_2_files = glob.glob(&amp;quot;./random_data/Fault_2/*&amp;quot;)
fault_3_files = glob.glob(&amp;quot;./random_data/Fault_3/*&amp;quot;)
fault_4_files = glob.glob(&amp;quot;./random_data/Fault_4/*&amp;quot;)
fault_5_files = glob.glob(&amp;quot;./random_data/Fault_5/*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)
fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)
fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)
fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)
fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)
fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)
fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)
fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)
fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train
validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val
test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test

# Shuffle data (We don&amp;#39;t need to shuffle validation and test data)
np.random.shuffle(train_file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Number of train_files:&amp;quot; ,len(train_file_names))
print(&amp;quot;Number of validation_files:&amp;quot; ,len(validation_file_names))
print(&amp;quot;Number of test_files:&amp;quot; ,len(test_file_names))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of train_files: 350
Number of validation_files: 50
Number of test_files: 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], 
                                              output_shapes = ((None,32,32,1),(None,)),
                                              output_types = (tf.float32, tf.float32))

validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],
                                                   output_shapes = ((None,32,32,1),(None,)),
                                                   output_types = (tf.float32, tf.float32))

test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],
                                             output_shapes = ((None,32,32,1),(None,)),
                                             output_types = (tf.float32, tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now create the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from tensorflow.keras import layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = tf.keras.Sequential([
    layers.Conv2D(16, 3, activation = &amp;quot;relu&amp;quot;, input_shape = (32,32,1)),
    layers.MaxPool2D(2),
    layers.Conv2D(32, 3, activation = &amp;quot;relu&amp;quot;),
    layers.MaxPool2D(2),
    layers.Flatten(),
    layers.Dense(16, activation = &amp;quot;relu&amp;quot;),
    layers.Dense(5, activation = &amp;quot;softmax&amp;quot;)
])
model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense (Dense)                (None, 16)                18448     
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 85        
=================================================================
Total params: 23,333
Trainable params: 23,333
Non-trainable params: 0
_________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile the model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.compile(loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments &lt;code&gt;steps_per_epoch&lt;/code&gt; and &lt;code&gt;validation_steps&lt;/code&gt; to desired numbers in &lt;code&gt;model.fit()&lt;/code&gt;. Similarly while evaluating model, we need to set the argument &lt;code&gt;steps&lt;/code&gt; to a desired number in &lt;code&gt;model.evaluate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set &lt;code&gt;steps_per_epoch&lt;/code&gt; to 35. Similarly, &lt;code&gt;validation_steps = 5&lt;/code&gt; and in &lt;code&gt;model.evaluate()&lt;/code&gt;, &lt;code&gt;steps = 10&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))
validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))
steps = np.int(np.ceil(len(test_file_names)/batch_size))
print(&amp;quot;steps_per_epoch = &amp;quot;, steps_per_epoch)
print(&amp;quot;validation_steps = &amp;quot;, validation_steps)
print(&amp;quot;steps = &amp;quot;, steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;steps_per_epoch =  35
validation_steps =  5
steps =  10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,
         validation_steps = validation_steps, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
35/35 [==============================] - 1s 40ms/step - loss: 1.6268 - accuracy: 0.2029 - val_loss: 1.6111 - val_accuracy: 0.2000
Epoch 2/10
35/35 [==============================] - 1s 36ms/step - loss: 1.6101 - accuracy: 0.2114 - val_loss: 1.6079 - val_accuracy: 0.2600
Epoch 3/10
35/35 [==============================] - 1s 35ms/step - loss: 1.6066 - accuracy: 0.2343 - val_loss: 1.6076 - val_accuracy: 0.2000
Epoch 4/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5993 - accuracy: 0.2143 - val_loss: 1.6085 - val_accuracy: 0.2400
Epoch 5/10
35/35 [==============================] - 1s 34ms/step - loss: 1.5861 - accuracy: 0.2657 - val_loss: 1.6243 - val_accuracy: 0.2000
Epoch 6/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5620 - accuracy: 0.3514 - val_loss: 1.6363 - val_accuracy: 0.2000
Epoch 7/10
35/35 [==============================] - 1s 36ms/step - loss: 1.5370 - accuracy: 0.2857 - val_loss: 1.6171 - val_accuracy: 0.2600
Epoch 8/10
35/35 [==============================] - 1s 35ms/step - loss: 1.5015 - accuracy: 0.4057 - val_loss: 1.6577 - val_accuracy: 0.2000
Epoch 9/10
35/35 [==============================] - 1s 35ms/step - loss: 1.4415 - accuracy: 0.5086 - val_loss: 1.6484 - val_accuracy: 0.1400
Epoch 10/10
35/35 [==============================] - 1s 36ms/step - loss: 1.3363 - accuracy: 0.6143 - val_loss: 1.6672 - val_accuracy: 0.2200





&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fcab40f6150&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;test_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10/10 [==============================] - 0s 25ms/step - loss: 1.6974 - accuracy: 0.1500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Test loss: &amp;quot;, test_loss)
print(&amp;quot;Test accuracy:&amp;quot;, test_accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss:  1.6973648071289062
Test accuracy: 0.15000000596046448&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, model performs terribly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make predictions?&lt;/h2&gt;
&lt;p&gt;Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But often times, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.&lt;/p&gt;
&lt;p&gt;If the whole of our prediction set fits into memory, we can just call &lt;code&gt;model.predict()&lt;/code&gt; on this data and then use &lt;code&gt;np.argmax()&lt;/code&gt; to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.&lt;/p&gt;
&lt;p&gt;Yet another pedantic way of doing this is to write a generator to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def create_prediction_set(num_files = 20):
    os.mkdir(&amp;quot;./random_data/prediction_set&amp;quot;)
    for i in range(num_files):
        data = np.random.randn(1024,)
        file_name = &amp;quot;./random_data/prediction_set/&amp;quot;  + &amp;quot;file_&amp;quot; + &amp;quot;{0:03}&amp;quot;.format(i+1) + &amp;quot;.csv&amp;quot; # This creates file_name
        np.savetxt(eval(&amp;quot;file_name&amp;quot;), data, delimiter = &amp;quot;,&amp;quot;, header = &amp;quot;V1&amp;quot;, comments = &amp;quot;&amp;quot;)
    print(str(eval(&amp;quot;num_files&amp;quot;)) + &amp;quot; &amp;quot;+ &amp;quot; files created in prediction set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create some files for prediction set.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;create_prediction_set(num_files = 55)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;55  files created in prediction set.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;prediction_files = glob.glob(&amp;quot;./random_data/prediction_set/*&amp;quot;)
print(&amp;quot;Total number of files: &amp;quot;, len(prediction_files))
print(&amp;quot;Showing first 10 files...&amp;quot;)
prediction_files[:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of files:  55
Showing first 10 files...





[&amp;#39;./random_data/prediction_set/file_001.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_002.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_003.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_004.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_005.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_006.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_007.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_008.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_009.csv&amp;#39;,
 &amp;#39;./random_data/prediction_set/file_010.csv&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only &lt;code&gt;yield&lt;/code&gt; data. This is how we achieve that.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def generator_for_prediction(file_list, batch_size = 20):
    i = 0
    while i &amp;lt;= (len(file_list)/batch_size):
        if i == np.floor(len(file_list)/batch_size):
            file_chunk = file_list[i*batch_size:len(file_list)]
            if len(file_chunk)==0:
                break
        else:
            file_chunk = file_list[i*batch_size:(i+1)*batch_size] 
        data = []
        for file in file_chunk:
            temp = pd.read_csv(open(file,&amp;#39;r&amp;#39;))
            data.append(temp.values.reshape(32,32,1)) 
        data = np.asarray(data).reshape(-1,32,32,1)
        yield data
        i = i + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check whether the generator works or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)
for data in pred_gen:
    print(data.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(10, 32, 32, 1)
(5, 32, 32, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a &lt;code&gt;tensorflow dataset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;batch_size = 10
prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],
                                                 output_shapes=(None,32,32,1), output_types=(tf.float32))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;steps = np.int(np.ceil(len(prediction_files)/batch_size))
predictions = model.predict(prediction_dataset,steps = steps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Shape of prediction array: &amp;quot;, predictions.shape)
predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shape of prediction array:  (55, 5)

array([[0.28138927, 0.3383776 , 0.17806269, 0.18918239, 0.01298801],
       [0.16730548, 0.20139892, 0.32996896, 0.16305783, 0.13826886],
       [0.08079846, 0.35669118, 0.4091237 , 0.13286887, 0.02051783],
       [0.01697877, 0.79075295, 0.17063092, 0.01676028, 0.00487713],
       [0.19006915, 0.02615157, 0.39364284, 0.09650648, 0.29362988],
       [0.05416911, 0.682985  , 0.19086388, 0.0668761 , 0.00510592],
       [0.21325852, 0.27782622, 0.10314588, 0.39539766, 0.01037181],
       [0.23633875, 0.3308002 , 0.30727112, 0.09573858, 0.02985144],
       [0.06442448, 0.34153524, 0.47356713, 0.08497778, 0.03549532],
       [0.37901744, 0.32311487, 0.12875995, 0.16359715, 0.00551067],
       [0.12227482, 0.49774405, 0.26021793, 0.1060346 , 0.01372868],
       [0.07139122, 0.17324339, 0.5490784 , 0.10136751, 0.10491937],
       [0.18757634, 0.2833261 , 0.3367256 , 0.14390293, 0.04846917],
       [0.23564269, 0.2800771 , 0.19150141, 0.2686058 , 0.02417296],
       [0.4835618 , 0.03908279, 0.09785527, 0.31918615, 0.06031401],
       [0.03285189, 0.5866938 , 0.3362034 , 0.0313101 , 0.01294078],
       [0.31367007, 0.05583594, 0.24806198, 0.2707511 , 0.1116809 ],
       [0.11204866, 0.05982558, 0.44611645, 0.16678827, 0.21522103],
       [0.04504926, 0.7100154 , 0.16532828, 0.0747861 , 0.00482096],
       [0.22441828, 0.01738338, 0.36729604, 0.0961706 , 0.29473177],
       [0.22392808, 0.23958267, 0.11669649, 0.41423568, 0.00555711],
       [0.11768451, 0.16422512, 0.49695587, 0.13158153, 0.08955302],
       [0.04941175, 0.31670955, 0.46190843, 0.12606393, 0.04590632],
       [0.19507076, 0.03239974, 0.3885634 , 0.14447391, 0.23949222],
       [0.3530666 , 0.08613478, 0.11636773, 0.4088019 , 0.03562902],
       [0.12874755, 0.3140329 , 0.3858064 , 0.1278494 , 0.0435637 ],
       [0.3001929 , 0.02791574, 0.11502622, 0.5044482 , 0.05241694],
       [0.0929171 , 0.1467541 , 0.6005069 , 0.06660035, 0.09322156],
       [0.10712272, 0.5518521 , 0.2632791 , 0.06340495, 0.01434106],
       [0.27723876, 0.25847596, 0.18952209, 0.25228631, 0.02247689],
       [0.12578863, 0.44461673, 0.25048074, 0.14304985, 0.03606399],
       [0.09593316, 0.06914104, 0.49921316, 0.1389045 , 0.19680816],
       [0.22185169, 0.0878747 , 0.33703303, 0.23808932, 0.11515129],
       [0.0850782 , 0.06328611, 0.57307494, 0.08615369, 0.19240707],
       [0.41479778, 0.07033634, 0.22154689, 0.2007963 , 0.09252268],
       [0.22052608, 0.10761442, 0.33570328, 0.25846007, 0.07769614],
       [0.03679338, 0.4369671 , 0.42453632, 0.07080499, 0.03089818],
       [0.17414902, 0.3666445 , 0.26953018, 0.16861232, 0.02106389],
       [0.04334973, 0.04427214, 0.5819794 , 0.02825493, 0.30214384],
       [0.23099631, 0.31964707, 0.31392127, 0.11803907, 0.01739628],
       [0.03072637, 0.6739159 , 0.25826213, 0.0309101 , 0.00618558],
       [0.20030826, 0.05058228, 0.42536664, 0.14415787, 0.17958501],
       [0.25894472, 0.0410106 , 0.25135538, 0.15487678, 0.29381245],
       [0.31544876, 0.05200702, 0.20838396, 0.31984535, 0.10431487],
       [0.10788545, 0.31769663, 0.44471365, 0.08522549, 0.04447879],
       [0.01864015, 0.35556656, 0.551683  , 0.02805553, 0.04605483],
       [0.20043266, 0.1211144 , 0.26670808, 0.33885604, 0.07288874],
       [0.29432756, 0.19128233, 0.19503927, 0.2826192 , 0.03673161],
       [0.2151616 , 0.05391361, 0.34218988, 0.11304423, 0.27569073],
       [0.241943  , 0.05663572, 0.23858468, 0.36390153, 0.09893499],
       [0.24665013, 0.22702417, 0.33673155, 0.11996701, 0.06962712],
       [0.05448309, 0.33466634, 0.49283266, 0.07876839, 0.03924957],
       [0.3060696 , 0.03565398, 0.33453086, 0.12989788, 0.19384763],
       [0.1417291 , 0.40642622, 0.20021752, 0.22896914, 0.02265806],
       [0.10395318, 0.20624556, 0.46823606, 0.12000521, 0.10156006]],
      dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use &lt;code&gt;np.argmax()&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.argmax(predictions, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([1, 2, 2, 1, 2, 1, 3, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 3, 2,
       2, 2, 3, 2, 3, 2, 1, 0, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 4, 3,
       2, 2, 3, 0, 2, 3, 2, 2, 2, 1, 2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are randomly generated. So we should not be surprised by this result. Also note that the for each new data, softmax outputs are close to each other. This means that the network is not very sure about the classification result.&lt;/p&gt;
&lt;p&gt;This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a generator, and trained a model using that generator. The above code can be tweaked slightly to read any type of files other than &lt;code&gt;.csv&lt;/code&gt;. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.&lt;/p&gt;
&lt;p&gt;As a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in &lt;code&gt;Tensorflow&lt;/code&gt;, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.&lt;/p&gt;
&lt;p&gt;I hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 1&lt;/strong&gt;: While generators are convenient for handling chunks of data from a large dataset, they have limited portability and scalability. Therefore, in Tensorflow Sequences are preferred instead of generators. &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/&#34;&gt;See this blog&lt;/a&gt; for a complete workflow for reading multiple files using Sequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 2&lt;/strong&gt;: If along with reading, one has to perform complex transformations on extracted data (say, doing spectrogram on each segment of data, etc.), the naive approach presented in this blog may turn out to be slow. But there are ways to make these computations faster. One such speedup technique can be found at &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last modified: 27th April, 2020.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Python Generators</title>
      <link>https://biswajitsahoo1111.github.io/post/using-python-generators/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/post/using-python-generators/</guid>
      <description>


&lt;center&gt;
(&lt;a href=&#34;https://gist.github.com/biswajitsahoo1111/33cea59f24de6c19d1a513b42d28674d&#34;&gt;Jupyter notebook for this post can be found here&lt;/a&gt;)
&lt;/center&gt;
&lt;center&gt;
(&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;Check out this post for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;&lt;/a&gt;)
&lt;/center&gt;
&lt;p&gt;In this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful to the beginner.&lt;/p&gt;
&lt;p&gt;While functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It’s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.&lt;/p&gt;
&lt;p&gt;First let’s create artificial data that we will extract later batch by batch.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
data = np.random.randint(100,150, size = (10,2,2))
labels = np.random.permutation(10)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[102 146]
  [141 125]]

 [[120 128]
  [114 119]]

 [[143 110]
  [132 148]]

 [[133 105]
  [126 140]]

 [[116 108]
  [125 103]]

 [[121 125]
  [102 107]]

 [[146 126]
  [130 138]]

 [[136 145]
  [103 135]]

 [[148 100]
  [128 106]]

 [[144 118]
  [149 143]]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;labels:&amp;quot;, labels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;labels: [6 2 0 9 5 8 3 7 4 1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def my_gen(data, labels, batch_size = 2):
    i = 0
    while True:
        if i*batch_size &amp;gt;= len(labels):
            i = 0
            idx = np.random.permutation(len(labels))
            data, labels = data[idx], labels[idx]
            continue
        else:
            X = data[i*batch_size:(i+1)*batch_size,:]
            y = labels[i*batch_size:(i+1)*batch_size]
            i += 1
            yield X,y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found &lt;a href=&#34;https://stackoverflow.com/questions/42727412/efficient-way-to-partially-read-large-numpy-file&#34;&gt;here&lt;/a&gt;. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using “np.shuffle()”.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_data = my_gen(data,labels)
for i in range(10):
    X,y = next(get_data)
    print(X,y)
    print(X.shape, y.shape)
    print(&amp;quot;=========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[102 146]
  [141 125]]

 [[120 128]
  [114 119]]] [6 2]
(2, 2, 2) (2,)
=========================
[[[143 110]
  [132 148]]

 [[133 105]
  [126 140]]] [0 9]
(2, 2, 2) (2,)
=========================
[[[116 108]
  [125 103]]

 [[121 125]
  [102 107]]] [5 8]
(2, 2, 2) (2,)
=========================
[[[146 126]
  [130 138]]

 [[136 145]
  [103 135]]] [3 7]
(2, 2, 2) (2,)
=========================
[[[148 100]
  [128 106]]

 [[144 118]
  [149 143]]] [4 1]
(2, 2, 2) (2,)
=========================
[[[120 128]
  [114 119]]

 [[133 105]
  [126 140]]] [2 9]
(2, 2, 2) (2,)
=========================
[[[116 108]
  [125 103]]

 [[146 126]
  [130 138]]] [5 3]
(2, 2, 2) (2,)
=========================
[[[136 145]
  [103 135]]

 [[148 100]
  [128 106]]] [7 4]
(2, 2, 2) (2,)
=========================
[[[143 110]
  [132 148]]

 [[121 125]
  [102 107]]] [0 8]
(2, 2, 2) (2,)
=========================
[[[102 146]
  [141 125]]

 [[144 118]
  [149 143]]] [6 1]
(2, 2, 2) (2,)
=========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
import math

class my_new_gen(tf.keras.utils.Sequence):
    def __init__(self, data, labels, batch_size= 2 ):
        self.x, self.y = data, labels
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])

    def __len__(self):
        return math.floor(self.x.shape[0] / self.batch_size)

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x = self.x[inds]
        batch_y = self.y[inds]
        return batch_x, batch_y
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we must add &lt;code&gt;len&lt;/code&gt; method and &lt;code&gt;getitem&lt;/code&gt; method within the class and if we want to shuffle data between epochs, we have to add &lt;code&gt;on_epoch_end()&lt;/code&gt; method. &lt;code&gt;len&lt;/code&gt; finds out the number of batches possible in an epoch and &lt;code&gt;getitem&lt;/code&gt; extracts batches one by one. When one epoch is complete, &lt;code&gt;on_epoch_end()&lt;/code&gt; shuffles the data and the process continues. We will test it with an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;get_new_data = my_new_gen(data, labels)

for i in range(10):
    if i == 5:
        get_new_data.on_epoch_end()
        i = 0
    elif i &amp;gt; 5:
        i = i-5
    dat,labs = get_new_data.__getitem__(i)
    print(dat,labs)
    print(dat.shape, labs.shape)
    print(&amp;quot;===========================&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[[102 146]
  [141 125]]

 [[120 128]
  [114 119]]] [6 2]
(2, 2, 2) (2,)
===========================
[[[143 110]
  [132 148]]

 [[133 105]
  [126 140]]] [0 9]
(2, 2, 2) (2,)
===========================
[[[116 108]
  [125 103]]

 [[121 125]
  [102 107]]] [5 8]
(2, 2, 2) (2,)
===========================
[[[146 126]
  [130 138]]

 [[136 145]
  [103 135]]] [3 7]
(2, 2, 2) (2,)
===========================
[[[148 100]
  [128 106]]

 [[144 118]
  [149 143]]] [4 1]
(2, 2, 2) (2,)
===========================
[[[143 110]
  [132 148]]

 [[136 145]
  [103 135]]] [0 7]
(2, 2, 2) (2,)
===========================
[[[102 146]
  [141 125]]

 [[148 100]
  [128 106]]] [6 4]
(2, 2, 2) (2,)
===========================
[[[133 105]
  [126 140]]

 [[144 118]
  [149 143]]] [9 1]
(2, 2, 2) (2,)
===========================
[[[146 126]
  [130 138]]

 [[116 108]
  [125 103]]] [3 5]
(2, 2, 2) (2,)
===========================
[[[120 128]
  [114 119]]

 [[121 125]
  [102 107]]] [2 8]
(2, 2, 2) (2,)
===========================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have also used generators to train MNIST example. The code can be found &lt;a href=&#34;https://gist.github.com/biswajitsahoo1111/33cea59f24de6c19d1a513b42d28674d&#34;&gt;here&lt;/a&gt;. The example might seem bit stretched as we don’t need generators for small data sets like MNIST. The aim of the example is just to show different implementation using generators.&lt;/p&gt;
&lt;p&gt;Perhaps the most detailed blog about using generators for deep learning is &lt;a href=&#34;https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&#34;&gt;this one&lt;/a&gt;. I also found &lt;a href=&#34;https://github.com/keras-team/keras/issues/9707#issuecomment-374609666&#34;&gt;these comments&lt;/a&gt; helpful.&lt;/p&gt;
&lt;p&gt;Update: With the release of &lt;code&gt;tensorflow-2.0&lt;/code&gt;, it is much easier to use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API for handling large datasets. Generators can still be used for training using &lt;code&gt;tf.keras&lt;/code&gt;. As a final note, use generators if it is absolutely essential to do so. Otherwise, use &lt;code&gt;tf.data.Dataset&lt;/code&gt; API. Check out &lt;a href=&#34;https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/&#34;&gt;this post&lt;/a&gt; for an end-to-end data pipeline and training using generators in &lt;code&gt;Tensorflow 2&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data-Driven Remaining Useful Life (RUL) Prediction</title>
      <link>https://biswajitsahoo1111.github.io/project/rul_codes_open/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/project/rul_codes_open/</guid>
      <description>

&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;left&#34;&gt;
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=rul_codes_open&amp;type=star&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;

  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  
  &lt;!----
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
  &lt;/td&gt;
  ----&gt;
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/rul_codes_open/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download all code (.zip)&lt;/a&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;right&#34;&gt;  
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=rul_codes_open&amp;type=fork&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Remaining useful life (RUL) prediction is the study of predicting when something is going to fail, given its present state. The problem has a prophetic charm associated with it. While a soothsayer can make a prediction about almost anything (including RUL of a machine) confidently, many people will not accept the prediction because of its lack of scientific basis. Here, we will try to solve the problem with scientific reasoning.&lt;/p&gt;

&lt;p&gt;A component (or a machine) is said to have failed when it can no longer perform its desired task to the satisfaction of the user. For example, Li-Ion battery of an electric vehicle is said to have failed when it requires frequent recharging to travel a small distance. Similarly, a bearing of a machine is said to have failed, if level of vibration produced at the bearing goes above some acceptable limit. Other examples can be thought of for different applications. The goal then is to predict beforehand when something is going to fail. Knowledge of a component&amp;rsquo;s expected time of failure will help us prepare well for the inevitable. In industrial setting, where any unplanned shutdown of a critical component has huge monetary cost, knowing when a machine is going to fail will result in significant monetary gains.&lt;/p&gt;

&lt;p&gt;There are many techniques developed over the years to predict RUL of a component. All those techniques can be broadly divided into two categories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model Based Methods&lt;/li&gt;
&lt;li&gt;Data-Driven Methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In model based methods, we try to formulate a mathematical model of the system under consideration. Then using that model we try to predict RUL of the component. Though model based methods are used in some cases, there are many other applications where formulating a full mathematical model of the system is extremely difficult. In some cases, the underlying physics is so complex that we have to make many simplifying assumptions. Whether the simplifying assumptions are justified or not is determined by collecting real data from the machine. Therefore, it requires extensive domain knowledge and thus is a territory of only a select few who can actually do these things.&lt;/p&gt;

&lt;p&gt;In contrast, in data-driven methods all information about a machine is gained from the data collected from it. With readily available sensors we can collect huge amounts of data for almost any application. By analyzing that data we can get an idea about the condition of the machine. That will help us in making an informed decision about the RUL of the machine. In this process we make no assumptions about the machine. Increasingly, data-driven methods are getting better at making reliable predictions. As the name of the project suggests, we will only focus on data-driven methods for RUL prediction. The problem of RUL prediction is also know as prognosis in some fields. Some people also call it prognostics. We will only use the term RUL prediction. In the beginning, we will mainly focus on predicting RUL of mechanical components. Later we will explore other application areas.&lt;/p&gt;

&lt;h3 id=&#34;aim-of-the-project&#34;&gt;Aim of the project&lt;/h3&gt;

&lt;p&gt;Like my previous project on &lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;fault diagnosis&lt;/a&gt;, aim of this project is to produce reproducible results for RUL prediction. RUL prediction is a broad subject that can be applied to many problems such as RUL prediction of Li-Ion batteries, RUL prediction of machinery bearings, RUL prediction of machine tool, etc. We will start with mechanical applications and then gradually move to other applications over time. As our aim is reproducibility, we will use publicly available datasets. Interested readers can download the data and use our code to get exact results as we have obtained. As we will use well known datasets, readers might observe that, in some cases, our results are in fact worse than some reported results elsewhere. Our goal is not to verify someone else&amp;rsquo;s claim. If someone else claims a better result, onus is on them to demonstrate their result. Here, whatever results I have claimed can be reproduced by readers by just running the jupyter notebooks after downloading relevant data.&lt;/p&gt;

&lt;p&gt;This is an ongoing project and modifications and additions of new techniques will be done over time. &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; are two popular programming languages that are used in machine learning applications. We will use &lt;strong&gt;Python&lt;/strong&gt; to demonstrate our results. At a later stage we might add equivalent &lt;strong&gt;R&lt;/strong&gt; code. To implement deep learning models, we will use &lt;strong&gt;Tensorflow&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;results-using-nasa-s-turbofan-engine-degradation-dataset-https-ti-arc-nasa-gov-tech-dash-groups-pcoe-prognostic-data-repository-turbofan&#34;&gt;Results using &lt;a href=&#34;https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan&#34;&gt;NASA&amp;rsquo;s Turbofan Engine Degradation Dataset&lt;/a&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;We will first apply classical machine learning methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;Dataset description and preprocessing&lt;/a&gt; steps can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;this link&lt;/a&gt;. We will use the same preprocessing steps, with minor changes, in all notebooks. We strongly encourage readers to first go over &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;data preparation notebook&lt;/a&gt; before using results notebooks. In the table below, we report Root Mean Square Error (RMSE) values. &lt;span style=&#34;color:blue&#34;&gt;Click on the numbers in the table to view corresponding notebooks&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note on last column of following table&lt;/strong&gt;: The last column specifies the degradation model used in the notebooks. There are two common degradation models that are used for this particular turbofan dataset: Linear degradation model and Piecewise linear degradation model. For more details about both, see &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_data_description_and_preprocessing.ipynb&#34;&gt;this&lt;/a&gt;. When we use piecewise linear degradation model, we have to assume an early RUL value. This is nothing but the value of RUL that is assumed when the component is relatively new. In literature, different people use different early RUL values. In our examples, when we specify an early RUL value, that means that we apply the same early RUL across all 4 datasets.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD001&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD002&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD003&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD004&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Degradation Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Gradient Boosting&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;19.06&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;28.97&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;20.55&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_xgboost_piecewise_linear_degradation_model.ipynb&#34;&gt;29.49&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Random Forest&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;19.15&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;29.00&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;20.53&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_random_forest_piecewise_linear_degradation_model.ipynb&#34;&gt;29.75&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Gradient Boosting&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_xgboost_linear_degradation_model.ipynb&#34;&gt;33.24&lt;/a&gt;&lt;sup&gt;* &lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD002_xgboost_linear_degradation_model.ipynb&#34;&gt;29.88&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_xgboost_linear_degradation_model.ipynb&#34;&gt;47.94&lt;/a&gt;&lt;sup&gt;* &lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD004_xgboost_linear_degradation_model.ipynb&#34;&gt;40.34&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;See the notebook to get a complete picture.&lt;/p&gt;

&lt;h2 id=&#34;enter-deep-learning&#34;&gt;Enter Deep Learning&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;In this section, we will apply deep learning to predict RUL of Turbofan dataset. Due to the nondeterministic nature of operations used in deep learning and dependence of libraries like &lt;code&gt;Tensorflow&lt;/code&gt; on computer architecture, readers might obtain slightly different results than those in the notebooks. For reproducibility of our results, we also share the saved models of each notebook. All saved models for Turbofan dataset can be found at this &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/tree/master/saved_models/cmapss&#34;&gt;link&lt;/a&gt;. A notebook describing the steps to use the saved models can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_using_saved_model_deep_learning.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD001&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD002&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD003&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;FD004&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Degradation Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;LSTM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_LSTM_piecewise_linear_degradation_model.ipynb&#34;&gt;15.16&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_LSTM_piecewise_linear_degradation_model.ipynb&#34;&gt;15.54&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1D CNN&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD001_1D_CNN_piecewise_linear_degradation_model.ipynb&#34;&gt;15.84&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/rul_codes_open/blob/master/notebooks/cmapss_notebooks/CMAPSS_FD003_1D_CNN_piecewise_linear_degradation_model.ipynb&#34;&gt;15.78&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Piecewise Linear (Early RUL = 125)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(This table will be updated gradually.)&lt;/p&gt;

&lt;h2 id=&#34;some-other-related-stuff&#34;&gt;Some other related stuff&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;Data-Driven Machinery Fault Diagnosis&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/&#34;&gt;Fault diagnosis of machines (A non-technical introduction)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/categories/blog/&#34;&gt;Blog articles by yours truly&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;why-have-i-used-only-jupyter-notebooks&#34;&gt;Why have I used only Jupyter notebooks?&lt;/h3&gt;

&lt;p&gt;These notebooks are for educational purposes only. Our experiments are relatively small scale and can be run in a reasonable amount of time in a notebook. I personally love the interactive nature of jupyter notebooks. We can see what we are doing. So the answer to the above question is: personal choice. I also don&amp;rsquo;t intend to deploy these, at least for the time being, in a production environment. Readers who wish to build deployment ready systems should bear in mind that they have to do many other things than just run an algorithm in a jupyter notebook.&lt;/p&gt;

&lt;p&gt;For attribution, cite this project as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{sahoo2018datadrivenrul,
  author = {Sahoo, Biswajit},
  title = {Data-Driven Remaining Useful Life (RUL) Prediction},
  url = {https://biswajitsahoo1111.github.io/rul_codes_open/},
  year = {2018}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readers should cite original datasets separately.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data-Driven Machinery Fault Diagnosis</title>
      <link>https://biswajitsahoo1111.github.io/project/cbm_codes_open/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://biswajitsahoo1111.github.io/project/cbm_codes_open/</guid>
      <description>

&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;left&#34;&gt;
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=cbm_codes_open&amp;type=star&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://biswajitsahoo1111.github.io/cbm_codes_open/&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View GitHub Page&lt;/a&gt;
  &lt;/td&gt;
  &lt;!----
  &lt;td align=&#34;center&#34;&gt;
    &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/GitHub-Mark-32px.png&#34; /&gt;View source on GitHub&lt;/a&gt;
  &lt;/td&gt;
  ----&gt;
  &lt;td align=&#34;left&#34; rowspan=&#34;2&#34;&gt;
    &lt;a href=&#34;https://codeload.github.com/biswajitsahoo1111/cbm_codes_open/zip/master&#34;&gt;&lt;img src=&#34;https://www.tensorflow.org/images/download_logo_32px.png&#34; /&gt;Download all code (.zip)&lt;/a&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
  &lt;td align=&#34;right&#34;&gt;  
    &lt;iframe src=&#34;https://ghbtns.com/github-btn.html?user=biswajitsahoo1111&amp;repo=cbm_codes_open&amp;type=fork&amp;count=true&amp;size=large&#34; frameborder=&#34;0&#34; scrolling=&#34;0&#34; width=&#34;170&#34; height=&#34;30&#34; title=&#34;GitHub&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;&lt;/iframe&gt;
  &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Condition based maintenance (CBM) is the process of doing maintenance only when it is required. Adoption of this maintenance strategy leads to significant monetary gains as it precludes periodic maintenance and reduces unplanned downtime. Another term commonly used for condition based maintenance is predictive maintenance. As the name suggests, in this method we predict in advance when to perform maintenance. Maintenance is required, if fault has already occurred or is imminent. This leads us to the problem of fault diagnosis and prognosis.&lt;/p&gt;

&lt;p&gt;In fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognosis, our aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model Based Approaches&lt;/li&gt;
&lt;li&gt;Data-Driven Approaches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In model based approach, a complete model of the system is formulated and it is then used for fault diagnosis and prognosis. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modeling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.&lt;/p&gt;

&lt;p&gt;In data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognosis. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognosis tasks.&lt;/p&gt;

&lt;h2 id=&#34;aim-of-the-project&#34;&gt;Aim of the project&lt;/h2&gt;

&lt;p&gt;In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.&lt;/p&gt;

&lt;p&gt;This is an ongoing project and modifications and additions of new techniques will be done over time. &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; are two popular programming languages that are used in machine learning applications. We will use those for our demonstrations. &lt;strong&gt;Tensorflow&lt;/strong&gt; will be used for deep learning applications. This page contains results on fault diagnosis only. Results on fault prognosis can be found &lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;results-using-case-western-reserve-university-bearing-data-https-csegroups-case-edu-bearingdatacenter-pages-welcome-case-western-reserve-university-bearing-data-center-website-sup-sup&#34;&gt;Results using &lt;a href=&#34;https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-western-reserve-university-bearing-data-center-website&#34;&gt;Case Western Reserve University Bearing Data&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;We will first apply classical feature based methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. In feature based methods, we will extensively use &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_energy_features.ipynb&#34;&gt;wavelet packet energy features&lt;/a&gt; and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_entropy_features.ipynb&#34;&gt;wavelet packet entropy featues&lt;/a&gt; that are calculated from raw time domain data. Dataset description and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/CWRU_time_domain_data_preprocessing.ipynb&#34;&gt;time domain preprocessing&lt;/a&gt; steps can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/CWRU_time_domain_data_preprocessing.ipynb&#34;&gt;here&lt;/a&gt;. Steps to &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Calculating_time_domain_features_CWRU.ipynb&#34;&gt;compute time domain features&lt;/a&gt; are explained in &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Calculating_time_domain_features_CWRU.ipynb&#34;&gt;this notebook&lt;/a&gt;. The procedure detailing calculation of wavelet packet energy features can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_energy_features.ipynb&#34;&gt;this link&lt;/a&gt; and similar calculations for wavelet packet entropy features can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/calculate_wavelet_packet_entropy_features.ipynb&#34;&gt;this link&lt;/a&gt;. Also see the following two notebooks for computation of wavelet packet features in Python: &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Wavelet_packet_energy_features_python.ipynb&#34;&gt;Wavelet packet energy features in Python&lt;/a&gt; and &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Wavelet_packet_entropy_features_python.ipynb&#34;&gt;Wavelet packet entropy features in Python&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time_cwru_python.ipynb&#34;&gt;SVM on time domain
features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;96.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_multiclass_time.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru_python.ipynb&#34;&gt;SVM on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.3%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_energy_multiclass_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Visualizing High Dimensional Data Using Dimensionality Reduction Techniques&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Dimensionality_Reduction.ipynb&#34;&gt;Python Code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/dimensionality_reduction_projection.pdf&#34;&gt;R Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru_python.ipynb&#34;&gt;SVM on wavelet packet entropy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.3%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/SVM_wavelet_entropy_multiclass_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru_python.ipynb&#34;&gt;SVM on time and wavelet packet features&lt;/a&gt; (12 classes, sampling frequency: 12k) (&lt;strong&gt;Achieves 100% test accuracy in one case&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/svm_12k_cwru.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_python.ipynb&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k_python.ipynb&#34;&gt;Multiclass Logistic Regression on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.7%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/multiclass_logistic_regression_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k_python.ipynb&#34;&gt;LDA on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;89.8%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k_python.ipynb&#34;&gt;LDA on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/LDA_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k_python.ipynb&#34;&gt;QDA on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;96.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k_python.ipynb&#34;&gt;QDA on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/QDA_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k_python.ipynb&#34;&gt;kNN on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;89.8%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k_python.ipynb&#34;&gt;kNN on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/kNN_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k_python.ipynb&#34;&gt;Decision tree on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;94.5%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k_python.ipynb&#34;&gt;Decision tree on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;99.7%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/decision_tree_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k_python.ipynb&#34;&gt;Bagging on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;97%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k_python.ipynb&#34;&gt;Bagging on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/bagging_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k_python.ipynb&#34;&gt;Boosting on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k_python.ipynb&#34;&gt;Boosting on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/boosting_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k_python.ipynb&#34;&gt;Random forest on wavelet packet energy features&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.1%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_48k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k_python.ipynb&#34;&gt;Random forest on wavelet packet energy features&lt;/a&gt; (12 classes, sampling frequency: 12k) (Overall accuracy: &lt;strong&gt;100%&lt;/strong&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k_python.ipynb&#34;&gt;Python code&lt;/a&gt;) (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/random_forest_12k.pdf&#34;&gt;R code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; This hyperlink points to the actual website of CWRU bearing dataset. Unfortunately, it has been recently observed that the original website remains down most of the time. As the dataset is well known, it can still be found on the internet at different places. Interested readers who want to experiment with this dataset can find it &lt;a href=&#34;https://data.mendeley.com/datasets/fkp3nn4tp7/1#folder-a8bb9715-4b7b-4fa1-8550-5b0cdcf62602&#34;&gt;here&lt;/a&gt; (If it&amp;rsquo;s not down). Actual data are stored in &lt;code&gt;.mat&lt;/code&gt; format. But the data in previous link are first extracted from &lt;code&gt;.mat&lt;/code&gt; format and then individually stored in &lt;code&gt;.csv&lt;/code&gt; format. Readers should first try to download the data from the original website. If that attempt fails, they should explore other options.&lt;/p&gt;

&lt;h2 id=&#34;enter-deep-learning&#34;&gt;Enter Deep Learning&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;In this section, we will show results of fault diagnosis task using deep learning on the same Case Western Reserve University bearing dataset. Due to the nondeterministic nature of operations used in deep learning and dependence of libraries like &lt;code&gt;Tensorflow&lt;/code&gt; on computer architecture, readers might obtain slightly different results than those in the notebooks. As a more reliable measure, we report average results of ten iterations. Our models are small enough to permit us to run those that many times in a reasonable amount of time. For reproducibility of our results, we also share the saved models of each notebook. All saved models can be found at &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/tree/master/notebooks/saved_models&#34;&gt;this link&lt;/a&gt;. A notebook describing the steps to use the saved models can be found &lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Using_saved_models_tensorflow.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Deep_learning_based_fault_diagnosis_using_CNN_on_raw_time_domain_data.ipynb&#34;&gt;Fault diagnosis using convolutional neural network (CNN) on raw time domain data&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;98.7%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Deep_learning_based_fault_diagnosis_using_CNN_on_continuous_wavelet_transform_of_time_data.ipynb&#34;&gt;CNN based fault diagnosis using continuous wavelet transform (CWT) of time domain data&lt;/a&gt; (10 classes, sampling frequency: 48k) (Overall accuracy: &lt;strong&gt;99.1%&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(This list will be updated gradually.)&lt;/p&gt;

&lt;h2 id=&#34;some-other-related-stuff&#34;&gt;Some other related stuff&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/D2L_Attention_Mechanisms_in_TF&#34;&gt;Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/rul_codes_open/&#34;&gt;Data-Driven Remaining Useful Life (RUL) Prediction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/&#34;&gt;Fault diagnosis of machines (A non-technical introduction)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biswajitsahoo1111.github.io/categories/blog/&#34;&gt;Blog articles by yours truly&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/matlab_intro.pdf&#34;&gt;A quick introduction to MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/transient_vibration_and_SRS_plots.pdf&#34;&gt;Transient vibration and shock response spectrum plots in MATLAB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;Simple examples on finding instantaneous frequency using Hilbert transform&lt;/a&gt; (&lt;a href=&#34;https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/hilbert_inst_freq_modulation.pdf&#34;&gt;MATLAB Code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;why-have-i-used-only-jupyter-notebooks&#34;&gt;Why have I used only Jupyter notebooks?&lt;/h3&gt;

&lt;p&gt;These notebooks are for educational purposes only. Our experiments are relatively small scale and can be run in a reasonable amount of time in a notebook. I personally love the interactive nature of jupyter notebooks. We can see what we are doing. So the answer to the above question is: personal choice. I also don&amp;rsquo;t intend to deploy these, at least for the time being, in a production environment. Readers who wish to build deployment ready systems should bear in mind that they have to do many other things than just run an algorithm in a jupyter notebook.&lt;/p&gt;

&lt;p&gt;For attribution, cite this project as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BibTeX citation
@misc{sahoo2016datadriven,
  author = {Sahoo, Biswajit},
  title = {Data-Driven Machinery Fault Diagnosis},
  url = {https://biswajitsahoo1111.github.io/cbm_codes_open/},
  year = {2016}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readers should cite original datasets separately.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
