<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Biswajit Sahoo</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Biswajit Sahoo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Biswajit Sahoo</copyright>
    <lastBuildDate>Thu, 09 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reading multiple files in Tensorflow 2</title>
      <link>/post/reading-multiple-files-in-tensorflow-2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/reading-multiple-files-in-tensorflow-2/</guid>
      <description>(Jupyter notebook for this post can be found here)In this post, we will read multiple .csv files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 .csv files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of .</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part III</title>
      <link>/post/principal-component-analysis-part-iii/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/principal-component-analysis-part-iii/</guid>
      <description>(Python codes for this post can be found here)In this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part II</title>
      <link>/post/principal-component-analysis-part-ii/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/principal-component-analysis-part-ii/</guid>
      <description>This post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.
Part-I: Basic Theory of PCAPart-III: Reproducing results of a published paper on PCAIn this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis - Part I</title>
      <link>/post/principal-component-analysis-part-i/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/principal-component-analysis-part-i/</guid>
      <description>In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function.</description>
    </item>
    
  </channel>
</rss>