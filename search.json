[{"authors":null,"categories":["Blog"],"content":"\r\rThis story was originally written for “Augmented Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. The full list of awardees and the title of their stories can be found here.\n\rPrelude\rRising sun with its gentle light marks the arrival of morning. Birds’ chirp as well as time on our clock, sometimes with a blaring alarm, confirm the arrival of morning. Each of these, among several others, is an indicator of the morning. But can we know about morning by following only one indicator? Let’s deliberate. What if the sky is cloudy and we don’t see the sun rising, will this mean that morning is yet to come? Of course not! Our alarm will remind us of morning irrespective of whether there is sun or not. But what if, on some occasion, our clock doesn’t work. In that case, birds may chirp or sun may rise or our near and dear ones may remind us that it’s morning already. So in essence, we usually don’t look for only one indicator, rather we consider several indicators. If one indicator fails, we can check another and thus be sure. It is very unlikely that all the indicators will fail simultaneously.\nSo the best way to get an idea about an event, it seems, is not to rely on only one indicator. Rather, observe several indicators and depending on their collective state, arrive at some conclusion. In this way, we deliberately add redundancy in order to get reliable results. This is exactly what we do in fault diagnosis of machines. Fault diagnosis is a broad term that addresses mainly three questions. First, find out whether fault is there in the machine or not. If fault is present, next question is to find the location of the fault. Once location of the fault is found, finally, find out the type of fault and its severity. In this article, we will only limit ourselves to the last aspect. But for simplicity, we will still use the term fault diagnosis to address that particular problem.\n\rThe method\rTo determine the health of a machine, we collect a set of indicators that best explain the condition of the machine. In scientific jargon, we call those features. Before discussing further let’s first discuss what are those features and how they are calculated.\nFirst, data needs to be collected from a machine whose health needs to be assessed. Data might pertain to vibration level of the machine or its temperature distribution or the sound produced by the machine or something else. Sensors are needed to collect each type of data. By analogy, a thermometer, which is used to measure body temperature of humans, is a sensor that measures temperature. Likewise different types of sensors are available to measure different quantities of interest related to the machine. From research it has been found that vibration based data are more suitable for fault diagnosis as compared to other types of data, say temperature or sound. So in this article, we will limit our attention to vibration based fault diagnosis. And the sensor that is most commonly used to measure the vibration of a machine is called an accelerometer. Form the data collected by accelerometer(s) we calculate features like the maximum level of vibration, similarly, the minimum level and other statistical features like skewness, kurtosis, etc. It is not uncommon to collect 10-15 features.\nAfter feature collection, the next task is to find out what type of faults are present by using those features. One way to do this is by comparing the obtained feature values to pre-existing standards. But standards are available for few specialized cases when each feature is considered in isolation. For multiple features, no concrete information can be obtained from standards. The way out of this problem is to come up with an algorithm that takes all feature values as input and produces the output related to the type of fault present.\nConstruction of such an algorithm requires prior faulty and non-faulty data of similar machine be fed to it. The algorithm should ideally work well on this prior data. Once fine-tuning of its parameters are done, new data are fed into the algorithm and from its output, we infer the fault type. If the algorithm is carefully constructed, error in prediction of fault type will be very small. In some cases, it is also possible to get perfect accuracy. The problem just considered is a sub-class of a broad field called pattern recognition. In pattern recognition, we try to find underlying patterns in features that correspond to different fault types. This type of pattern recognition tasks are best performed by machine learning algorithms. The simple technique just described works fine for a large class of problems. But there exist some problems for which the features previously calculated are not sufficient to identify fault. However, it is possible to modify the technique by using transformation of data as well as features. Transformations are a way of converting the original data into another type such that after transformation more insight is gained out of it. This is similar to using logarithms in mathematics to do complex calculations. While direct computation of complex multiplications and divisions is difficult, using logarithm we transform the original problem into a simpler form that can be solved easily in less time. The transformation trick along with pattern recognition methods are surprisingly effective for most fault diagnosis task.\n\rSome recent advances\rUp to this point, we have argued that redundancy is important. It helps us take reliable decisions. However, it requires collection of huge amounts of data. Thus, continuous monitoring of machine, also known as online monitoring, becomes infeasible. So we seek an algorithm that is capable of finding fault types using only a few measurements. One way to do this is to select a few important features that can perform fault diagnosis. Research shows that it is indeed possible. But merely finding best features is not enough. Because to calculate the features, even though small in number, we need to collect all data. Hence issues related to online monitoring will still exist. A way around this problem is not to collect all data but only a fraction of it randomly in time. And the data should be collected in such a way that all information regarding the machine can be extracted from these limited observations. An even optimistic goal is to reconstruct the original data from the limited collected data. By analogy, this is similar to reconstructing the speech of a person, who speaks, say, 3000 words, from 300 random words that you have remembered of their entire speech. The problem just described is known as compressed sensing. And no matter how much counter-intuitive it may seem, encouraging results for this problem have been obtained in signal processing and these methods are beginning to get applied to problems of fault diagnosis. The problem is still in its infancy in fault diagnosis field.\n\rWhat we learned (and what we didn’t!)\rIn summary, we have learned that to diagnose faults, we need multiple features and sometimes we have to transform the data into different domains for better accuracy. We then observed that we can get rid of the redundancy inherent in this method by using compressed sensing methods. All these techniques come under data-driven methods. It is called data-driven because all analyses are done after we collect relevant data from the machine. These methods are quite general purpose and can be used to diagnose faults in different components, say detecting faults in cars or in other machines.\nApart from data-driven methods there also exists another class of techniques that go by the broad name of model-based methods. In model-based methods, we formulate a full mathematical model of the machine and then try to find out how the response of the model changes if a fault is introduced and using this fact, try to find the nature of fault for a new problem. Though model-based techniques are important in their own right, in some cases it becomes very difficult to find an accurate model of the system. In contrast, data-driven methods are more robust against external noise and flexible, meaning we can perform different analysis using the same data and obtain different insights. Another advantage of using data-driven methods is that the whole process of fault diagnosis can easily be automated.\nIn this article, we have only considered the field of fault diagnosis. In fault diagnosis, faults are already present and we wish to either detect them or segregate them depending on fault type. But there exists another branch that deals with ways to predict the time of occurrence of fault in future, given the present state. Basically, they determine the remaining useful life of the machine. This sub-branch is called fault prognosis which is also an active area of research.\nGiven the advancement of research and scope for automation, it may be possible, in not so distant future, to get updates on your phone about possible malfunction of a part of your car while driving your car or while enjoying a ride in a self-driving car, maybe!!\n\r","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"d3115ef77297a658af8e830ead2c1e11","permalink":"/post/fault-diagnosis-of-machines/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/fault-diagnosis-of-machines/","section":"post","summary":"This story was originally written for “Augmented Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. The full list of awardees and the title of their stories can be found here.\n\rPrelude\rRising sun with its gentle light marks the arrival of morning.","tags":["Story"],"title":"Fault Diagnosis of Machines","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rAlmost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable. Then the final solution is obtained by back substituting the obtained value of the variable into remaining equations. This method, in mathematical jargon, is called Gaussian elimination and back substitution.\nIt turns out (surprisingly) that linear systems form the basis of many interesting engineering applications. Ultimately the problem boils down to solution (or approximate solution) of a system of linear equations. So a thorough understanding of linear systems is essential to appreciate the applications. In this post we will outline all possible cases of finding solutions to linear systems and briefly outline two most important applications.\nWe will use matrix notation to represent the equations succinctly. It also gives us better insight into their solution. Using matrix notation the system can be represented as\r\\[\\textbf{Ax}=\\textbf{b}\\]\rWhere \\(\\textbf{A}\\) is the matrix of coefficients of size \\((m\\times n)\\), \\(\\textbf{x}\\) is a vector of variables of size \\((n\\times 1)\\), and \\(\\textbf{b}\\)\ris a vector of size \\((m\\times 1)\\) representing constant right hand sides. Note that \\(\\textbf{b}\\) can be a vector of all zeros, i.e., \\(\\textbf{b} = \\textbf{0}\\) or it can be any arbitrary vector with some nonzero values, i.e.,\\(\\textbf{b}\\neq \\textbf{0}\\). The solution(s) of linear systems depend to a large extent on what the right hand side is as we will see shortly.\nApart from notation, we need two other concepts from matrix theory. One is of rank and other is the range space (or column space) of a matrix. Rank \\((Rank(\\textbf{A}))\\) of a matrix (say, \\(\\textbf{A}\\)) is defined as number of independent rows or columns of a matrix. It is a well known result in matrix theory that row rank (number of independent rows) is equal to column rank (number of independent columns) and \\(Rank(\\textbf{A})\\leq min(m,n)\\).\nRange space \\((\\mathcal{R}(A))\\)(in short, Range) of a matrix is the vector space of all possible linear combinations of columns of the matrix. As we take all possible linear combination of columns, it is also known as column space. Readers who are slightly more familiar with linear algebra may know that Range is the span of columns of \\(\\textbf{A}\\). Zero vector \\((\\textbf{0})\\) is always in the range of \\(\\textbf{A}\\) because if we take linear combination of columns of \\(\\textbf{A}\\) with all coefficients as 0’s, we get zero vector. Hence \\(\\textbf{b}=0 \\in \\mathcal{R}(\\textbf{A})\\) is always true.\nLet’s now discuss different cases separately and their solutions. We will assume that our system of equations has real entries.\n\rCase - I: \\((m = n)\\)\n\r\\(Rank(\\textbf{A}) = m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution (for any \\(\\textbf{b}\\)). For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 8 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : Impossible (This case will never happen because \\(Rank(\\textbf{A})=m\\))\r\r\\(Rank(\\textbf{A}) \u0026lt; m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r8\r\\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r1 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\rCase - II: \\((m \u0026gt; n)\\)\n\r\\(Rank(\\textbf{A}) = n\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution. But this case is immensely useful from application point of view. Sometimes it is not desirable to obtain the exact solution. Rather an approximate solution suffices for all practical purposes. Finding an approximate solution to an overdetermined system leads to the famous Least Squares problem.\r\r\\(Rank(\\textbf{A}) \u0026lt; n\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 4 \\\\\r3 \u0026amp; 6 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r9\r\\end{bmatrix}\r\\end{equation}\\] It has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 4 \\\\\r3 \u0026amp; 6 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r8\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\rCase - III: \\((m \u0026lt; n)\\)\n\r\\(Rank(\\textbf{A}) = m\\) :\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 5 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r2 \\\\\r3 \\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions. This case is also used in many applications. As there are infinitely many solutions, a natural choice is to choose the best solution. The qualifier ‘best’ determines what application we have in our mind. If we seek minimum \\((l_2)\\) norm, we get the so called minimum energy solution, a concept used in signal processing. Yet another concern is to seek for the sparsest solution (a solution with only a few nonzero entries and all other entries being zero). This idea is used in Compressed Sensing, an active research area with many interesting applications.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : Impossible. This case will never happen since \\(Rank(\\textbf{A})=m\\).\r\r\\(Rank(\\textbf{A}) \u0026lt; m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r4 \\\\\r8 \\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r1 \\\\\r5 \\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\r\rHope this post gives a clear overview of linear systems of equations. Interested reader may explore further applications. Comments and clarifications are welcome.\n","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"e805d2cc09051cefef839cab06b96815","permalink":"/post/revisiting-systems-of-linear-equations/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/post/revisiting-systems-of-linear-equations/","section":"post","summary":"Almost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable.","tags":["Linear Algebra"],"title":"Revisiting Systems of Linear Equations","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rThis post is Part-III of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. This paper got published in 2010 and since then its popularity has only grown. The paper has been cited 3800+ times as per Google scholar data.\nThis post contains code snippets in R. Equivalent MATLAB codes can be obtained by using commands of Part-II. For figures, the reader has to write his/her own code in MATLAB.\nStructure of the paper\rAlong with basic theory the paper contains three examples on PCA, one example on correspondence analysis and one example on multiple factor analysis. In this post we will only focus on PCA examples.\nData for the examples have been taken from the paper [1]. To get the original source of the data, refer to the paper.\nTo run following R codes seamlessly, readers have to load following three packages. If these packages have not been installed previously use ‘install.packages(“package_name”)’ to install those.\nlibrary(ggplot2)\rlibrary(ggrepel)\rlibrary(raster)\r## Loading required package: sp\r# Table 1\r# Load data\r(words = read.csv(\u0026quot;pca_abdi_words.csv\u0026quot;,header = T))\r## Words Word_length Lines_in_dict\r## 1 Bag 3 14\r## 2 Across 6 7\r## 3 On 2 11\r## 4 Insane 6 9\r## 5 By 2 9\r## 6 MOnastery 9 4\r## 7 Relief 6 8\r## 8 Slope 5 11\r## 9 Scoundrel 9 5\r## 10 With 4 8\r## 11 Neither 7 2\r## 12 Pretentious 11 4\r## 13 Solid 5 12\r## 14 This 4 9\r## 15 For 3 8\r## 16 Therefore 9 1\r## 17 Generality 10 4\r## 18 Arise 5 13\r## 19 Blot 4 15\r## 20 Infectious 10 6\r(words_centered = scale(words[,2:3],scale = F)) # Removing the first column\r## Word_length Lines_in_dict\r## [1,] -3 6\r## [2,] 0 -1\r## [3,] -4 3\r## [4,] 0 1\r## [5,] -4 1\r## [6,] 3 -4\r## [7,] 0 0\r## [8,] -1 3\r## [9,] 3 -3\r## [10,] -2 0\r## [11,] 1 -6\r## [12,] 5 -4\r## [13,] -1 4\r## [14,] -2 1\r## [15,] -3 0\r## [16,] 3 -7\r## [17,] 4 -4\r## [18,] -1 5\r## [19,] -2 7\r## [20,] 4 -2\r## attr(,\u0026quot;scaled:center\u0026quot;)\r## Word_length Lines_in_dict ## 6 8\rpca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA\rfactor_scores_words = pca_words_cov$x\rround(factor_scores_words,2)# Observer that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as it is negative of the direction given in the paper. It can also be checked that both the principal components are orthogonal.\r## PC1 PC2\r## [1,] -6.67 0.69\r## [2,] 0.84 -0.54\r## [3,] -4.68 -1.76\r## [4,] -0.84 0.54\r## [5,] -2.99 -2.84\r## [6,] 4.99 0.38\r## [7,] 0.00 0.00\r## [8,] -3.07 0.77\r## [9,] 4.14 0.92\r## [10,] -1.07 -1.69\r## [11,] 5.60 -2.38\r## [12,] 6.06 2.07\r## [13,] -3.91 1.30\r## [14,] -1.92 -1.15\r## [15,] -1.61 -2.53\r## [16,] 7.52 -1.23\r## [17,] 5.52 1.23\r## [18,] -4.76 1.84\r## [19,] -6.98 2.07\r## [20,] 3.83 2.30\rsum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal\r## [1] -4.773959e-15\r# Contibution of each factor (It is defined as square of factor score divided by sum of squares of factor scores in that column)\rround(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)\r## [1] 11.36 0.18 5.58 0.18 2.28 6.34 0.00 2.40 4.38 0.29 8.00\r## [12] 9.37 3.90 0.94 0.66 14.41 7.78 5.77 12.43 3.75\rround(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)\r## [1] 0.92 0.55 5.98 0.55 15.49 0.28 0.00 1.13 1.63 5.48 10.87\r## [12] 8.25 3.27 2.55 12.32 2.90 2.90 6.52 8.25 10.18\r# The calculations in above two lines can be done in a single line\rround(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)\r## PC1 PC2\r## [1,] 11.36 0.92\r## [2,] 0.18 0.55\r## [3,] 5.58 5.98\r## [4,] 0.18 0.55\r## [5,] 2.28 15.49\r## [6,] 6.34 0.28\r## [7,] 0.00 0.00\r## [8,] 2.40 1.13\r## [9,] 4.38 1.63\r## [10,] 0.29 5.48\r## [11,] 8.00 10.87\r## [12,] 9.37 8.25\r## [13,] 3.90 3.27\r## [14,] 0.94 2.55\r## [15,] 0.66 12.32\r## [16,] 14.41 2.90\r## [17,] 7.78 2.90\r## [18,] 5.77 6.52\r## [19,] 12.43 8.25\r## [20,] 3.75 10.18\r# Squared distance to center of gravity\r(dist = rowSums(factor_scores_words^2))\r## [1] 45 1 25 1 17 25 0 10 18 4 37 41 17 5 9 58 32 26 53 20\r# Ssquared cosine of observations of first PC\r(sq_cos = round(factor_scores_words^2/rowSums(factor_scores_words^2)*100))\r## PC1 PC2\r## [1,] 99 1\r## [2,] 71 29\r## [3,] 88 12\r## [4,] 71 29\r## [5,] 53 47\r## [6,] 99 1\r## [7,] NaN NaN\r## [8,] 94 6\r## [9,] 95 5\r## [10,] 29 71\r## [11,] 85 15\r## [12,] 90 10\r## [13,] 90 10\r## [14,] 74 26\r## [15,] 29 71\r## [16,] 97 3\r## [17,] 95 5\r## [18,] 87 13\r## [19,] 92 8\r## [20,] 74 26\rNan’s are produced because of division by zero.\n# Figue 1\rp = ggplot(words,aes(x = Lines_in_dict,y = Word_length,label = Words))+\rgeom_point()+ geom_text_repel()+ geom_hline(yintercept = 6)+geom_vline(xintercept = 8)+\rlabs(x = \u0026quot;Lines in dictionary\u0026quot;,y = \u0026quot;Word length\u0026quot;)\rprint(p)\r# Show directions of PCs\r# Note that intercept argument in geom_abline considers the line to be at the origin. In our case the data are mean shifted.\r# So we have to adjust the intercept taking new origin into consideration. These adjustments have been made below.\rslope1 = pca_words_cov$rotation[1,1]/pca_words_cov$rotation[2,1] # Slope of first PC\rslope2 = pca_words_cov$rotation[1,2]/pca_words_cov$rotation[2,2] # Slope of second PC\r(new_origin = c(mean(words$Lines_in_dict),mean(words$Word_length)))\r## [1] 8 6\rintercept1 = 6 - slope1*8\rintercept2 = 6 - slope2*8\rp+geom_abline(slope = slope1,intercept = intercept1,linetype = \u0026quot;dashed\u0026quot;,size = 1.2,col = \u0026quot;red\u0026quot;)+\rgeom_abline(slope = slope2,intercept = intercept2,linetype = \u0026quot;dashed\u0026quot;,size = 1.2,col = \u0026quot;blue\u0026quot;)\rIn the above figure red dashed line is the 1st principal component (PC) and blue dashed line is the 2nd PC.\n# Rotated PCs\r# This figure is obtained by plotting facotor scores. Note that we will plot negative of the factor scores of 1st PC to make the figure consistent with the paper.\rggplot(as.data.frame(pca_words_cov$x),aes(-pca_words_cov$x[,1],pca_words_cov$x[,2],label = words$Words))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Factor score along PC1\u0026quot;)+ylab(\u0026quot;Factor score along PC2\u0026quot;)\rGiven a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix.\n# Finding factor score of a new point\rsur = c(3,12) # It has 3 letter and 12 lines of dictionary entry\r(sur_centered = sur - colMeans(words[,2:3]))\r## Word_length Lines_in_dict ## -3 4\r(factor_scores_sur = round(sur_centered %*% pca_words_cov$rotation,2))\r## PC1 PC2\r## [1,] -4.99 -0.38\rEigenvalues and variance\rSee Part-II for details.\rTotal variance before transformation\n(total_var_before = round(sum(diag(var(words_centered))),3))\r## [1] 23.368\r(total_var_after = round(sum(diag(var(pca_words_cov$x))),3))\r## [1] 23.368\rCorrelation between principal components and original variables\r(In the paper,this correlation is also termed loading. But we will strictly reserve the loading term to mean loading matrix \\(\\textbf{P}\\) (see Part-I)\nThe sum of correlation coefficients between variables and principal components is 1. Intuitively, this means that variables are orthogonally projected onto the principal components.\n# Correlation between PCs and original variables\r(cor(pca_words_cov$x,words_centered))\r## Word_length Lines_in_dict\r## PC1 0.8679026 -0.9741764\r## PC2 0.4967344 0.2257884\rNote that the answers for correlation coefficients don’t match with that of the paper. Readers who get actual answers as given in paper are encouraged to comment below the post. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below.\n# Squared correaltion\r(cor(pca_words_cov$x,words_centered)^2)\r## Word_length Lines_in_dict\r## PC1 0.7532549 0.94901961\r## PC2 0.2467451 0.05098039\r# sum of correlation coefficients between variables and principal components is 1\rcolSums((cor(pca_words_cov$x,words_centered)^2))\r## Word_length Lines_in_dict ## 1 1\r(loading_matrix = pca_words_cov$rotation)\r## PC1 PC2\r## Word_length 0.5368755 0.8436615\r## Lines_in_dict -0.8436615 0.5368755\rCorrelation score for supplementary variable\n# Supplementary variable (Table 4)\rFrequency = c(8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1)\rNum_entries = c(6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2)\rsupp_data = data.frame(Frequency,Num_entries) # Supplementary data\r# Table 5\rsupp_data_cent = scale(supp_data,scale = F) # Centered supplementary data\r(corr_score_supp = round(cor(pca_words_cov$x,supp_data),4))\r## Frequency Num_entries\r## PC1 -0.3012 -0.6999\r## PC2 -0.7218 -0.4493\r# Note that correlation doesn\u0026#39;t depent on whether supplementary data is centered or not.\r(round(cor(pca_words_cov$x,supp_data_cent),4))\r## Frequency Num_entries\r## PC1 -0.3012 -0.6999\r## PC2 -0.7218 -0.4493\r# Squared correlation\r(round(cor(pca_words_cov$x,supp_data_cent)^2,4))\r## Frequency Num_entries\r## PC1 0.0907 0.4899\r## PC2 0.5210 0.2019\r(round(colSums(cor(pca_words_cov$x,supp_data_cent)^2),4))\r## Frequency Num_entries ## 0.6118 0.6918\rCorrelation circle plot\n# First plot correlation circle\rx = seq(0,2*pi,length.out = 300)\rcircle = ggplot() + geom_path(data = data.frame(a = cos(x),b = sin(x)),\raes(cos(x),sin(x)),alpha = 0.3, size = 1.5)+\rgeom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rannotate(\u0026quot;text\u0026quot;,x = c(1.08,0.05),y = c(0.05,1.08),label = c(\u0026quot;PC1\u0026quot;,\u0026quot;PC2\u0026quot;),angle = c(0,90))+\rxlab(NULL)+ylab(NULL)\r# Plotting original variables\rvariable_plot_original = circle + geom_point(data = as.data.frame(pca_words_cov$rotation),\raes(pca_words_cov$rotation[,1],pca_words_cov$rotation[,2]))+\rgeom_text_repel(aes(pca_words_cov$rotation[,1],pca_words_cov$rotation[,2],\rlabel = c(\u0026quot;Length of words\u0026quot;,\u0026quot;Number of lines in Dict.\u0026quot;))) print(variable_plot_original)\r# Plotting supplementary variables\rvariable_plot_original+\rgeom_point(data = as.data.frame(corr_score_supp),\raes(corr_score_supp[,1],corr_score_supp[,2]))+\rgeom_text_repel(aes(corr_score_supp[,1],corr_score_supp[,2],\rlabel = c(\u0026quot;Frequency\u0026quot;,\u0026quot;Number of entries\u0026quot;))) \r\r\rExample 2\rWine example\r# Correlation PCA using wine data # Table 6\r(wine = read.csv(\u0026quot;pca_abdi_wine.csv\u0026quot;,header = T))\r## wine_type hedonic for_meat for_dessert price sugar alcohol acidity\r## 1 wine_1 14 7 8 7 7 13 7\r## 2 wine_2 10 7 6 4 3 14 7\r## 3 wine_3 8 5 5 10 5 12 5\r## 4 wine_4 2 4 7 16 7 11 3\r## 5 wine_5 6 2 4 13 3 10 3\rpca_wine_cor = prcomp(wine[2:8],scale = T)\rggplot(as.data.frame(pca_wine_cor$x),aes(x = pca_wine_cor$x[,1],y = pca_wine_cor$x[,2], label = paste0(\u0026quot;wine \u0026quot;,1:5)))+\rgeom_point()+geom_text_repel()+ geom_vline(xintercept = 0)+ geom_hline(yintercept = 0)+\rxlab(\u0026quot;Factor score along PC1\u0026quot;)+ylab(\u0026quot;Factor score along PC2\u0026quot;)\rAgain our figure seems upside down than that of the paper. This is a minor discrepancy. Our 2nd eigen vector is negative of the one considered in paper. We can match the plot with that of the paper by just flipping the second principal component but we will not do that here.\n# Table 7\r# Factor scores along 1st and 2nd PC\r(pca_wine_cor$x[,1:2])\r## PC1 PC2\r## [1,] -2.3301649 1.095284\r## [2,] -2.0842419 -1.223185\r## [3,] 0.1673228 -0.370258\r## [4,] 1.7842392 1.712563\r## [5,] 2.4628448 -1.214405\r# Contibution of each observation to principal component\rround(pca_wine_cor$x[,1:2]^2/matrix(rep(colSums(pca_wine_cor$x[,1:2]^2),nrow(wine)),ncol = 2,byrow = T)*100,2)\r## PC1 PC2\r## [1,] 28.50 16.57\r## [2,] 22.80 20.66\r## [3,] 0.15 1.89\r## [4,] 16.71 40.51\r## [5,] 31.84 20.37\r# Squared cosine of observations of first PC\r(sq_cos = round(pca_wine_cor$x[,1:2]^2/rowSums(pca_wine_cor$x^2)*100))\r## PC1 PC2\r## [1,] 77 17\r## [2,] 69 24\r## [3,] 7 34\r## [4,] 50 46\r## [5,] 78 19\r# Loading scores corresponding to first two principal components\r(round(pca_wine_cor$rotation[,1:2],2))\r## PC1 PC2\r## hedonic -0.40 -0.11\r## for_meat -0.45 0.11\r## for_dessert -0.26 0.59\r## price 0.42 0.31\r## sugar -0.05 0.72\r## alcohol -0.44 -0.06\r## acidity -0.45 -0.09\r# Correlation score variables with first two principal compoents\r(corr_score_wine = round(cor(pca_wine_cor$x,wine[,2:8])[1:2,],2))\r## hedonic for_meat for_dessert price sugar alcohol acidity\r## PC1 -0.87 -0.97 -0.58 0.91 -0.11 -0.96 -0.99\r## PC2 -0.15 0.15 0.79 0.42 0.97 -0.07 -0.12\r# Correlation circle for wine data\r# Figure 6\rcorr_score_wine = t(corr_score_wine)\rcircle + geom_point(data = as.data.frame(corr_score_wine),\raes(corr_score_wine[,1],corr_score_wine[,2]))+\rgeom_text_repel(aes(corr_score_wine[,1],corr_score_wine[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))\r\rVarimax rotation\rRotation is applied to loading matrix such that after rotation principal components are interpretable. By interpretable, we mean, some of the loading scores will have higher values and some other loading scores will have lower values. So it can be said that the variables whose loading scores have higher value, contribute significantly towards principal components as compared to other variables with lesser loading scores. Though rotation works in certain cases, it must be remembered that it is no magic wand for principal component interpretability. One of the popular rotations is Varimax rotation. R has a built-in command to perform varimax rotation.\nVarimax rotation can be performed on the whole loading matrix or on a few components only. In the paper, varimax has been applied to first two principal components.\n# Loading scores of first two principal components\r(round(pca_wine_cor$rotation[,1:2],2))\r## PC1 PC2\r## hedonic -0.40 -0.11\r## for_meat -0.45 0.11\r## for_dessert -0.26 0.59\r## price 0.42 0.31\r## sugar -0.05 0.72\r## alcohol -0.44 -0.06\r## acidity -0.45 -0.09\r# Varimax applied to first two principal components\rrotated_loading_scores = varimax(pca_wine_cor$rotation[,1:2])\r# Loading scores after rotation (Table 10)\r(round(rotated_loading_scores$loadings[,1:2],2))\r## PC1 PC2\r## hedonic -0.41 -0.02\r## for_meat -0.41 0.21\r## for_dessert -0.12 0.63\r## price 0.48 0.21\r## sugar 0.12 0.72\r## alcohol -0.44 0.05\r## acidity -0.46 0.02\r# The same result can also be obtained by mulitplying the original loading # matrix by the rotation matrix obtained from varimax\r(round(pca_wine_cor$rotation[,1:2] %*% rotated_loading_scores$rotmat,2))\r## [,1] [,2]\r## hedonic -0.41 -0.02\r## for_meat -0.41 0.21\r## for_dessert -0.12 0.63\r## price 0.48 0.21\r## sugar 0.12 0.72\r## alcohol -0.44 0.05\r## acidity -0.46 0.02\r#Figure 7\r# Plot of loading socres before rotation\rggplot(as.data.frame(pca_wine_cor$rotation[,1:2]),aes(x = pca_wine_cor$rotation[,1],y = pca_wine_cor$rotation[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Loading score along PC1\u0026quot;)+ylab(\u0026quot;Loading score along PC2\u0026quot;)\r# Plot of loading scores after rotation\rggplot(as.data.frame(rotated_loading_scores$loadings[,1:2]),\raes(x = rotated_loading_scores$loadings[,1],\ry = rotated_loading_scores$loadings[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Loading score along PC1 after rotation\u0026quot;)+\rylab(\u0026quot;Loading score along PC2 after rotation\u0026quot;)\r\r\rExample 3\rFrench food example (Covariance PCA example)\r# Load data (Table 11) (food = read.csv(\u0026quot;pca_abdi_food.csv\u0026quot;,header = T))\r## class children bread vegetables fruit meat poultry milk wine\r## 1 Blue_collar 2 332 428 354 1437 526 247 427\r## 2 White_collar 2 293 559 388 1527 567 239 258\r## 3 Upper_class 2 372 767 562 1948 927 235 433\r## 4 Blue_collar 3 406 563 341 1507 544 324 407\r## 5 White_collar 3 386 608 396 1501 558 319 363\r## 6 Upper_class 3 438 843 689 2345 1148 243 341\r## 7 Blue_collar 4 534 660 367 1620 638 414 407\r## 8 White_collar 4 460 699 484 1856 762 400 416\r## 9 Upper_class 4 385 789 621 2366 1149 304 282\r## 10 Blue_collar 5 655 776 423 1848 759 495 486\r## 11 White_collar 5 584 995 548 2056 893 518 319\r## 12 Upper_class 5 515 1097 887 2630 1167 561 284\rpca_food_cov = prcomp(food[,3:9],scale = F)\r# Table 12\r# Factor scores\r(factor_scores_food = round(pca_food_cov$x[,1:2],2))\r## PC1 PC2\r## [1,] -635.05 120.89\r## [2,] -488.56 142.33\r## [3,] 112.03 139.75\r## [4,] -520.01 -12.05\r## [5,] -485.94 -1.17\r## [6,] 588.17 188.44\r## [7,] -333.95 -144.54\r## [8,] -57.51 -42.86\r## [9,] 571.32 206.76\r## [10,] -39.38 -264.47\r## [11,] 296.04 -235.92\r## [12,] 992.83 -97.15\r# Contibution of each observation to principal component\rround(pca_food_cov$x[,1:2]^2/matrix(rep(colSums(pca_food_cov$x[,1:2]^2),nrow(food)),ncol = 2,byrow = T)*100,2)\r## PC1 PC2\r## [1,] 13.34 5.03\r## [2,] 7.90 6.97\r## [3,] 0.42 6.72\r## [4,] 8.94 0.05\r## [5,] 7.81 0.00\r## [6,] 11.44 12.22\r## [7,] 3.69 7.19\r## [8,] 0.11 0.63\r## [9,] 10.80 14.71\r## [10,] 0.05 24.07\r## [11,] 2.90 19.15\r## [12,] 32.61 3.25\rdist = pca_food_cov$x[,1]^2+pca_food_cov$x[,2]^2\r# Squared cosine of observations of first PC (rowSums command from \u0026#39;raster\u0026#39; pcakage has been used)\r(sq_cos = round(pca_food_cov$x[,1:2]^2/rowSums(pca_food_cov$x^2)*100))\r## PC1 PC2\r## [1,] 95 3\r## [2,] 86 7\r## [3,] 26 40\r## [4,] 100 0\r## [5,] 98 0\r## [6,] 89 9\r## [7,] 83 15\r## [8,] 40 22\r## [9,] 86 11\r## [10,] 2 79\r## [11,] 57 36\r## [12,] 97 1\r# Table 13\r# squared loading score\r(round(pca_food_cov$rotation[,1:2]^2,2))\r## PC1 PC2\r## bread 0.01 0.33\r## vegetables 0.11 0.17\r## fruit 0.09 0.01\r## meat 0.57 0.01\r## poultry 0.22 0.06\r## milk 0.01 0.40\r## wine 0.00 0.02\rNote that this table doesn’t match with that of the paper. We will stick to our analysis.\n# Correlation score\r(corr_score_food = round((cor(pca_food_cov$x,food[,3:9])[1:2,]),2))\r## bread vegetables fruit meat poultry milk wine\r## PC1 0.36 0.91 0.96 1.00 0.98 0.41 -0.43\r## PC2 -0.87 -0.35 0.10 0.04 0.16 -0.88 -0.33\r# squared correlation score\r(round((cor(pca_food_cov$x,food[,3:9])[1:2,])^2,2))\r## bread vegetables fruit meat poultry milk wine\r## PC1 0.13 0.83 0.92 1 0.96 0.17 0.18\r## PC2 0.76 0.12 0.01 0 0.03 0.77 0.11\r# Figure 9\r# Correlation circle for food data\rcorr_score_food = t(corr_score_food)\rcircle + geom_point(data = as.data.frame(corr_score_food), aes(x = corr_score_food[,1],y = corr_score_food[,2]))+\rgeom_text_repel(data = as.data.frame(corr_score_food), aes(x = corr_score_food[,1],y = corr_score_food[,2],\rlabel = c(\u0026quot;Bread\u0026quot;,\u0026quot;Vegetables\u0026quot;,\u0026quot;Fruit\u0026quot;,\u0026quot;Meat\u0026quot;,\u0026quot;Poultry\u0026quot;,\u0026quot;Milk\u0026quot;,\u0026quot;Wine\u0026quot;)))\rNow observe that our correlation circle plot is almost close to that of the papers (though in opposite quadrants. But this is not a problem as we have previously mentioned).\n## Table 14\rcent_food = food[,3:9]-matrix(rep(colMeans(food[,3:9]),times = 12),nrow = 12,\rbyrow = T)\rsvd_food = svd(cent_food)\r# Eigenvalues\r(Eigenvalues = (svd_food$d)^2)\r## [1] 3023141.2354 290575.8390 68795.2333 25298.9496 22992.2474\r## [6] 3722.3214 723.9238\rImportant Note: These eigenvalues are not the same as variance of factor scores in principal components. Variance of principal component factor scores can be obtained by dividing the eigenvalues by \\((n-1)\\), where \\(n\\) is number of data points (in this case \\(n = 12\\)). If this point is still not clear, refer to Part-II.\n# Percentage contribution of each PC\r(round(Eigenvalues/sum(Eigenvalues),2))\r## [1] 0.88 0.08 0.02 0.01 0.01 0.00 0.00\r# Cumulative sum of eigen values\r(round(cumsum(Eigenvalues),2))\r## [1] 3023141 3313717 3382512 3407811 3430804 3434526 3435250\r# Cumulative percentage\r(round(cumsum(Eigenvalues)/sum(Eigenvalues),2))\r## [1] 0.88 0.96 0.98 0.99 1.00 1.00 1.00\r# RESS (Refer to the paper for a description)\rRESS = array(rep(0,7))\rfor (i in 1:7){\rRESS[i] = sum(Eigenvalues)-sum(Eigenvalues[1:i])\r}\rRESS\r## [1] 412108.5146 121532.6756 52737.4423 27438.4927 4446.2453 723.9238\r## [7] 0.0000\r# RESS/sum of eigenvalues\rround(RESS/sum(Eigenvalues),2)\r## [1] 0.12 0.04 0.02 0.01 0.00 0.00 0.00\rWe have not calculated the value of PRESS in this post as it will require us to consider random models. We will not pursue it here.\nThough unusually long, I hope, this post will be of help to (courageous) readers who work there way through till end. Please comment below if you find any errors or omissions.\nR Markdown file for this post\n\rReference\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\r\r\r","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"d862839c3611a99e119cc5e51a673237","permalink":"/post/principal-component-analysis-part-iii/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-iii/","section":"post","summary":"This post is Part-III of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J.","tags":["PCA","Machine Learning","R"],"title":"Principal Component Analysis - Part III","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rThis post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-III: Reproducing results of a published paper on PCA\r\rIn this post we will first apply built in commands to obtain results and then we will show how the same results can be obtained without using built-in commands. By this post, our aim is not to advocate the use of non-built-in functions. Rather, in our opinion, it enhances understanding by knowing what happens under the hood when a built-in function is called. In actual applications, readers should always use built functions as they are robust(almost always) and tested for efficiency.\nIn this post readers can find code snippets for R. Equivalent MATLAB codes for the same can be obtained from this link.\nWe will use French food data form reference [2]. Refer to the paper to know about the original source of the data. We will apply different methods to this data and compare the result.\nLoad Data\r#Load abdi food data\r(food = read.csv(\u0026quot;pca_abdi_food.csv\u0026quot;,header= T))\r## class children bread vegetables fruit meat poultry milk wine\r## 1 Blue_collar 2 332 428 354 1437 526 247 427\r## 2 White_collar 2 293 559 388 1527 567 239 258\r## 3 Upper_class 2 372 767 562 1948 927 235 433\r## 4 Blue_collar 3 406 563 341 1507 544 324 407\r## 5 White_collar 3 386 608 396 1501 558 319 363\r## 6 Upper_class 3 438 843 689 2345 1148 243 341\r## 7 Blue_collar 4 534 660 367 1620 638 414 407\r## 8 White_collar 4 460 699 484 1856 762 400 416\r## 9 Upper_class 4 385 789 621 2366 1149 304 282\r## 10 Blue_collar 5 655 776 423 1848 759 495 486\r## 11 White_collar 5 584 995 548 2056 893 518 319\r## 12 Upper_class 5 515 1097 887 2630 1167 561 284\r# Centerd data matrix\rcent_food = scale(food[,3:9],scale = F)\r# Scaled data matrix\rscale_food = scale(food[,3:9],scale = T)\r\rCovariance PCA\rUsing built-in function\r# Using built-in function\rpca_food_cov = prcomp(food[,3:9],scale = F)\r# Loading scores (we have printed only four columns out of seven)\r(round(pca_food_cov$rotation[,1:4],2))\r## PC1 PC2 PC3 PC4\r## bread 0.07 -0.58 -0.40 0.11\r## vegetables 0.33 -0.41 0.29 0.61\r## fruit 0.30 0.10 0.34 -0.40\r## meat 0.75 0.11 -0.07 -0.29\r## poultry 0.47 0.24 -0.38 0.33\r## milk 0.09 -0.63 0.23 -0.41\r## wine -0.06 -0.14 -0.66 -0.31\r# Factor score (we have printed only four PCs out of seven)\rWe have printed only four columns of loading scores out of seven.\n(round(pca_food_cov$x[,1:4],2))\r## PC1 PC2 PC3 PC4\r## [1,] -635.05 120.89 -21.14 -68.97\r## [2,] -488.56 142.33 132.37 34.91\r## [3,] 112.03 139.75 -61.86 44.19\r## [4,] -520.01 -12.05 2.85 -13.70\r## [5,] -485.94 -1.17 65.75 11.51\r## [6,] 588.17 188.44 -71.85 28.56\r## [7,] -333.95 -144.54 -34.94 10.07\r## [8,] -57.51 -42.86 -26.26 -46.55\r## [9,] 571.32 206.76 -38.45 3.69\r## [10,] -39.38 -264.47 -126.43 -12.74\r## [11,] 296.04 -235.92 58.84 87.43\r## [12,] 992.83 -97.15 121.13 -78.39\rWe have printed only four principal components out of seven.\n# Variances using built-in function\r(round(pca_food_cov$sdev^2,2))\r## [1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\r# Total variance\r(sum(round(pca_food_cov$sdev^2,2)))\r## [1] 312295.4\r\r\rComparison of variance before and after transformation\r# Total variance before transformation\rsum(diag(cov(food[,3:9])))\r## [1] 312295.4\r# Total variance after transformation\rsum(diag(cov(pca_food_cov$x)))\r## [1] 312295.4\rAnother important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.\n# Variance along variables before transformation\rround(diag(cov(food[,3:9])),2)\r## bread vegetables fruit meat poultry milk ## 11480.61 35789.09 27255.45 156618.39 62280.52 13718.75 ## wine ## 5152.63\rNote that calculation of variance is unaffected by centering data matrix. So variance of original data matrix as well as centered data matrix is same. Check it for yourself. Now see how PCA transforms these variance.\n# Variance along principal compoennts\rround(diag(cov(pca_food_cov$x)),2)\r## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\r# We can obtain the same result using built-in fucntion\rround(pca_food_cov$sdev^2,2)\r## [1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rPerforming covariance PCA manually using SVD\rsvd_food_cov = svd(cent_food)\r# Loading scores\rround(svd_food_cov$v[,1:4],2) # We have printed only four columns\r## [,1] [,2] [,3] [,4]\r## [1,] 0.07 -0.58 -0.40 0.11\r## [2,] 0.33 -0.41 0.29 0.61\r## [3,] 0.30 0.10 0.34 -0.40\r## [4,] 0.75 0.11 -0.07 -0.29\r## [5,] 0.47 0.24 -0.38 0.33\r## [6,] 0.09 -0.63 0.23 -0.41\r## [7,] -0.06 -0.14 -0.66 -0.31\r# Factor scores\rround((cent_food %*% svd_food_cov$v)[,1:4],2) # only 4 columns printed\r## [,1] [,2] [,3] [,4]\r## [1,] -635.05 120.89 -21.14 -68.97\r## [2,] -488.56 142.33 132.37 34.91\r## [3,] 112.03 139.75 -61.86 44.19\r## [4,] -520.01 -12.05 2.85 -13.70\r## [5,] -485.94 -1.17 65.75 11.51\r## [6,] 588.17 188.44 -71.85 28.56\r## [7,] -333.95 -144.54 -34.94 10.07\r## [8,] -57.51 -42.86 -26.26 -46.55\r## [9,] 571.32 206.76 -38.45 3.69\r## [10,] -39.38 -264.47 -126.43 -12.74\r## [11,] 296.04 -235.92 58.84 87.43\r## [12,] 992.83 -97.15 121.13 -78.39\r# Variance of principal components\rround(svd_food_cov$d^2/11,2)\r## [1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rOur data matrix contains 12 data points. So to find variance of principal components we have to divide the square of the diagonal matrix by 11. To know the theory behind it, refer Part-I\n\rPerforming covariance PCA using Eigen-decomoposition(Not recommended)\rThis procedure is not recommended because forming a covariance matrix is computationally not efficient for large matrices if data matrix contains smaller entries. So doing eigen analysis on covariance matrix may give erroneous results. However, for our example we can use it to obtain results.\neigen_food_cov = eigen(cov(cent_food))\r# Loading scores\rround(eigen_food_cov$vectors[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] -0.07 0.58 -0.40 0.11\r## [2,] -0.33 0.41 0.29 0.61\r## [3,] -0.30 -0.10 0.34 -0.40\r## [4,] -0.75 -0.11 -0.07 -0.29\r## [5,] -0.47 -0.24 -0.38 0.33\r## [6,] -0.09 0.63 0.23 -0.41\r## [7,] 0.06 0.14 -0.66 -0.31\r# Factor scores\rround((cent_food %*% eigen_food_cov$vectors)[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] 635.05 -120.89 -21.14 -68.97\r## [2,] 488.56 -142.33 132.37 34.91\r## [3,] -112.03 -139.75 -61.86 44.19\r## [4,] 520.01 12.05 2.85 -13.70\r## [5,] 485.94 1.17 65.75 11.51\r## [6,] -588.17 -188.44 -71.85 28.56\r## [7,] 333.95 144.54 -34.94 10.07\r## [8,] 57.51 42.86 -26.26 -46.55\r## [9,] -571.32 -206.76 -38.45 3.69\r## [10,] 39.38 264.47 -126.43 -12.74\r## [11,] -296.04 235.92 58.84 87.43\r## [12,] -992.83 97.15 121.13 -78.39\r# Variance along principal components\rround(eigen_food_cov$values,2)\r## [1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rInstead of using the ‘cov()’ command to find the covariance matrix manually and perform its eigen analysis.\ncov_matrix_manual_food = (1/11)*t(cent_food) %*% cent_food\reigen_food_new = eigen(cov_matrix_manual_food)\r# Loading scores\rround(eigen_food_new$vectors[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] -0.07 0.58 -0.40 -0.11\r## [2,] -0.33 0.41 0.29 -0.61\r## [3,] -0.30 -0.10 0.34 0.40\r## [4,] -0.75 -0.11 -0.07 0.29\r## [5,] -0.47 -0.24 -0.38 -0.33\r## [6,] -0.09 0.63 0.23 0.41\r## [7,] 0.06 0.14 -0.66 0.31\r# Variance along principal components\rround(eigen_food_new$values,2)\r## [1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rThere are also different ways to find total variance of the data matrix. We will explore some of the options.\n# Total varaiance before transformation\rsum(diag(cov(cent_food)))\r## [1] 312295.4\rNote that total variance is invariant to translations. So calculating the total variance on raw data will also give the same answer. Check it to convince yourself.\n\r\rCorrelation PCA\rWhen PCA is performed on a scaled data matrix (each variable is centered as well as variance of each variable is one), it is called correlation PCA. Before discussing correlation PCA we will take some time to see different ways in which we can obtain correlation matrix.\nDifferent ways to obtain correlation matrix.\r# Using built-in command\rround(cor(food[,3:9]),2)[,1:4] # We have printed only four columns\r## bread vegetables fruit meat\r## bread 1.00 0.59 0.20 0.32\r## vegetables 0.59 1.00 0.86 0.88\r## fruit 0.20 0.86 1.00 0.96\r## meat 0.32 0.88 0.96 1.00\r## poultry 0.25 0.83 0.93 0.98\r## milk 0.86 0.66 0.33 0.37\r## wine 0.30 -0.36 -0.49 -0.44\r# manually\rround((1/11)*t(scale_food) %*% scale_food,2)[,1:4]\r## bread vegetables fruit meat\r## bread 1.00 0.59 0.20 0.32\r## vegetables 0.59 1.00 0.86 0.88\r## fruit 0.20 0.86 1.00 0.96\r## meat 0.32 0.88 0.96 1.00\r## poultry 0.25 0.83 0.93 0.98\r## milk 0.86 0.66 0.33 0.37\r## wine 0.30 -0.36 -0.49 -0.44\r\r\rPerforming correlation PCA using built-in function\rpca_food_cor = prcomp(food[,3:9],scale = T)\r# Loading scores\rround(pca_food_cor$rotation[,1:4],2) # Printed only four\r## PC1 PC2 PC3 PC4\r## bread 0.24 -0.62 0.01 -0.54\r## vegetables 0.47 -0.10 0.06 -0.02\r## fruit 0.45 0.21 -0.15 0.55\r## meat 0.46 0.14 -0.21 -0.05\r## poultry 0.44 0.20 -0.36 -0.32\r## milk 0.28 -0.52 0.44 0.45\r## wine -0.21 -0.48 -0.78 0.31\r# Factor scores\rround(pca_food_cor$x[,1:4],2)\r## PC1 PC2 PC3 PC4\r## [1,] -2.86 0.36 -0.40 0.36\r## [2,] -1.89 1.79 1.31 -0.16\r## [3,] -0.12 0.73 -1.42 0.20\r## [4,] -2.04 -0.32 0.11 0.10\r## [5,] -1.69 0.16 0.51 0.16\r## [6,] 1.69 1.35 -0.99 -0.43\r## [7,] -0.93 -1.37 0.28 -0.26\r## [8,] -0.25 -0.63 -0.27 0.29\r## [9,] 1.60 1.74 -0.10 -0.40\r## [10,] 0.22 -2.78 -0.57 -0.25\r## [11,] 1.95 -1.13 0.99 -0.32\r## [12,] 4.32 0.10 0.57 0.72\r# Variances along principal componentes\rround(pca_food_cor$sdev^2,2)\r## [1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\r# Sum of vairances\rsum(pca_food_cor$sdev^2)\r## [1] 7\r\rComparison of variance before and after transformation\r# Total variance before transformation\rsum(diag(cov(scale_food)))\r## [1] 7\r# Total variance after transformation\rsum(diag(cov(pca_food_cor$x)))\r## [1] 7\rAnother important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.\n# Variance along variables before transformation\rround(diag(cov(scale_food)),2)\r## bread vegetables fruit meat poultry milk ## 1 1 1 1 1 1 ## wine ## 1\rThis is obvious as we have scaled the matrix. Now see how PCA transforms these variance.\n# Variance along principal compoennts\rround(diag(cov(pca_food_cor$x)),2)\r## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## 4.33 1.83 0.63 0.13 0.06 0.02 0.00\r# We can obtain the same result using built-in fucntion\rround(pca_food_cor$sdev^2,2)\r## [1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\rPerforming correlation PCA manually using SVD\rsvd_food_cor = svd(scale_food)\r# Loading scores\rround(svd_food_cor$v[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] 0.24 -0.62 0.01 -0.54\r## [2,] 0.47 -0.10 0.06 -0.02\r## [3,] 0.45 0.21 -0.15 0.55\r## [4,] 0.46 0.14 -0.21 -0.05\r## [5,] 0.44 0.20 -0.36 -0.32\r## [6,] 0.28 -0.52 0.44 0.45\r## [7,] -0.21 -0.48 -0.78 0.31\r# Factor scores\rround((scale_food %*% svd_food_cor$v)[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] -2.86 0.36 -0.40 0.36\r## [2,] -1.89 1.79 1.31 -0.16\r## [3,] -0.12 0.73 -1.42 0.20\r## [4,] -2.04 -0.32 0.11 0.10\r## [5,] -1.69 0.16 0.51 0.16\r## [6,] 1.69 1.35 -0.99 -0.43\r## [7,] -0.93 -1.37 0.28 -0.26\r## [8,] -0.25 -0.63 -0.27 0.29\r## [9,] 1.60 1.74 -0.10 -0.40\r## [10,] 0.22 -2.78 -0.57 -0.25\r## [11,] 1.95 -1.13 0.99 -0.32\r## [12,] 4.32 0.10 0.57 0.72\r# Variance along each principcal component\rround(svd_food_cor$d^2/11,2)\r## [1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\r# Sum of variances\rsum(svd_food_cor$d^2/11)\r## [1] 7\rAgain we have to divide by 11 to get eigenvalues of correlation matrix. Check the formulation of correlation matrix using scaled data matrix to convince yourself.\n\rUsing eigen-decomposition (Not Recommended)\reigen_food_cor = eigen(cor(food[,3:9]))\r# Loading scores\rround(eigen_food_cor$vectors)\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r## [1,] 0 1 0 -1 0 1 0\r## [2,] 0 0 0 0 1 0 0\r## [3,] 0 0 0 1 0 1 0\r## [4,] 0 0 0 0 0 0 1\r## [5,] 0 0 0 0 0 0 -1\r## [6,] 0 1 0 0 0 0 0\r## [7,] 0 0 -1 0 0 0 0\r# Factor scores\rround((scale_food %*% eigen_food_cor$vectors)[,1:4],2)\r## [,1] [,2] [,3] [,4]\r## [1,] 2.86 -0.36 -0.40 0.36\r## [2,] 1.89 -1.79 1.31 -0.16\r## [3,] 0.12 -0.73 -1.42 0.20\r## [4,] 2.04 0.32 0.11 0.10\r## [5,] 1.69 -0.16 0.51 0.16\r## [6,] -1.69 -1.35 -0.99 -0.43\r## [7,] 0.93 1.37 0.28 -0.26\r## [8,] 0.25 0.63 -0.27 0.29\r## [9,] -1.60 -1.74 -0.10 -0.40\r## [10,] -0.22 2.78 -0.57 -0.25\r## [11,] -1.95 1.13 0.99 -0.32\r## [12,] -4.32 -0.10 0.57 0.72\r# Variances along each principal component\rround(eigen_food_cor$values,2)\r## [1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\rI hope this post would help clear some of the confusions that a beginner might have while encountering PCA for the first time. Please comment below if you find any errors.\nR Markdown file for this post\n\r\rReferences\rI.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\r\r","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549238400,"objectID":"b37d1d5f68fa7976b26279b7dfbcecbb","permalink":"/post/principal-component-analysis-part-ii/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-ii/","section":"post","summary":"This post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-III: Reproducing results of a published paper on PCA\r\rIn this post we will first apply built in commands to obtain results and then we will show how the same results can be obtained without using built-in commands. By this post, our aim is not to advocate the use of non-built-in functions.","tags":["PCA","Machine Learning","R","MATLAB"],"title":"Principal Component Analysis - Part II","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rIn this post we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The application of PCA and its variants are ubiquitous. In almost all software, such as MATLAB, R, etc., built-in commands are available to perform PCA. In this post we will show how can we obtain results of PCA from raw data first using and then without using built in commands. Then we will reproduce the results of a published paper on PCA that is very popular in academic circles. The post is divided into three parts to make it manageable to read. Readers who are totally familiar with PCA should read none and leave the page immediately. Other readers who are familiar with PCA but want to see different implementations, should jump to the part they wish to read. Absolute beginners should start with Part-I and work their way through gradually. Beginners are also encouraged to explore the references for further information. Here is the outline of different parts:\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\rPart-III: Reproducing results of a published paper on PCA\r\rFor Part-II, both MATLAB and R codes are available to reproduce all the results. Part-III contains only R codes. Equivalent MATLAB codes can be obtained by using commands of Part-II. In this post, we will discuss the theory behind PCA.\nPrincipal Component Analysis (PCA) is one of the most popular dimensionality reduction techniques. Though its origin dates back to early 20th century, it has never gone out of fashion it seems. Its popularity grows steady as can be gauged by the number of papers and articles published related to PCA or its variants.\nAll popular programming platforms contain built in functions that perform PCA given a data matrix. In this blog we will use the open source statistical programming environment R to demonstrate the result.\nWe will also show how we can obtain the results using simple matrix operations without using the built-in function.\nIn this blog we will reproduce using R all the results of an immensely popular paper on PCA by Abdi et. al. The paper got published in 2010 and within 8 years it has got more than 3800 citations. The data will be taken from the\rpaper itself.\nPrincipal Component Analysis\rTheory:\rThe usual convention is to place variables as columns and different observations as rows (Data frames in R follow this convention). For example, let’s suppose we are collecting data about daily weather for a year. Our variables of interest may include maximum temperature in a day, minimum temperature, humidity, max. wind speed, etc. For every day, we have to collect observations for each of these variables. In vector form, our data point for one day will contain number of observations equal to the number of variables under study and this becomes one row of our data matrix. Assuming that we are observing 10 variables everyday, our data matrix for one year (assuming it’s not a leap year) will contain 365 rows and 10 columns. Once data matrix is obtained, further analysis is done on this data matrix to obtain important hidden information regarding the data. We will use notations from matrix theory to simplify our analysis.\nLet be the data matrix of size \\(n\\times p\\), where is the number of data points and is the number of variables. We can assume without any loss of generality that is centered, meaning its column means are zero. This only shifts the data towards the origin without changing their relative orientation. So if originally is not centered, it is first centered before doing PCA. From now onward we will assume that is always centered.\nVariance of a variable (a column)in \\(\\textbf{X}\\) is equal to sum of squares of entries (because the column is centered) of that column divided by (n - 1) to make it unbiased. So sum of variance of all variables is \\(\\frac{1}{n - 1}\\) times sum of squares of all elements of the matrix . Readers who are familiar with matrix norms would instantly recognize that total variance is \\(\\frac{1}{n - 1}\\) times the square of Frobenius norm of \\(\\textbf{X}\\).\nTotal variance before transformation =\r\\[\\frac{1}{n-1}\\sum_{i,j}{x_{ij}^2}=trace(\\frac{1}{n-1}\\textbf{X}^T\\textbf{X})=\\frac{1}{n-1}\\|\\textbf{X}\\|_{F}^2\\]\rWhere trace of a matrix is sum of its diagonal entries.\nThe aim of PCA is to transform the data in such a way that along first principal direction, variance of transformed data is maximum. It subsequently finds second principal direction orthogonal to the first one in such a way that it explains maximum of the remaining variance among all possible direction in the orthogonal subspace.\nIn matrix form the transformation can be written as\r\\[\\textbf{Y}_{n\\times p}=\\textbf{X}_{n\\times p}\\textbf{P}_{p\\times p}\\]\rWhere \\(\\textbf{Y}\\) is the transformed data matrix. The columns of \\(\\textbf{Y}\\) are called principal components and \\(\\textbf{P}\\) is usually called loading matrix. Our aim is to find matrix \\(\\textbf{P}\\). Once we find \\(\\textbf{P}\\) we can then find \\(\\textbf{Y}\\) just by a matrix multiplication. Though we will not go into to proof here, it can be easily proved (see references), that matrix \\(\\textbf{P}\\) is the eigenvector matrix of the covariance matrix. Let’s first define covariance matrix.\nGiven a data matrix \\(\\textbf{X}\\)(centered), its covariance matrix \\((\\textbf{S})\\) is defined as\r\\[\\textbf{S} = \\frac{1}{n-1}\\textbf{X}^T\\textbf{X}\\]\rAs principal directions are orthogonal, we will also require \\(\\textbf{P}\\) to be an orthogonal matrix.\nNow, it is straightforward to form the covariance matrix and by placing its eigenvectors as columns, we can find matrix \\(\\textbf{P}\\) and consequently the principal components. The eigenvectors are arranged in such a way that first column is the eigenvector corresponding to largest eigenvector, second column (second eigenvector) corresponds to second largest eigenvalue and so on. Here we have assumed that we will always be able to find all the \\(p\\) orthogonal eigenvectors. In fact, we will always be able to find \\(p\\) orthogonal eigenvectors as the matrix is symmetric. It can also be shown that the transformed matrix \\(\\textbf{Y}\\) is centered and more remarkably, total variance of columns of \\(\\textbf{Y}\\) is same as total variance of columns of \\(\\textbf{X}\\). We will prove these two propositions as the proof are short.\nLet \\(\\textbf{1}\\) be a column vector of all ones of size \\((n\\times 1)\\). To prove that columns of \\(\\textbf{Y}\\) are centered, just premultiply it by \\(\\textbf{1}^T\\) (this finds column sum for each column). So\r\\[\\textbf{1}^T \\textbf{Y} = \\textbf{1}^T\\textbf{X}\\textbf{P}\\]\rBut columns of \\(\\textbf{X}\\) are already centered, so \\(\\textbf{1}^T\\textbf{X}=\\textbf{0}\\). Thus \\(\\textbf{1}^T \\textbf{Y}= \\textbf{0}\\). Hence columns of \\(\\textbf{Y}\\) are centered.\nTo prove that total variance of \\(\\textbf{Y}\\) also remains same, observe that\ntotal covariance of \\(\\textbf{Y}\\) =\r\\[trace(\\frac{1}{n-1}\\textbf{Y}^{T}\\textbf{Y})=\\frac{1}{n-1}trace((\\textbf{P}^T\\textbf{X}^{T}\\textbf{X})\\textbf{P})=\\\\\\frac{1}{n-1}trace((\\textbf{P}\\textbf{P}^T)\\textbf{X}^{T}\\textbf{X})=trace(\\frac{1}{n-1}\\textbf{X}^T\\textbf{X})\\]\rThe previous equation uses the fact that trace is commutative(i.e.\\(trace(\\textbf{AB})=trace(\\textbf{BA})\\)) and \\(\\textbf{P}\\) is orthogonal (i.e. \\(\\textbf{P}\\textbf{P}^T=\\textbf{I}\\)).\nLink between total variance and eigenvalues\rTotal variance is sum of eigenvalues of covriance matrix \\((\\textbf{S})\\). We will further discuss this point in Part-II.\n\rVariations in PCA\rSometimes our data matrix contains variables that are measured in different units. So we might have to scale the centered matrix to reduce the effect of variables with large variation. So depending on the matrix on which PCA is performed, it is divided into two types.\n\rCovariance PCA (Data matrix is centered but not scaled)\rCorrelation PCA (Data matrix is centered and scaled)\r\rExamples of these two types can be found in Part-II.\n##Some common terminology associated with PCA:\r* Factor scores corresponding to a principal component\nValues of that column of \\(\\textbf{Y}\\) that corresponds to the principal component.\n\rLoading score\r\rValues corresponding to a column of \\(\\textbf{P}\\). For example,loading scores of variables corresponding to first principal component are the values of the first column of \\(\\textbf{P}\\).\n\rInertia\r\rSquare of Frobenius norm of the matrix.\n\rHow actually are principal components computed\rThe previously stated method of finding eigenvectors of covariance matrix is not computationally effective. In practice, singular value decomposition (SVD) is used to find the matrix \\(\\textbf{P}\\). SVD theorem tells that any real matrix \\(\\textbf{X}\\) can be decomposed into three matrices such that\r\\[ \\textbf{X} = \\textbf{U}\\Sigma\\textbf{V}^T\\]\rWhere, \\(\\textbf{X}\\) is of size \\(n\\times p\\). \\(\\textbf{U}\\) and \\(\\textbf{V}\\) are orthogonal matrices of size \\(n\\times n\\) and \\(p\\times p\\) respectively. \\(\\Sigma\\) is a diagonal matrix of size \\(n\\times p\\).\nGiven the SVD decomposition of a matrix \\(\\textbf{X}\\),\r\\[\\textbf{X}^T\\textbf{X}=\\textbf{V}\\Sigma^2\\textbf{V}^T\\]\rThis is the eigen-decomposition of \\(\\textbf{X}^T\\textbf{X}\\). So \\(\\textbf{V}\\) is the eigenvector matrix of \\(\\textbf{X}^T\\textbf{X}\\). For PCA we need eigenvector matrix of covariance matrix. So converting the equation into convenient form, we get\r\\[\\textbf{S} = \\frac{1}{n-1}\\textbf{X}^T\\textbf{X}=\\textbf{V}(\\frac{1}{n-1}\\Sigma^2)\\textbf{V}^T\\]\rThus eigenvalues of S are diagonal entries of \\((\\frac{1}{n-1}\\Sigma^2)\\). As SVD is computationally efficient, all built-in functions use SVD to find the loading matrix and then use it to find principal components.\nIn the interest of keeping the post at a reasonable length, we will stop our exposition of theory here. Whatever we have discussed is only a fraction of everything. Entire books have been written on PCA. Interested readers who want to pursue further can refer to the references given here and later to the references given in the references. Please comment below if you find any errors.\nR Markdown file for this post\n\r\rReferences\rI.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\r\r\r","date":1549152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549152000,"objectID":"ab28fc880c4077431666d7f4775dd934","permalink":"/post/principal-component-analysis-part-i/","publishdate":"2019-02-03T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-i/","section":"post","summary":"In this post we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The application of PCA and its variants are ubiquitous. In almost all software, such as MATLAB, R, etc., built-in commands are available to perform PCA. In this post we will show how can we obtain results of PCA from raw data first using and then without using built in commands.","tags":["PCA","Machine Learning","R"],"title":"Principal Component Analysis - Part I","type":"post"},{"authors":null,"categories":null,"content":" Introduction to the problem Condition based maintenance is the process of doing maintenance only when it is required. This has many advantages along with monetary gain as it precludes periodic maintenance and reduces unplanned downtime. The next logical question is to figure out when maintenance is required. Maintenance is required if fault has either occurred or is imminent. This leads us to the problem of fault diagnosis and prognostics.\nIn fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognostics out aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:\n Model Based Approach Data-Driven Approach  In model based a complete model of the system is formulated and it is then used for fault diagnosis and prognostics. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modelling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.\nIn data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognostics. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognostics tasks.\nAim of the project In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.\nThis is an ongoing project and modifications and additions of new techniques will be done over time. Python, R, and MATLAB are popular programming languages that are used for machine learning applications. We will use those for our demonstrations.\nResults  Multiclass bearing fault classification using SVM on time domain features (On Case Western Reserve University Bearing Data)\n Multiclass bearing fault classification using SVM on wavelet packet energy features (On Case Western Reserve University Bearing Data)\n Visualizing High Dimensional Data Using Dimensionality Reduction Techniques (Python Code) (R Code)\n Multiclass bearing fault classification using SVM on wavelet packet entropy features (On Case Western Reserve University Bearing Data)\n Multiclass bearing fault classification using SVM on CWRU data (sampling frequency 12k) (Achieves 100% test accuracy in one case)\n  (This list will be updated gradually.)\nSome other related stuff  Simple examples on finding instantaneous frequency using Hilbert transform (MATLAB Code)  ","date":1461695400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461695400,"objectID":"fff49bc595c6f4bb0b2abd821deb85a2","permalink":"/project/personal-project/","publishdate":"2016-04-27T00:00:00+05:30","relpermalink":"/project/personal-project/","section":"project","summary":"The main aim of this project is to produce reproducible results. We will apply some of the standard machine learning techniques to publicly available machinery data sets and show the results with code. This is an ongoing project and will evolve over time. Related notebooks and data can be found in this [github page](https://github.com/biswajitsahoo1111/cbm_codes_open).","tags":null,"title":"Personal Project: Data-Driven Machinery Condition Monitoring","type":"project"}]