[{"authors":["admin"],"categories":null,"content":"I am a PhD student in the Department of Mechanical Engineering at IIT Kharagpur, India. Presently, I am working on machinery fault diagnosis and prognosis using deep learning. During my PhD, I developed an interest in machine learning and deep learning in particular. Though my training is in mechanical engineering, I have acquired machine learning skills by self-study and from MOOCs through online certifications.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student in the Department of Mechanical Engineering at IIT Kharagpur, India. Presently, I am working on machinery fault diagnosis and prognosis using deep learning. During my PhD, I developed an interest in machine learning and deep learning in particular. Though my training is in mechanical engineering, I have acquired machine learning skills by self-study and from MOOCs through online certifications.","tags":null,"title":"Biswajit Sahoo","type":"authors"},{"authors":[],"categories":["Blog"],"content":"\rNote: Whether this method is efficient or not is contestable. Efficiency of a pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they run this method on their own system. The system on which we ran this notebook has 44 CPU cores. Tensorflow version is 2.2.0 and it is XLA enabled. We did not use any GPU. We achieved 20% improvement over naive method. For one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope that this method can be applied for other applications as well. Please note that for some weird reason, the speedup technique doesn’t work in Google Colab. But it works in GPU enabled personal systems, that I have checked.\n\rView source on GitHub\r\r\rDownload notebook\r\r\rThis post is a sequel to an older post. In the previous post, we discussed ways in which we can read multiple files in Tensorflow 2. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are spectrogram and normalizing (converting each value to a standard normal value). We have chosen these transformations just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this tensorflow guide.\nAs this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on section 4 which is the main topic of this post.\nOutline:\rCreate 500 \".csv\" files and save it in the folder “random_data” in current directory.\rWrite a generator that reads data from the folder in chunks and transforms it.\rBuild data pipeline and train a CNN model.\rHow to make the code run faster?\rHow to make predictions?\r\r\n\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.\r* Each .csv file will have one column of data with 1024 entries.\r* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”\rcategory will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.\nimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026quot;./random_data/\u0026quot;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026quot;./random_data/\u0026quot; + eval(\u0026quot;fault_class\u0026quot;) + \u0026quot;_\u0026quot; + \u0026quot;{0:03}\u0026quot;.format(i+1) + \u0026quot;.csv\u0026quot; # This creates file_name\rnp.savetxt(eval(\u0026quot;file_name\u0026quot;), data, delimiter = \u0026quot;,\u0026quot;, header = \u0026quot;V1\u0026quot;, comments = \u0026quot;\u0026quot;)\rprint(str(eval(\u0026quot;number_of_files_in_each_class\u0026quot;)) + \u0026quot; \u0026quot; + eval(\u0026quot;fault_class\u0026quot;) + \u0026quot; files\u0026quot; + \u0026quot; created.\u0026quot;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = np.sort(glob.glob(\u0026quot;./random_data/*\u0026quot;))\rprint(\u0026quot;Total number of files: \u0026quot;, len(files))\rprint(\u0026quot;Showing first 10 files...\u0026quot;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\rarray([\u0026#39;./random_data/Fault_1_001.csv\u0026#39;, \u0026#39;./random_data/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_003.csv\u0026#39;, \u0026#39;./random_data/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_005.csv\u0026#39;, \u0026#39;./random_data/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_007.csv\u0026#39;, \u0026#39;./random_data/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_009.csv\u0026#39;, \u0026#39;./random_data/Fault_1_010.csv\u0026#39;],\rdtype=\u0026#39;\u0026lt;U29\u0026#39;)\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data/Fault_1_001.csv\rprint(files[0][14:21])\rFault_1\rNow that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.\n\n\r2. Write a generator that reads data in chunks and preprocesses it\rThese are the few things that we want our generator to have.\nIt should run indefinitely, i.e., it is an infinite loop.\rInside generator loop, read individual files using pandas.\rDo transformations on data if required.\rYield the data.\r\rAs we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.\n\r\rClass\rLabel\r\r\r\rFault_1\r0\r\rFault_2\r1\r\rFault_3\r2\r\rFault_4\r3\r\rFault_5\r4\r\r\r\rThe generator will yield both data and labels. The generator takes a list of file names as first argument. The second argument is batch_size.\nimport tensorflow as tf\rimport pandas as pd\rimport re\rdef tf_data_generator(file_list, batch_size = 20):\ri = 0\rwhile True: # This loop makes the generator an infinite loop\rif i*batch_size \u0026gt;= len(file_list): i = 0\rnp.random.shuffle(file_list)\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size] data = []\rlabels = []\rlabel_classes = tf.constant([\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]) for file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;)).astype(np.float32) # Read data\r#########################################################################################################\r# Apply transformations. Comment this portion if you don\u0026#39;t have to do any.\r# Try to use Tensorflow transformations as much as possible. First compute a spectrogram.\rtemp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))\r# After STFT transformation with given parameters, shape = (31,33)\rtemp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization\r##########################################################################################################\r# temp = tf.reshape(temp, (32,32,1)) # Uncomment this line if you have not done any transformation.\rdata.append(temp)\rpattern = tf.constant(eval(\u0026quot;file[14:21]\u0026quot;)) for j in range(len(label_classes)):\rif re.match(pattern.numpy(), label_classes[j].numpy()): labels.append(j)\rdata = np.asarray(data).reshape(-1,31,33,1) labels = np.asarray(labels)\ryield data, labels\ri = i + 1\rbatch_size = 15\rdataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\routput_shapes = ((None,31,33,1),(None,)))\rfor data, labels in dataset.take(7):\rprint(data.shape)\rprint(labels)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 31, 33, 1)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)\rThe generator works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the tf_data_generator create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.\n\n\r3. Building data pipeline and training a CNN model\rBefore building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.\nimport shutil\rCreate five different folders.\nfault_folders = [\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]\rfor folder_name in fault_folders:\ros.mkdir(os.path.join(\u0026quot;./random_data\u0026quot;, folder_name))\rMove files into those folders.\nfor file in files:\rpattern = \u0026quot;^\u0026quot; + eval(\u0026quot;file[14:21]\u0026quot;)\rfor j in range(len(fault_folders)):\rif re.match(pattern, fault_folders[j]):\rdest = os.path.join(\u0026quot;./random_data/\u0026quot;,eval(\u0026quot;fault_folders[j]\u0026quot;))\rshutil.move(file, dest)\rglob.glob(\u0026quot;./random_data/*\u0026quot;)\r[\u0026#39;./random_data/Fault_1\u0026#39;,\r\u0026#39;./random_data/Fault_2\u0026#39;,\r\u0026#39;./random_data/Fault_3\u0026#39;,\r\u0026#39;./random_data/Fault_4\u0026#39;,\r\u0026#39;./random_data/Fault_5\u0026#39;]\rnp.sort(glob.glob(\u0026quot;./random_data/Fault_1/*\u0026quot;))[:10] # Showing first 10 files of Fault_1 folder\rarray([\u0026#39;./random_data/Fault_1/Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_010.csv\u0026#39;], dtype=\u0026#39;\u0026lt;U37\u0026#39;)\rnp.sort(glob.glob(\u0026quot;./random_data/Fault_3/*\u0026quot;))[:10] # Showing first 10 files of Falut_3 folder\rarray([\u0026#39;./random_data/Fault_3/Fault_3_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_010.csv\u0026#39;], dtype=\u0026#39;\u0026lt;U37\u0026#39;)\rPrepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.\nfault_1_files = glob.glob(\u0026quot;./random_data/Fault_1/*\u0026quot;)\rfault_2_files = glob.glob(\u0026quot;./random_data/Fault_2/*\u0026quot;)\rfault_3_files = glob.glob(\u0026quot;./random_data/Fault_3/*\u0026quot;)\rfault_4_files = glob.glob(\u0026quot;./random_data/Fault_4/*\u0026quot;)\rfault_5_files = glob.glob(\u0026quot;./random_data/Fault_5/*\u0026quot;)\rfrom sklearn.model_selection import train_test_split\rfault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)\rfault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)\rfault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)\rfault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)\rfault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)\rfault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)\rfault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)\rfault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)\rfault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)\rfault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)\rtrain_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train\rvalidation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val\rtest_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test\r# Shuffle files\rnp.random.shuffle(train_file_names)\rprint(\u0026quot;Number of train_files:\u0026quot; ,len(train_file_names))\rprint(\u0026quot;Number of validation_files:\u0026quot; ,len(validation_file_names))\rprint(\u0026quot;Number of test_files:\u0026quot; ,len(test_file_names))\rNumber of train_files: 350\rNumber of validation_files: 50\rNumber of test_files: 100\rbatch_size = 32\rtrain_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], output_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rvalidation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\routput_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rtest_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\routput_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rNow create the model.\nfrom tensorflow.keras import layers\rmodel = tf.keras.Sequential([\rlayers.Conv2D(16, 3, activation = \u0026quot;relu\u0026quot;, input_shape = (31,33,1)),\rlayers.MaxPool2D(2),\rlayers.Conv2D(32, 3, activation = \u0026quot;relu\u0026quot;),\rlayers.MaxPool2D(2),\rlayers.Flatten(),\rlayers.Dense(16, activation = \u0026quot;relu\u0026quot;),\rlayers.Dense(5, activation = \u0026quot;softmax\u0026quot;)\r])\rmodel.summary()\rModel: \u0026quot;sequential\u0026quot;\r_________________________________________________________________\rLayer (type) Output Shape Param # =================================================================\rconv2d (Conv2D) (None, 29, 31, 16) 160 _________________________________________________________________\rmax_pooling2d (MaxPooling2D) (None, 14, 15, 16) 0 _________________________________________________________________\rconv2d_1 (Conv2D) (None, 12, 13, 32) 4640 _________________________________________________________________\rmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 32) 0 _________________________________________________________________\rflatten (Flatten) (None, 1152) 0 _________________________________________________________________\rdense (Dense) (None, 16) 18448 _________________________________________________________________\rdense_1 (Dense) (None, 5) 85 =================================================================\rTotal params: 23,333\rTrainable params: 23,333\rNon-trainable params: 0\r_________________________________________________________________\rCompile the model.\nmodel.compile(loss = \u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer = \u0026quot;adam\u0026quot;, metrics = [\u0026quot;accuracy\u0026quot;])\rBefore we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments steps_per_epoch and validation_steps to desired numbers in model.fit(). Similarly while evaluating model, we need to set the argument steps to a desired number in model.evaluate().\nThere are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set steps_per_epoch to 35. Similarly, validation_steps = 5 and in model.evaluate(), steps = 10.\nsteps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))\rvalidation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))\rsteps = np.int(np.ceil(len(test_file_names)/batch_size))\rprint(\u0026quot;steps_per_epoch = \u0026quot;, steps_per_epoch)\rprint(\u0026quot;validation_steps = \u0026quot;, validation_steps)\rprint(\u0026quot;steps = \u0026quot;, steps)\rsteps_per_epoch = 11\rvalidation_steps = 2\rsteps = 4\rmodel.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\rvalidation_steps = validation_steps, epochs = 5)\rEpoch 1/5\r11/11 [==============================] - 64s 6s/step - loss: 1.6222 - accuracy: 0.1486 - val_loss: 1.6067 - val_accuracy: 0.1800\rEpoch 2/5\r11/11 [==============================] - 65s 6s/step - loss: 1.6088 - accuracy: 0.2200 - val_loss: 1.6078 - val_accuracy: 0.2000\rEpoch 3/5\r11/11 [==============================] - 65s 6s/step - loss: 1.6090 - accuracy: 0.2029 - val_loss: 1.6088 - val_accuracy: 0.2000\rEpoch 4/5\r11/11 [==============================] - 65s 6s/step - loss: 1.6003 - accuracy: 0.2886 - val_loss: 1.6075 - val_accuracy: 0.1800\rEpoch 5/5\r11/11 [==============================] - 66s 6s/step - loss: 1.5956 - accuracy: 0.3229 - val_loss: 1.6073 - val_accuracy: 0.2200\r\u0026lt;tensorflow.python.keras.callbacks.History at 0x7fc3b818dc50\u0026gt;\rtest_loss, test_accuracy = model.evaluate(test_dataset, steps = steps)\r4/4 [==============================] - 13s 3s/step - loss: 1.6098 - accuracy: 0.2000\rprint(\u0026quot;Test loss: \u0026quot;, test_loss)\rprint(\u0026quot;Test accuracy:\u0026quot;, test_accuracy)\rTest loss: 1.6098381280899048\rTest accuracy: 0.20000000298023224\rAs expected, model performs terribly.\n\n\rHow to make the code run faster?\rIf no transformations are used, just using prefetch might improve performance. In deep learning usually GPUs are used for training. But all the data processing is done in CPU. In the naive approach, we will first process data in CPU, then send the processed data to GPU and after training finishes, we will prepare another batch of data. This approach is not efficient because GPU has to wait for data to get prepared. But using prefetch, we prepare and keep ready batches of data while training continues. In this way, waiting time of GPU is minimized.\nWhen data transformations are used, out aim should always be to use parallel processing capabilities of tensorflow. We can achieve this using map function. Inside the map function, all transformations are defined. Then we can prefetch batches to further improve performance. The whole pipeline is as follows.\n1. def transformation_function(...):\r# Define all transormations (STFT, Normalization, etc.)\r2. def generator(...):\r# Read data\r# Call transformation_function using tf.data.Dataset.map so that it can parallelize operations.\r# Finally yield the processed data\r3. Create tf.data.Dataset s.\r4. Prefecth datasets.\r5. Create model and train it.\rWe will use one extra library tensorflow_datasets that will allow us to switch from tf.dataset to numpy. If tensorflow_datasets is not installed in your system, use pip install tensorflow-datasets to install it and then run following codes.\nimport tensorflow_datasets as tfds\rdef data_transformation_func(data):\rtransformed_data = tf.math.abs(tf.signal.stft(data,frame_length = 64, frame_step = 32, fft_length = 64))\rtransformed_data = tf.image.per_image_standardization(tf.reshape(transformed_data, shape = (-1,31,33,1))) # Normalization\rreturn transformed_data\rdef tf_data_generator_new(file_list, batch_size = 4):\ri = 0\rwhile True:\rif i*batch_size \u0026gt;= len(file_list): i = 0\rnp.random.shuffle(file_list)\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size]\rdata = []\rlabels = []\rlabel_classes = tf.constant([\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]) for file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;)).astype(np.float32) # Read data\rdata.append(tf.reshape(temp.values, shape = (1,1024)))\rpattern = tf.constant(eval(\u0026quot;file[22:29]\u0026quot;))\rfor j in range(len(label_classes)):\rif re.match(pattern.numpy(), label_classes[j].numpy()): labels.append(j)\rdata = np.asarray(data)\rlabels = np.asarray(labels)\rfirst_dim = data.shape[0]\r# Create tensorflow dataset so that we can use `map` function that can do parallel computation.\rdata_ds = tf.data.Dataset.from_tensor_slices(data)\rdata_ds = data_ds.batch(batch_size = first_dim).map(data_transformation_func,\rnum_parallel_calls = tf.data.experimental.AUTOTUNE)\r# Convert the dataset to a generator and subsequently to numpy array\rdata_ds = tfds.as_numpy(data_ds) # This is where tensorflow-datasets library is used.\rdata = np.asarray([data for data in data_ds]).reshape(first_dim,31,33,1)\ryield data, labels\ri = i + 1\rtrain_file_names[:10]\r[\u0026#39;./random_data/Fault_3/Fault_3_045.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_032.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_025.csv\u0026#39;,\r\u0026#39;./random_data/Fault_2/Fault_2_013.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_053.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_087.csv\u0026#39;,\r\u0026#39;./random_data/Fault_5/Fault_5_053.csv\u0026#39;,\r\u0026#39;./random_data/Fault_4/Fault_4_019.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_034.csv\u0026#39;,\r\u0026#39;./random_data/Fault_2/Fault_2_044.csv\u0026#39;]\rtrain_file_names[0][22:29]\r\u0026#39;Fault_3\u0026#39;\rbatch_size = 20\rdataset_check = tf.data.Dataset.from_generator(tf_data_generator_new,args= [train_file_names, batch_size],output_types = (tf.float32, tf.float32),\routput_shapes = ((None,31,33,1),(None,)))\rfor data, labels in dataset_check.take(7):\rprint(data.shape)\rprint(labels)\r(20, 31, 33, 1)\rtf.Tensor([2. 0. 0. 1. 2. 0. 4. 3. 2. 1. 1. 0. 3. 3. 2. 3. 1. 4. 2. 4.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([3. 1. 1. 3. 4. 4. 2. 3. 4. 3. 3. 0. 1. 2. 0. 3. 2. 2. 2. 4.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([2. 3. 0. 2. 2. 4. 3. 0. 4. 1. 0. 0. 2. 0. 0. 1. 0. 3. 2. 1.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([4. 2. 2. 2. 0. 3. 4. 2. 0. 1. 2. 2. 3. 4. 0. 4. 2. 0. 4. 4.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([1. 0. 4. 4. 0. 1. 0. 4. 0. 2. 1. 4. 3. 2. 1. 4. 4. 2. 4. 3.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([2. 2. 0. 1. 3. 2. 2. 2. 1. 3. 3. 4. 0. 1. 4. 1. 3. 2. 1. 3.], shape=(20,), dtype=float32)\r(20, 31, 33, 1)\rtf.Tensor([2. 1. 2. 2. 4. 4. 1. 0. 2. 2. 1. 2. 3. 0. 0. 2. 2. 0. 3. 3.], shape=(20,), dtype=float32)\rbatch_size = 32\rtrain_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [train_file_names, batch_size], output_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rvalidation_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [validation_file_names, batch_size],\routput_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rtest_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [test_file_names, batch_size],\routput_shapes = ((None,31,33,1),(None,)),\routput_types = (tf.float32, tf.float32))\rPrefetch datasets.\ntrain_dataset_new = train_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\rvalidation_dataset_new = validation_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\rmodel.compile(loss = \u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer = \u0026quot;adam\u0026quot;, metrics = [\u0026quot;accuracy\u0026quot;])\rmodel.fit(train_dataset_new, validation_data = validation_dataset_new, steps_per_epoch = steps_per_epoch,\rvalidation_steps = validation_steps, epochs = 5)\rEpoch 1/5\r11/11 [==============================] - 45s 4s/step - loss: 1.5939 - accuracy: 0.2714 - val_loss: 1.6075 - val_accuracy: 0.2000\rEpoch 2/5\r11/11 [==============================] - 46s 4s/step - loss: 1.5890 - accuracy: 0.2886 - val_loss: 1.6082 - val_accuracy: 0.2600\rEpoch 3/5\r11/11 [==============================] - 44s 4s/step - loss: 1.5771 - accuracy: 0.3257 - val_loss: 1.6066 - val_accuracy: 0.2000\rEpoch 4/5\r11/11 [==============================] - 44s 4s/step - loss: 1.5710 - accuracy: 0.4400 - val_loss: 1.6057 - val_accuracy: 0.2200\rEpoch 5/5\r11/11 [==============================] - 45s 4s/step - loss: 1.5564 - accuracy: 0.3771 - val_loss: 1.6074 - val_accuracy: 0.1600\r\u0026lt;tensorflow.python.keras.callbacks.History at 0x7fc398126090\u0026gt;\rtest_loss_new, test_acc_new = model.evaluate(test_dataset_new, steps = steps)\r4/4 [==============================] - 6s 2s/step - loss: 1.6097 - accuracy: 0.1700\r\n\rHow to make predictions?\rIn the generator used for prediction, we can also use map function to parallelize data preprocessing. But in practice, inference is much faster. So we can make fast predictions using naive method also. We show the naive implementation below.\ndef create_prediction_set(num_files = 20):\ros.mkdir(\u0026quot;./random_data/prediction_set\u0026quot;)\rfor i in range(num_files):\rdata = np.random.randn(1024,)\rfile_name = \u0026quot;./random_data/prediction_set/\u0026quot; + \u0026quot;file_\u0026quot; + \u0026quot;{0:03}\u0026quot;.format(i+1) + \u0026quot;.csv\u0026quot; # This creates file_name\rnp.savetxt(eval(\u0026quot;file_name\u0026quot;), data, delimiter = \u0026quot;,\u0026quot;, header = \u0026quot;V1\u0026quot;, comments = \u0026quot;\u0026quot;)\rprint(str(eval(\u0026quot;num_files\u0026quot;)) + \u0026quot; \u0026quot;+ \u0026quot; files created in prediction set.\u0026quot;)\rCreate some files for prediction set.\ncreate_prediction_set(num_files = 55)\r55 files created in prediction set.\rprediction_files = glob.glob(\u0026quot;./random_data/prediction_set/*\u0026quot;)\rprint(\u0026quot;Total number of files: \u0026quot;, len(prediction_files))\rprint(\u0026quot;Showing first 10 files...\u0026quot;)\rprediction_files[:10]\rTotal number of files: 55\rShowing first 10 files...\r[\u0026#39;./random_data/prediction_set/file_001.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_002.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_003.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_004.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_005.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_006.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_007.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_008.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_009.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_010.csv\u0026#39;]\rNow, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only yield data. This is how we achieve that.\ndef generator_for_prediction(file_list, batch_size = 20):\ri = 0\rwhile i \u0026lt;= (len(file_list)/batch_size):\rif i == np.floor(len(file_list)/batch_size):\rfile_chunk = file_list[i*batch_size:len(file_list)]\rif len(file_chunk)==0:\rbreak\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size] data = []\rfor file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;)).astype(np.float32)\rtemp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))\r# After STFT transformation with given parameters, shape = (31,33)\rtemp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization\rdata.append(temp) data = np.asarray(data).reshape(-1,31,33,1)\ryield data\ri = i + 1\rCheck whether the generator works or not.\npred_gen = generator_for_prediction(prediction_files, batch_size = 10)\rfor data in pred_gen:\rprint(data.shape)\r(10, 31, 33, 1)\r(10, 31, 33, 1)\r(10, 31, 33, 1)\r(10, 31, 33, 1)\r(10, 31, 33, 1)\r(5, 31, 33, 1)\rCreate a tensorflow dataset.\nbatch_size = 10\rprediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],\routput_shapes=(None,31,33,1), output_types=(tf.float32))\rsteps = np.int(np.ceil(len(prediction_files)/batch_size))\rpredictions = model.predict(prediction_dataset,steps = steps)\rprint(\u0026quot;Shape of prediction array: \u0026quot;, predictions.shape)\rpredictions\rShape of prediction array: (55, 5)\rarray([[0.13783312, 0.06810743, 0.18828638, 0.4399181 , 0.16585506],\r[0.2011155 , 0.0909321 , 0.12722781, 0.34147328, 0.23925126],\r[0.184051 , 0.11195082, 0.15630874, 0.41264012, 0.13504937],\r[0.17021744, 0.15275575, 0.17176864, 0.36582083, 0.13943738],\r[0.22107455, 0.13893652, 0.16182247, 0.22719847, 0.25096804],\r[0.16544239, 0.09297101, 0.19448881, 0.37793893, 0.16915883],\r[0.20981115, 0.09095117, 0.1454936 , 0.37553373, 0.17821036],\r[0.18948458, 0.08287238, 0.16043249, 0.31469837, 0.25251222],\r[0.14806318, 0.08988151, 0.18063019, 0.43348154, 0.14794365],\r[0.19300967, 0.17423573, 0.1853214 , 0.29504803, 0.15238515],\r[0.14796554, 0.10064519, 0.17332935, 0.46094754, 0.11711246],\r[0.1620164 , 0.10878453, 0.19735815, 0.28250632, 0.2493346 ],\r[0.17244144, 0.13593125, 0.18931074, 0.3498449 , 0.1524716 ],\r[0.16827711, 0.08276799, 0.16664039, 0.38747287, 0.19484173],\r[0.16345006, 0.1138956 , 0.17773166, 0.39695117, 0.14797151],\r[0.17923051, 0.12203053, 0.20120224, 0.23441198, 0.26312473],\r[0.1487248 , 0.09016878, 0.17162901, 0.43704256, 0.15243487],\r[0.16879848, 0.05954535, 0.14414911, 0.45952848, 0.16797861],\r[0.14453672, 0.11703113, 0.19364771, 0.4488474 , 0.09593706],\r[0.20345339, 0.1580022 , 0.17898531, 0.22838299, 0.23117617],\r[0.16221416, 0.05681619, 0.14693654, 0.39674726, 0.23728591],\r[0.200225 , 0.16417584, 0.18793206, 0.26401222, 0.18365487],\r[0.21399722, 0.13131607, 0.17154819, 0.21897295, 0.26416558],\r[0.1392046 , 0.05873166, 0.1671688 , 0.45915708, 0.17573787],\r[0.16581263, 0.08813614, 0.18449506, 0.3109786 , 0.25057748],\r[0.15438257, 0.11544537, 0.19926623, 0.37426034, 0.15664549],\r[0.15011945, 0.08316098, 0.1305757 , 0.5280127 , 0.10813113],\r[0.18201515, 0.12078299, 0.17261833, 0.29602036, 0.22856328],\r[0.17936407, 0.09680162, 0.17545709, 0.2755434 , 0.27283388],\r[0.19463587, 0.11394399, 0.17677711, 0.3607436 , 0.15389942],\r[0.1834404 , 0.07998151, 0.16563387, 0.38610289, 0.18484135],\r[0.20745523, 0.14513774, 0.18552025, 0.2872524 , 0.1746344 ],\r[0.18435965, 0.15455365, 0.19811183, 0.28113118, 0.18184367],\r[0.19053918, 0.13114992, 0.18859585, 0.28579548, 0.20391957],\r[0.1874934 , 0.13049673, 0.15486516, 0.4116317 , 0.11551296],\r[0.16247028, 0.09362978, 0.15978761, 0.41194388, 0.17216852],\r[0.16956635, 0.06130224, 0.13529454, 0.41197857, 0.2218583 ],\r[0.17781247, 0.14515027, 0.17571223, 0.34297863, 0.15834646],\r[0.18537633, 0.14020114, 0.17088792, 0.30255666, 0.20097792],\r[0.21270326, 0.13349937, 0.1523133 , 0.263564 , 0.23792005],\r[0.19857787, 0.07489564, 0.1436915 , 0.29431146, 0.2885235 ],\r[0.18926036, 0.11828965, 0.17655246, 0.24165332, 0.2742442 ],\r[0.18234053, 0.082383 , 0.16731356, 0.30818883, 0.25977406],\r[0.17025198, 0.08521349, 0.16441138, 0.41964525, 0.16047782],\r[0.17940679, 0.09788707, 0.15743247, 0.35294068, 0.21233296],\r[0.11456501, 0.05288012, 0.16385278, 0.5418783 , 0.12682377],\r[0.15863904, 0.06855461, 0.16147587, 0.40622538, 0.2051051 ],\r[0.19545631, 0.08327787, 0.13592716, 0.38202846, 0.20331012],\r[0.17398484, 0.14876288, 0.18257992, 0.33674046, 0.15793191],\r[0.21319063, 0.08506136, 0.15001011, 0.37536374, 0.17637412],\r[0.20356631, 0.21442604, 0.20090103, 0.22577564, 0.15533103],\r[0.18141419, 0.11649938, 0.18554828, 0.23423648, 0.2823017 ],\r[0.15753253, 0.10006633, 0.18498763, 0.36755162, 0.18986186],\r[0.18776654, 0.11064088, 0.20178466, 0.2612361 , 0.23857178],\r[0.20099026, 0.14279291, 0.15887792, 0.2843657 , 0.21297309]],\rdtype=float32)\rOutputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use np.argmax() command.\nnp.argmax(predictions, axis = 1)\rarray([3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3,\r4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\r3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3])\rAs a final comment, read the note at the beginning of this post.\n\r","date":1589673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589673600,"objectID":"1559bc3c805724c8ed3df1e4d4484b49","permalink":"/post/efficiently-reading-multiple-files-in-tensorflow-2/","publishdate":"2020-05-17T00:00:00Z","relpermalink":"/post/efficiently-reading-multiple-files-in-tensorflow-2/","section":"post","summary":"Note: Whether this method is efficient or not is contestable. Efficiency of a pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they run this method on their own system. The system on which we ran this notebook has 44 CPU cores.","tags":["Tensorflow","Deep Learning"],"title":"Efficiently reading multiple files in Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog"],"content":"\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will explore the ways of doing linear algebra only using tensorflow. We will only import tensorflow and nothing else. As we will see, we can do all the common linear algebra operations without using any other library. This post is very long as it covers almost all the functions that are there in the linear algebra library tf.linalg. But this is not a copy of tensorflow documentation. Rather, the tensorflow documentation is a super set of what has been discussed here. This post also assumes that readers have a working knowledge of linear algebra. Most of the times, we will give examples to illustrate a function without going into the underlying theory. Interested readers should use the contents to browse relevant sections of their interest.\n\rBasics\r\rCreating tensors\rCreating a sequence of numbers\rSlicing\rModifying elements of a matrix\rCreating a complex matrix\rTranspose of a matrix\r\rTranspose of a real matrix\rTranspose of a complex matrix\r\rSome common matrices\r\rIdentity matrix\rDiagonal matrix\rTri-diagonal matrix\rMatrix of all zeros and ones\rRandom matrices\r\rRandom uniform matrix\rRandom normal matrix\rTruncated random normal matrix\rRandom Poisson matrix\rRandom gamma matrix\r\rSome special matrices\r\rSparse matrices\rMatrix multiplication\r\rMultiplying two column vectors\r\rInner product\rOuter product\r\rMultiplying a matrix with a vector\rMultiplying two matrices\rMultiplying two tri-diagonal matrices\r\rSome common operations on matrices\r\rTrace\rDeterminant\rRank\rMatrix inverse\rExtract diagonals of a matrix\rExtract band part of a matrix\r\r\rMatrix factorizations\r\rLU\rCholesky\rQR\rSVD\r\rEigenvalues and eigenvectors\r\rEigen-analysis of Hermitian matrices\rEigen-analysis of other matrices\r\rSolving linear systems\r\rUsing LU decomposition\rUsing Cholesky decomposition\rSolving tri-diagonal systems\r\rSolving least squares problems\r\rOrdinary least squares\rRegularized least squares\r\rSome specialized operations\r\rNorm\rNormalizing a tensor\rGlobal norm\rCross product of vectors\rMatrix_square_root\rMatrix exponential\rMatrix logarithm\rLog-determinant of a matrix\rPseudo inverse of a matrix\r\rLinear operators\r\rCommon methods on linear operators\rSpecial matrices using operators\r\rToeplitz matrix\rCirculant matrix\rBlock diagonal matrix\rBlock lower triangular matrix\rHouseholder matrix\rKronecker matrix\rPermutation matrix\r\rCommon matrices using operators\r\rIdentity matrix\rScaled identity matrix\rDiagonal matrix\rTri-diagonal matrix\rLower triangular matrix\rMatrix of zeros\r\rMatrix operations using operators\r\rLow-rank update\rOperator inversion\rOperator composition\r\r\rConclusion\r\rimport tensorflow as tf\rprint(tf.__version__)\r2.2.0\rOne thing we have to keep in mind is that while accessing a function, we have to always append the function by tf.linalg. It is possible to remove the tf part by importing the linalg library from tensorflow. But even then we have to append every function by linalg. In this post, we will always use tf.linalg followed by function name. This amounts to little more typing. But we will do this to remind ourselves that we are using linalg library of tensorflow. This might seem little awkward to seasoned users of MATLAB or Julia where you just need to type the function name to use it without having to write the library name all the time. Except that, linear algebra in tensorflow seems quite natural.\nNote: In this post, we will show some of the ways in which we can handle matrix operations in Tensorflow. We will mainly use 1D or 2D arrays in our examples. But matrix operations in Tensorflow are not limited to 2D arrays. In fact, the operations can be done on multidimensional arrays. If an array has more than 2 dimensions, the matrix operation is done on the last two dimensions and the same operation is carried across other dimensions. For example, if our array has a shape of (3,5,5), it can be thought of as 3 matrices each of shape (5,5). When we call a matrix function on this array, the matrix function is applied to all 3 matrices of shape (5,5). This is also true for higher dimensional arrays.\n\nBasics\rTensorflow operates on Tensors. Tensors are characterized by their rank. Following table shows different types of tensors and their corresponding rank.\n\r\rTensors\rRank\r\r\r\rScalars\rRank 0 Tensor\r\rVectors (1D array)\rRank 1 Tensor\r\rMatrices (2D array)\rRank 2 Tensor\r\r3D array\rRank 3 Tensor\r\r\r\r\nCreating tensors\rIn this section, we will create tensors of different rank, starting from scalars to multi-dimensional arrays. Though tensors can be both real or complex, we will mainly focus on real tensors.\nA scalar contains a single (real or complex) value.\na = tf.constant(5.0)\ra\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0\u0026gt;\rThe output shows that the result is a tf.Tensor. As scalars are rank 0 tensors, its shape is empty. Data type of the tensor is float32. And corresponding numpy array is 5. We can get only the value of the tensor by calling numpy method.\na.numpy()\r5.0\rSimilarly, we can define 1D and 2D tensors. While 1D tensors are called vectors, 2D tensors are called matrices.\ntf.constant([1, 3, 7, 9]) # Note the shape in result. Only one shape parameter is used for vectors.\r\u0026lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 3, 7, 9], dtype=int32)\u0026gt;\rtf.constant([[1,2,3,4],\r[5,6,7,8]]) # Note the shape in result. There are two shape parameters (rows, columns).\r\u0026lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\rarray([[1, 2, 3, 4],\r[5, 6, 7, 8]], dtype=int32)\u0026gt;\rAnother way to define a 2D array is given below.\ntf.constant([1,2,3,4,5,6,7,8.0], shape = (2,4))\r\u0026lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=\rarray([[1., 2., 3., 4.],\r[5., 6., 7., 8.]], dtype=float32)\u0026gt;\r\n\rCreating a sequence of numbers\rThere are two ways to generate sequence of numbers in Tensorflow. Functions tf.range and tf.linspace can be used for that purpose. Sequences generated by these functions are equally spaced.\nsequence = tf.range(start = 1,limit = 10, delta = 1)\rsequence.numpy()\rarray([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\rNote that the last element (limit) is not included in the array. This is consistent with Python behavior but in departure with MATLAB and Julia convention. It is also possible to set delta to a fraction.\ntf.range(start = 1, limit = 10, delta = 1.5).numpy()\rarray([1. , 2.5, 4. , 5.5, 7. , 8.5], dtype=float32)\rtf.linspace(start = 1.0, stop = 10, num = 25) # Start must be a `float`. See documentation for more details.\r\u0026lt;tf.Tensor: shape=(25,), dtype=float32, numpy=\rarray([ 1. , 1.375, 1.75 , 2.125, 2.5 , 2.875, 3.25 , 3.625,\r4. , 4.375, 4.75 , 5.125, 5.5 , 5.875, 6.25 , 6.625,\r7. , 7.375, 7.75 , 8.125, 8.5 , 8.875, 9.25 , 9.625,\r10. ], dtype=float32)\u0026gt;\rThough in this post we will mainly focus on matrices, it is easy to create higher dimensional arrays in Tensorflow.\ntf.constant(tf.range(1,13), shape = (2,3,2))\r\u0026lt;tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\rarray([[[ 1, 2],\r[ 3, 4],\r[ 5, 6]],\r[[ 7, 8],\r[ 9, 10],\r[11, 12]]], dtype=int32)\u0026gt;\r\n\rSlicing\rSlicing is similar to that of numpy slicing. For vectors (rank 1 tensor with only one shape parameter), only one argument is passed that corresponds to the location of starting index and end index of sliced array. For matrices (rank 2 tensor with two shape parameters), two input arguments need to be passed. First one for rows and second one for columns.\nvector = tf.range(start = 1, limit = 10)\rvector\r\u0026lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\u0026gt;\rvector[3:7].numpy() \rarray([4, 5, 6, 7], dtype=int32)\rIndexing in tensorflow starts from zero. In the above example, start index is 3. So that corresponds to 4th element of the vector. And end index is not included. This is similar to Python convention.\nmatrix = tf.constant(tf.range(20, dtype = tf.float32), shape = (4,5))\rmatrix\r\u0026lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=\rarray([[ 0., 1., 2., 3., 4.],\r[ 5., 6., 7., 8., 9.],\r[10., 11., 12., 13., 14.],\r[15., 16., 17., 18., 19.]], dtype=float32)\u0026gt;\rmatrix[1:3, 2:4]\r\u0026lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\rarray([[ 7., 8.],\r[12., 13.]], dtype=float32)\u0026gt;\rmatrix[:3,2:] # Same behavior as numpy\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 2., 3., 4.],\r[ 7., 8., 9.],\r[12., 13., 14.]], dtype=float32)\u0026gt;\r\n\rModifying elements of a matrix\rTensors in tensorflow, once created, can’t be modified. So the following code segment will result in an error.\n\u0026gt;\u0026gt;\u0026gt; a = tf.constant([1,2,3,4])\r\u0026gt;\u0026gt;\u0026gt; a[2] = 5 # Error\rBut there is a way to modify values of a matrix. Instead of creating a tensor, we create a Variable. Variables work just like tensors with the added advantage that their values can be modified. So if we want to modify entries of our matrix at a later stage, we have to first create our matrix as a variable. Then we can do assignment using assign command.\nvariable_mat = tf.Variable(tf.constant(tf.range(12, dtype = tf.float32), shape = (3,4)))\rvariable_mat\r\u0026lt;tf.Variable \u0026#39;Variable:0\u0026#39; shape=(3, 4) dtype=float32, numpy=\rarray([[ 0., 1., 2., 3.],\r[ 4., 5., 6., 7.],\r[ 8., 9., 10., 11.]], dtype=float32)\u0026gt;\rvariable_mat[:2,2:4].assign(-1*tf.ones(shape = (2,2)))\rvariable_mat\r\u0026lt;tf.Variable \u0026#39;Variable:0\u0026#39; shape=(3, 4) dtype=float32, numpy=\rarray([[ 0., 1., -1., -1.],\r[ 4., 5., -1., -1.],\r[ 8., 9., 10., 11.]], dtype=float32)\u0026gt;\r\n\rCreating a complex matrix\rTo create a complex matrix, we have to first create the real part and imaginary part separately. Then both real and imaginary parts can be combined element wise to create a complex matrix. Elements of both real and imaginary part should be floats. This is the hard way of creating complex a complex matrix. We will discuss the simpler way next.\nreal_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)\rimag_part = tf.random.uniform(shape = (3,2), minval = 1, maxval = 5)\rprint(\u0026quot;Real part:\u0026quot;)\rprint(real_part)\rprint()\rprint(\u0026quot;Imaginary part:\u0026quot;)\rprint(imag_part)\rReal part:\rtf.Tensor(\r[[4.0592647 2.7457805]\r[4.5665903 2.4078012]\r[2.9457803 3.0998607]], shape=(3, 2), dtype=float32)\rImaginary part:\rtf.Tensor(\r[[4.0819793 1.4122672]\r[3.3663173 1.0939579]\r[2.2030935 1.1165142]], shape=(3, 2), dtype=float32)\rcomplex_mat = tf.dtypes.complex(real = real_part, imag = imag_part)\rprint(complex_mat)\rtf.Tensor(\r[[4.0592647+4.0819793j 2.7457805+1.4122672j]\r[4.5665903+3.3663173j 2.4078012+1.0939579j]\r[2.9457803+2.2030935j 3.0998607+1.1165142j]], shape=(3, 2), dtype=complex64)\rThere is a simpler way to create a complex matrix.\ncomplex_mat_2 = tf.constant([1+2j, 2+3j , 3+4j, 4+5j, 5+6j, 6+7j], shape = (2,3))\rcomplex_mat_2\r\u0026lt;tf.Tensor: shape=(2, 3), dtype=complex128, numpy=\rarray([[1.+2.j, 2.+3.j, 3.+4.j],\r[4.+5.j, 5.+6.j, 6.+7.j]])\u0026gt;\r\n\rTranspose of a matrix\r\nTranspose of a real matrix\rFor real matrices transpose just means changing the rows into columns and vice versa. There are three functions that achieve this.\n\rtf.transpose\rtf.adjoint\rtf.matrix_transpose\r\rFor real matrices, all three functions give identical results.\nmatrix\r\u0026lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=\rarray([[ 0., 1., 2., 3., 4.],\r[ 5., 6., 7., 8., 9.],\r[10., 11., 12., 13., 14.],\r[15., 16., 17., 18., 19.]], dtype=float32)\u0026gt;\rtf.transpose(matrix)\r\u0026lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=\rarray([[ 0., 5., 10., 15.],\r[ 1., 6., 11., 16.],\r[ 2., 7., 12., 17.],\r[ 3., 8., 13., 18.],\r[ 4., 9., 14., 19.]], dtype=float32)\u0026gt;\rtf.linalg.adjoint(matrix)\r\u0026lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=\rarray([[ 0., 5., 10., 15.],\r[ 1., 6., 11., 16.],\r[ 2., 7., 12., 17.],\r[ 3., 8., 13., 18.],\r[ 4., 9., 14., 19.]], dtype=float32)\u0026gt;\rtf.linalg.matrix_transpose(matrix)\r\u0026lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=\rarray([[ 0., 5., 10., 15.],\r[ 1., 6., 11., 16.],\r[ 2., 7., 12., 17.],\r[ 3., 8., 13., 18.],\r[ 4., 9., 14., 19.]], dtype=float32)\u0026gt;\r\n\rTranspose of a complex matrix\rThings are little different when we have a complex matrix. For complex matrices, we can take regular transpose or conjugate transpose if we want. Default is regular transpose. To take conjugate transpose, we have to set conjugate = False in tf.transpose and tf.linalg.matrix_transpose or use tf.linalg.adjoint function.\ncomplex_mat_2.numpy()\rarray([[1.+2.j, 2.+3.j, 3.+4.j],\r[4.+5.j, 5.+6.j, 6.+7.j]])\rtranspose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = False) # Regular transpose\rprint(transpose_of_complex_mat)\rtf.Tensor(\r[[1.+2.j 4.+5.j]\r[2.+3.j 5.+6.j]\r[3.+4.j 6.+7.j]], shape=(3, 2), dtype=complex128)\rconjugate_transpose_of_complex_mat = tf.transpose(complex_mat_2, conjugate = True) # Conjugate transpose\rprint(conjugate_transpose_of_complex_mat)\rtf.Tensor(\r[[1.-2.j 4.-5.j]\r[2.-3.j 5.-6.j]\r[3.-4.j 6.-7.j]], shape=(3, 2), dtype=complex128)\rWe can also do conjugate transpose by using function linalg.adjoint function.\ntf.linalg.adjoint(complex_mat_2).numpy()\rarray([[1.-2.j, 4.-5.j],\r[2.-3.j, 5.-6.j],\r[3.-4.j, 6.-7.j]])\rAnother way to take transpose of a matrix is to use the function linalg.matrix_transpose. In this function, we can set argument conjugate to True or False depending on whether we want regular transpose or conjugate transpose. Default is conjugate = False.\ntf.linalg.matrix_transpose(complex_mat_2) # Conjugate = False is the default\r\u0026lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\rarray([[1.+2.j, 4.+5.j],\r[2.+3.j, 5.+6.j],\r[3.+4.j, 6.+7.j]])\u0026gt;\rtf.linalg.matrix_transpose(complex_mat_2, conjugate = True)\r\u0026lt;tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\rarray([[1.-2.j, 4.-5.j],\r[2.-3.j, 5.-6.j],\r[3.-4.j, 6.-7.j]])\u0026gt;\r\n\r\rSome common matrices\r\nIdentity matrix\rtf.linalg.eye(5)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 0., 0., 0.],\r[0., 1., 0., 0., 0.],\r[0., 0., 1., 0., 0.],\r[0., 0., 0., 1., 0.],\r[0., 0., 0., 0., 1.]], dtype=float32)\u0026gt;\r\n\rDiagonal matrix\rtf.linalg.diag([1,2,3,4,5])\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[1, 0, 0, 0, 0],\r[0, 2, 0, 0, 0],\r[0, 0, 3, 0, 0],\r[0, 0, 0, 4, 0],\r[0, 0, 0, 0, 5]], dtype=int32)\u0026gt;\rTo create diagonal matrix, we can also use tf.linalg.tensor_diag with main diagonal as input.\ntf.linalg.tensor_diag(tf.constant([1,2,3,4,5.]))\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 0., 0., 0.],\r[0., 2., 0., 0., 0.],\r[0., 0., 3., 0., 0.],\r[0., 0., 0., 4., 0.],\r[0., 0., 0., 0., 5.]], dtype=float32)\u0026gt;\rWe can also create a matrix whose only nonzero entries are on its super-diagonals or sub-diagonals.\ntf.linalg.diag([1,2,3,4], k = 1) # Values in super-diagonal\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[0, 1, 0, 0, 0],\r[0, 0, 2, 0, 0],\r[0, 0, 0, 3, 0],\r[0, 0, 0, 0, 4],\r[0, 0, 0, 0, 0]], dtype=int32)\u0026gt;\rtf.linalg.diag([1,2,3,4], k = -1) # Values in sub-diagonal\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[0, 0, 0, 0, 0],\r[1, 0, 0, 0, 0],\r[0, 2, 0, 0, 0],\r[0, 0, 3, 0, 0],\r[0, 0, 0, 4, 0]], dtype=int32)\u0026gt;\rtf.linalg.diag([1,2,3,4,5], k = 0, padding_value = -1)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[ 1, -1, -1, -1, -1],\r[-1, 2, -1, -1, -1],\r[-1, -1, 3, -1, -1],\r[-1, -1, -1, 4, -1],\r[-1, -1, -1, -1, 5]], dtype=int32)\u0026gt;\rAnother way to create a diagonal matrix is by using tf.linalg.set_diag function.\nmat = tf.zeros(shape = (5,5))\rdiag = tf.constant([1,2,3,4,5.])\rtf.linalg.set_diag(input = mat, diagonal = diag, k = 0)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 0., 0., 0.],\r[0., 2., 0., 0., 0.],\r[0., 0., 3., 0., 0.],\r[0., 0., 0., 4., 0.],\r[0., 0., 0., 0., 5.]], dtype=float32)\u0026gt;\rtf.linalg.set_diag(mat, tf.constant([1,2,3,4.]), k = 1) # Set super-diagonal\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[0., 1., 0., 0., 0.],\r[0., 0., 2., 0., 0.],\r[0., 0., 0., 3., 0.],\r[0., 0., 0., 0., 4.],\r[0., 0., 0., 0., 0.]], dtype=float32)\u0026gt;\rdiags = tf.constant([[1,2,3,4,5],\r[6,7,8,9,0.]])\rtf.linalg.set_diag(mat, diags, k = (-1,0))\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 0., 0., 0.],\r[6., 2., 0., 0., 0.],\r[0., 7., 3., 0., 0.],\r[0., 0., 8., 4., 0.],\r[0., 0., 0., 9., 5.]], dtype=float32)\u0026gt;\rIn the next section, we will see a way to create tri-diagonal matrix using tf.linalg.set_diag.\n\n\rTri-diagonal matrix\rLet’s create Gilbert Strang’s favorite matrix.\ntf.linalg.diag(tf.repeat(2,repeats = 5)) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = -1) + tf.linalg.diag(tf.repeat(-1, repeats = 4), k = 1)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[ 2, -1, 0, 0, 0],\r[-1, 2, -1, 0, 0],\r[ 0, -1, 2, -1, 0],\r[ 0, 0, -1, 2, -1],\r[ 0, 0, 0, -1, 2]], dtype=int32)\u0026gt;\rUsing tf.linalg.set_diag. While setting more that one diagonals using set_diag, if k = (-2,3), we have to have 6 diagonals (2 sub-diagonals, 1 main diagonal, and 3 super-diagonals). First three rows of the input diagonals will correspond to super-diagonals and have to be appended at the right by zeros. Fourth row corresponds to main diagonal. Last two rows correspond to sub-diagonals and have to be appended at the left by zeros. This type of appending works when align=\"LEFT_RIGHT\". We chose this alignment strategy as it is consistent with tri-diagonal matrix convention, to be discussed in later sections.\ndiags = tf.constant([[-1,-1,-1,-1, 0],\r[ 2, 2, 2, 2, 2],\r[ 0,-1,-1,-1,-1]], dtype = tf.float32)\rmat = tf.zeros(shape = (5,5))\rtf.linalg.set_diag(mat,diags, k = (-1,1), align = \u0026quot;LEFT_RIGHT\u0026quot;)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 2., -1., 0., 0., 0.],\r[-1., 2., -1., 0., 0.],\r[ 0., -1., 2., -1., 0.],\r[ 0., 0., -1., 2., -1.],\r[ 0., 0., 0., -1., 2.]], dtype=float32)\u0026gt;\rThere is yet another simpler way to create a tri-diagonal matrix using a linear operator. We will see that technique in a later section.\n\n\rMatrix of all zeros and ones\rMatrices of all 1s or all 0s are not in linalg library. But those are available in core Tensorflow.\ntf.zeros(shape = (3,5), dtype = tf.float32)\r\u0026lt;tf.Tensor: shape=(3, 5), dtype=float32, numpy=\rarray([[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.]], dtype=float32)\u0026gt;\rtf.ones(shape = (5,4), dtype = tf.int32)\r\u0026lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=\rarray([[1, 1, 1, 1],\r[1, 1, 1, 1],\r[1, 1, 1, 1],\r[1, 1, 1, 1],\r[1, 1, 1, 1]], dtype=int32)\u0026gt;\r\n\rRandom matrices\rRandom matrices are also not part of linalg library. Rather, they are part of tf.random library. Using Tensorflow we can create matrices whose entries come from normal, uniform, poisson, and gamma distributions.\n\nRandom uniform matrix\rtf.random.uniform(shape = (5,5), minval = 0, maxval = 5, seed= 32)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[4.8578553 , 0.26324332, 1.7549878 , 4.434555 , 2.3975224 ],\r[3.219039 , 0.4039365 , 0.92039883, 2.9136662 , 4.9377174 ],\r[4.617196 , 3.6782126 , 4.0351195 , 4.8321657 , 4.206293 ],\r[2.3059547 , 4.922245 , 4.186061 , 2.1761923 , 0.88124394],\r[2.7422066 , 1.5948689 , 2.6099925 , 4.4901986 , 2.4033623 ]],\rdtype=float32)\u0026gt;\r# Random matrix of integers\runiform_int = tf.random.uniform(shape = (5,5), minval= 10, maxval = 20, dtype = tf.int32, seed = 1234)\runiform_int\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[19, 15, 13, 14, 10],\r[16, 18, 10, 15, 10],\r[12, 13, 19, 12, 16],\r[18, 11, 10, 18, 12],\r[17, 18, 14, 19, 10]], dtype=int32)\u0026gt;\rFor further processing we usually require matrix entries to be floating point numbers. This can be achieved by using tf.cast.\ntf.dtypes.cast(uniform_int, dtype = tf.float32)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[19., 15., 13., 14., 10.],\r[16., 18., 10., 15., 10.],\r[12., 13., 19., 12., 16.],\r[18., 11., 10., 18., 12.],\r[17., 18., 14., 19., 10.]], dtype=float32)\u0026gt;\r\n\rRandom normal matrix\rtf.random.normal(shape = (5,5), mean = 1, stddev= 3, seed = 253)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 1.5266892 , -5.114835 , 4.4653835 , -1.013567 , -1.1874261 ],\r[ 5.503375 , -1.4568713 , -1.3270268 , 0.2747649 , 3.1374507 ],\r[ 4.211556 , 4.618066 , 1.2217634 , 0.04707384, 1.4131291 ],\r[-2.7024255 , 0.81293994, -3.11763 , -3.043394 , 5.5663233 ],\r[ 1.4549919 , 3.7368293 , 1.2184538 , 2.0713992 , 0.19450545]],\rdtype=float32)\u0026gt;\r\n\rTruncated random normal matrix\rtruncated_normal function gives values within two standard deviations of mean on both sides of normal curve.\ntf.random.truncated_normal(shape = (5,5), mean = 0, stddev= 2, seed = 82) \r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 2.3130474 , 1.917585 , 1.1134342 , -3.6221776 , -2.242488 ],\r[ 2.8108876 , -1.8440692 , 1.7630143 , -0.4591654 , -0.20763761],\r[-0.4769438 , 2.3582413 , -0.45690525, -0.4208855 , -1.8990422 ],\r[-2.2638845 , 2.9536312 , 0.9591611 , 2.670887 , 1.4793464 ],\r[-0.60492915, 3.6320126 , 3.9752324 , -0.4684417 , -3.2791114 ]],\rdtype=float32)\u0026gt;\rThere are ways to create deterministic random numbers using stateless_normal, stateless_uniform, etc. To know more about random number generation in Tensorflow, go to this link\n\n\rRandom Poisson matrix\rtf.random.poisson(shape = (5,5), lam = 2, seed = 12)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 1., 2., 3.],\r[0., 1., 2., 3., 4.],\r[2., 0., 2., 2., 2.],\r[2., 0., 2., 2., 3.],\r[1., 4., 2., 5., 4.]], dtype=float32)\u0026gt;\r\n\rRandom gamma matrix\rtf.random.gamma(shape = (5,5), alpha = 0.7, beta= 0.3, seed = 232)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[0.78733766, 2.5200539 , 0.9812998 , 5.141082 , 1.9184761 ],\r[1.1069427 , 0.32923967, 0.13172682, 5.066955 , 2.8487072 ],\r[0.39204285, 0.53647757, 5.3083944 , 1.618826 , 0.41352856],\r[1.0327125 , 0.27330002, 0.34577194, 0.22123706, 0.77021873],\r[0.38616025, 9.153643 , 1.4737413 , 6.029133 , 0.05517024]],\rdtype=float32)\u0026gt;\r\n\r\rSome special matrices\rSpecial matrices like toeplitz, circulant, Kronecker, etc can be created using linear operators. We will discuss this in the linear operator section.\n\n\r\rSparse matrices\rSparse matrices are within tf.sparse library. There are several functions specifically designed for sparse matrices. Full list of function in tf.sparse library can be found at this link. In this section, we will see how sparse matrices are created. The first argument is set of indices (rows and columns), second argument is the values at those indices. Third argument is the dense_shape of the sparse matrix.\nsparse_mat = tf.sparse.SparseTensor([[0,1],[1,3],[3,2]], [-5, -10, 7], dense_shape= (5,5))\rsparse_mat\r\u0026lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fdc6168d110\u0026gt;\rTo see the actual matrix, we have to convert the sparse matrix to a dense matrix. This is achieved using to_dense function.\ntf.sparse.to_dense(sparse_mat)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[ 0, -5, 0, 0, 0],\r[ 0, 0, 0, -10, 0],\r[ 0, 0, 0, 0, 0],\r[ 0, 0, 7, 0, 0],\r[ 0, 0, 0, 0, 0]], dtype=int32)\u0026gt;\rIt should be noted that special algorithms exist to deal with sparse matrices. Those algorithms don’t require the sparse matrix to be converted into its dense equivalent. By converting a sparse matrix into a dense one, all its special properties are lost. Therefore, sparse matrices should not be converted into dense ones.\n\n\rMatrix multiplication\rTo multiply two vectors, or two matrices, or a matrix with a vector in a linear algebra sense, we have to use linalg.matmul function. Using * operator in python does element wise multiplication with broadcasting wherever possible. So to multiply two matrices, we have to call linalg.matmul function. Inputs to linalg.matmul function are matrices. Therefore, while multiplying two arrays, we have to first convert them into vectors and then multiply. Also note that linalg.matmul is same as tf.matmul. Both are aliases.\n\nMultiplying two column vectors\rVectors in tensorflow have only 1 shape parameter, where as a column vector (a matrix with one column) has two shape parameters. For example, a vector \\([1,2,3]\\) has shape \\((3,)\\), but the column vector \\([1,2,3]^T\\) has shape \\((3,1)\\).\n\nInner product\rvector_1 = tf.constant([1., 2., 3.], shape = (3,1))\rvector_2 = tf.constant([2., 3., 4.], shape = (3,1))\rresult = tf.matmul(a = vector_1, b = vector_2, transpose_a=True) # Inner product\rresult.numpy()\rarray([[20.]], dtype=float32)\r\n\rOuter product\rtf.matmul(a = vector_1, b = vector_2, transpose_b = True) # Outer product\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 2., 3., 4.],\r[ 4., 6., 8.],\r[ 6., 9., 12.]], dtype=float32)\u0026gt;\r\n\r\rMultiplying a matrix with a vector\rThere are two ways in which we can achieve this. We can convert the vector into a column vector (matrix with 1 column) and then apply tf.matmul, or we can use the inbuilt function tf.linalg.matvec to multiply a matrix with a vector.\nmat_1 = tf.constant([1,2,3,4,5,6],shape = (2,3), dtype = tf.float32)\rmat_1.numpy()\rarray([[1., 2., 3.],\r[4., 5., 6.]], dtype=float32)\rtf.matmul(a = mat_1, b = vector_1).numpy()\rarray([[14.],\r[32.]], dtype=float32)\rtf.linalg.matvec(mat_1, tf.constant([1,2,3.])) # Note the shape of input vector and result.\r\u0026lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([14., 32.], dtype=float32)\u0026gt;\r\n\rMultiplying two matrices\rtf.linalg.matmul(a = mat, b = mat, transpose_a=True).numpy() # Without `transpose_a` argument, result will be an error.\rarray([[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.]], dtype=float32)\rtf.linalg.matmul(a = mat, b = mat, transpose_b=True).numpy()\rarray([[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.]], dtype=float32)\r\n\rMultiplying two tri-diagonal matrices\rIf matrices have some sparse structure, usual matrix multiplication is not an efficient method for those type of matrices. Special algorithms are there that exploit the sparsity of the matrices.\nOne such sparse matrix is tri-diagonal matrix. It has nonzero entries only on its super-diagonal, main diagonal, and sub-diagonal. To multiply a tri-diagonal matrix with another matrix, we can use tf.linalg.tridiagonal_matmul function.\ndiagonals = tf.constant([[-1,-1,-1,-1,0],\r[ 2, 2, 2, 2, 2],\r[ 0,-1,-1,-1,-1.]])\rrhs = tf.constant([[1,2,3],\r[2,1,3],\r[4,5,6],\r[7,8,9],\r[2,5,4.]])\rtf.linalg.tridiagonal_matmul(diagonals, rhs) \r\u0026lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=\rarray([[ 0., 3., 3.],\r[-1., -5., -3.],\r[-1., 1., 0.],\r[ 8., 6., 8.],\r[-3., 2., -1.]], dtype=float32)\u0026gt;\rWe can verify the result by dense matrix multiplication. However, note that this is only for verification. For large matrix multiplications involving tri-diagonal matrix, dense multiplication will be considerably slower.\ntridiag_mat = tf.linalg.set_diag(tf.zeros(shape = (5,5)), diagonals, k = (-1,1), align = \u0026quot;LEFT_RIGHT\u0026quot;)\rtf.matmul(tridiag_mat, rhs)\r\u0026lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=\rarray([[ 0., 3., 3.],\r[-1., -5., -3.],\r[-1., 1., 0.],\r[ 8., 6., 8.],\r[-3., 2., -1.]], dtype=float32)\u0026gt;\rHow to multiply two tri-diagonal matrices?\nIn this case, we have to convert the right tri-diagonal matrix into a full matrix and then multiply it with the left one using only the diagonals of left tri-diagonal matrix. For example, we will multiply the previous tri-diagonal matrix with itself.\ntf.linalg.tridiagonal_matmul(diagonals, rhs = tridiag_mat)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 5., -4., 1., 0., 0.],\r[-4., 6., -4., 1., 0.],\r[ 1., -4., 6., -4., 1.],\r[ 0., 1., -4., 6., -4.],\r[ 0., 0., 1., -4., 5.]], dtype=float32)\u0026gt;\rtf.matmul(tridiag_mat, tridiag_mat)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 5., -4., 1., 0., 0.],\r[-4., 6., -4., 1., 0.],\r[ 1., -4., 6., -4., 1.],\r[ 0., 1., -4., 6., -4.],\r[ 0., 0., 1., -4., 5.]], dtype=float32)\u0026gt;\r\n\r\rSome common operations on matrices\r\nTrace\rComputes the trace of a tensor. For non-square rank 2 tensors, trace of the main diagonal is computed.\nmat = tf.constant([[2,4,6],\r[5,1,9.]])\rtf.linalg.trace(mat).numpy()\r\r3.0\r\n\rDeterminant\rComputes the determinant of the matrix.\nmat = -2*tf.linalg.diag([1,2,3.])\rtf.linalg.det(mat)\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=-48.0\u0026gt;\r\n\rRank\rComputes the rank of a matrix.\nA = tf.constant([[1,4,5],\r[3,2,5],\r[2,1,3.]])\rrank = tf.linalg.matrix_rank(A)\rprint(\u0026quot;Rank of A = \u0026quot;, rank.numpy())\rRank of A = 2\r\n\rMatrix inverse\rComputes the matrix inverse if it exists. It uses \\(LU\\) decomposition to calculate inverse. What happens if inverse doesn’t exist? Here is the answer taken directly from tensorflow documentation:\n\r… If a matrix is not invertible there is no guarantee what the op does. It may detect the condition and raise an exception or it may simply return a garbage result.\n\rHaving read the documentation, we will apply the function to an invertible matrix.\nA = tf.constant([[2, 2, 3],\r[4,5,6],\r[1,2,4.]])\rA_inv = tf.linalg.inv(A)\rprint(A_inv)\rtf.Tensor(\r[[ 1.5999998e+00 -3.9999992e-01 -6.0000008e-01]\r[-1.9999999e+00 9.9999994e-01 7.9472862e-08]\r[ 5.9999996e-01 -3.9999998e-01 3.9999998e-01]], shape=(3, 3), dtype=float32)\rtf.matmul(A,A_inv)\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 9.9999970e-01, 1.1920929e-07, -1.1920929e-07],\r[-5.9604645e-07, 1.0000002e+00, 0.0000000e+00],\r[-2.3841858e-07, 0.0000000e+00, 1.0000000e+00]], dtype=float32)\u0026gt;\rIf \\(LU\\) decomposition result is already available from some prior computation, it can be used to compute the inverse using command tf.linalg.lu_matrix_inverse.\nlu, p = tf.linalg.lu(A)\rA_inv_by_lu = tf.linalg.lu_matrix_inverse(lu,p)\rA_inv_by_lu\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 1.5999998e+00, -3.9999992e-01, -6.0000008e-01],\r[-1.9999999e+00, 9.9999994e-01, 7.9472862e-08],\r[ 5.9999996e-01, -3.9999998e-01, 3.9999998e-01]], dtype=float32)\u0026gt;\r\n\rExtract diagonals of a matrix\rDiagonals of a matrix can be extracted using tf.linalg.diag_part function. Diagonal entries are obtained by setting k=0 which is the default. By setting k to any other value, either sub-diagonal or super-diagonal can be obtained.If two values are given to k, the values correspond respectively to the lower limit and upper limit of the diagonal. And the result contains all diagonals within those limits. The result is not a matrix. It is an array of diagonals, appended if required. Sub-diagonals are appended at the right and super diagonals are appended at the left.\nAnother function tf.linalg.tensor_diag_part can be used to extract the main diagonal of the matrix. But it can extract only the main diagonal.\nmat = tf.random.uniform(shape = (5,5), minval = 1, maxval = 20, dtype = tf.int32)\rmat\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=int32, numpy=\rarray([[ 1, 15, 19, 1, 1],\r[ 8, 5, 17, 14, 16],\r[12, 13, 12, 16, 19],\r[ 6, 8, 3, 5, 17],\r[12, 8, 3, 5, 8]], dtype=int32)\u0026gt;\rtf.linalg.diag_part(mat).numpy()\rarray([ 1, 5, 12, 5, 8], dtype=int32)\rtf.linalg.tensor_diag_part(mat).numpy()\rarray([ 1, 5, 12, 5, 8], dtype=int32)\rtf.linalg.diag_part(mat, k = (-1,0)).numpy()\rarray([[ 1, 5, 12, 5, 8],\r[ 8, 13, 3, 5, 0]], dtype=int32)\rtf.linalg.diag_part(mat, k = [-2,1]) # 2 subdiagonals, main diagonal, and 1 super diagonal\r\u0026lt;tf.Tensor: shape=(4, 5), dtype=int32, numpy=\rarray([[ 0, 15, 17, 16, 17],\r[ 1, 5, 12, 5, 8],\r[ 8, 13, 3, 5, 0],\r[12, 8, 3, 0, 0]], dtype=int32)\u0026gt;\r\n\rExtract band part of a matrix\rA band matrix is one that has nonzero values along its diagonal and a few sub-diagonals and super-diagonals. All other entries are zero. It is a sparse matrix. All of its nonzero entries are concentrated in a band along the diagonal. For example, tri-diagonal matrix is a banded matrix. It has lower bandwidth of 1 and upper bandwidth of 1. It is possible for a matrix to have different upper and lower bandwidths. It is still called a banded matrix.\nBanded matrices are useful because computations are significantly faster using these matrices as compared to dense matrices of same shape. If for some application, we want the band part of a matrix, we can use linalg.band_part function to extract it. This function takes three arguments (input, num_lower, num_upper). First argument is the tensor whose band part we want to extract. Second argument is the number of sub-diagonals to keep. If set to 0, no sub-diagonal is kept. num_lower = -1 keeps all the sub-diagonals. Similarly for num_upper.\nmatrix = tf.constant(tf.range(25, dtype=tf.float32), shape=(5,5))\rmatrix\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 0., 1., 2., 3., 4.],\r[ 5., 6., 7., 8., 9.],\r[10., 11., 12., 13., 14.],\r[15., 16., 17., 18., 19.],\r[20., 21., 22., 23., 24.]], dtype=float32)\u0026gt;\rtf.linalg.band_part(matrix, num_lower = 2, num_upper = 1)\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 0., 1., 0., 0., 0.],\r[ 5., 6., 7., 0., 0.],\r[10., 11., 12., 13., 0.],\r[ 0., 16., 17., 18., 19.],\r[ 0., 0., 22., 23., 24.]], dtype=float32)\u0026gt;\rtf.linalg.band_part(matrix, num_lower = -1, num_upper = 0) # Lower traiangular part\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 0., 0., 0., 0., 0.],\r[ 5., 6., 0., 0., 0.],\r[10., 11., 12., 0., 0.],\r[15., 16., 17., 18., 0.],\r[20., 21., 22., 23., 24.]], dtype=float32)\u0026gt;\r\n\r\r\rMatrix factorizations\rSome of the most common and widely used matrix factorizations are available in Tensorflow.\n\nLU\rMatrix \\(A\\) is factorized into a unit lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\), such that \\[A=LU\\]\nTo reduce round-off errors, partial pivoting is used. In partial pivoting, the following factorization is done.\r\\[PA = LU\\]\rWhere, \\(P\\) is called the permutation matrix.\nA = tf.constant([[1, 4, 7, 8],\r[24, -5, -13, 9],\r[-7, 21, 8, 19],\r[0, 18, 6, 4]], dtype = tf.float32)\rlu, p = tf.linalg.lu(A) # As per documentation\rprint(\u0026quot;LU = \u0026quot;)\rprint(lu)\rprint()\rprint(\u0026quot;P = \u0026quot;)\rprint(p)\rLU = tf.Tensor(\r[[ 24. -5. -13. 9. ]\r[ -0.29166666 19.541666 4.2083335 21.625 ]\r[ 0.04166667 0.21535183 6.635394 2.9680166 ]\r[ 0. 0.9211088 0.32005137 -16.868896 ]], shape=(4, 4), dtype=float32)\rP = tf.Tensor([1 2 0 3], shape=(4,), dtype=int32)\rWhat does the above result mean? Well, both \\(L\\), and \\(U\\) matrices have been merged into one. And \\(P\\) contains permutation indices. In practice, we don’t have to reconstruct individual matrices \\(L\\),\\(U\\), and \\(P\\), because tensorflow has built-in functions for further analysis that uses the result of tf.linalg.lu as given above. For the sake of demonstration, we will show how to reconstruct those matrices from above result. To reconstruct \\(P\\), we will use a linear operator that is discussed next. After constructing the matrices \\(A\\) will be \\[A = P^TLU\\]\nL = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))\rU = tf.linalg.band_part(lu, 0, -1)\rpermu_operator = tf.linalg.LinearOperatorPermutation(p)\rP = permu_operator.to_dense()\rprint(\u0026quot;L:\u0026quot;)\rprint(L)\rprint()\rprint(\u0026quot;U:\u0026quot;)\rprint(U)\rprint()\rprint(\u0026quot;P:\u0026quot;)\rprint(P)\rL:\rtf.Tensor(\r[[ 1. 0. 0. 0. ]\r[-0.29166666 1. 0. 0. ]\r[ 0.04166667 0.21535183 1. 0. ]\r[ 0. 0.9211088 0.32005137 1. ]], shape=(4, 4), dtype=float32)\rU:\rtf.Tensor(\r[[ 24. -5. -13. 9. ]\r[ 0. 19.541666 4.2083335 21.625 ]\r[ 0. 0. 6.635394 2.9680166]\r[ 0. 0. 0. -16.868896 ]], shape=(4, 4), dtype=float32)\rP:\rtf.Tensor(\r[[0. 1. 0. 0.]\r[0. 0. 1. 0.]\r[1. 0. 0. 0.]\r[0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\rtf.matmul(P, tf.matmul(L,U), transpose_a = True)\r\u0026lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\rarray([[ 1. , 4.0000005, 7. , 8. ],\r[ 24. , -5. , -13. , 9. ],\r[ -7. , 21. , 8. , 19. ],\r[ 0. , 18. , 6. , 3.999998 ]],\rdtype=float32)\u0026gt;\rWe can easily reconstruct our original matrix from \\(LU\\) factors using the function tf.linalg.lu_reconstruct.\ntf.linalg.lu_reconstruct(lu,p)\r\u0026lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\rarray([[ 1. , 4.0000005, 7. , 8. ],\r[ 24. , -5. , -13. , 9. ],\r[ -7. , 21. , 8. , 19. ],\r[ 0. , 18. , 6. , 3.999998 ]],\rdtype=float32)\u0026gt;\r\n\rCholesky\rIt is defined for symmetric positive definite matrices. If A is a symmetric positive definite matrix, its Cholesky decomposition can be written as:\r\\[ A = LL^T\\]\rWhere, \\(L\\) is a lower triangular matrix.\nA = tf.constant([[1,1,1],\r[1,5,5],\r[1,5,14]], dtype = tf.float32)\rL = tf.linalg.cholesky(A)\rL\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 0., 0.],\r[1., 2., 0.],\r[1., 2., 3.]], dtype=float32)\u0026gt;\rtf.matmul(L,L,transpose_b=True)\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 1., 1., 1.],\r[ 1., 5., 5.],\r[ 1., 5., 14.]], dtype=float32)\u0026gt;\r\n\rQR\rGiven a matrix \\(A\\), \\(QR\\) decomposition decomposes the matrix into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\) such that product of \\(Q\\) and \\(R\\) gives back \\(A\\). Columns of \\(Q\\) are an orthogonal basis for the column space of \\(A\\) (also known as range of \\(A\\)).\nA = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])\rQ,R = tf.linalg.qr(A)\rprint(\u0026quot;Q:\u0026quot;)\rprint(Q)\rprint()\rprint(\u0026quot;R:\u0026quot;)\rprint(R)\rQ:\rtf.Tensor(\r[[-0.18257415 0.4079837 ]\r[-0.36514837 -0.44398218]\r[-0.5477225 -0.575977 ]\r[-0.73029673 0.5519779 ]], shape=(4, 2), dtype=float32)\rR:\rtf.Tensor(\r[[-5.477226 -4.7469287]\r[ 0. 2.7778888]], shape=(2, 2), dtype=float32)\rWe can also get full \\(Q\\) and \\(R\\) matrices by setting full_matrices = True in the argument.\nQ_full, R_full = tf.linalg.qr(A, full_matrices = True)\rprint(\u0026quot;Q full:\u0026quot;)\rprint(Q_full)\rprint()\rprint(\u0026quot;R full:\u0026quot;)\rprint(R_full)\rQ full:\rtf.Tensor(\r[[-0.18257415 0.4079837 -0.17102492 -0.8780469 ]\r[-0.36514837 -0.44398218 -0.81774735 0.02891004]\r[-0.5477225 -0.575977 0.54808205 -0.2604928 ]\r[-0.73029673 0.5519779 0.04056833 0.4004264 ]], shape=(4, 4), dtype=float32)\rR full:\rtf.Tensor(\r[[-5.477226 -4.7469287]\r[ 0. 2.7778888]\r[ 0. 0. ]\r[ 0. 0. ]], shape=(4, 2), dtype=float32)\r\n\rSVD\rSingular value decomposition (SVD) of a matrix \\(A\\in R^{m\\times n}\\) is defined as\r\\[A = U\\Sigma V^T\\]\rWhere, \\(U\\in R^{m\\times m}\\) and \\(V\\in R^{n\\times n}\\) are orthogonal matrices, commonly known as left and right singular vectors respectively. \\(\\Sigma \\in R^{m\\times n}\\) is a diagonal matrix.\nmat = tf.constant([[5,2,3],\r[2,9,4],\r[3,2,6],\r[7,8,9.]])\rs,u,v = tf.linalg.svd(mat)\rprint(\u0026quot;S:\u0026quot;)\rprint(s)\rprint()\rprint(\u0026quot;U:\u0026quot;)\rprint(u)\rprint()\rprint(\u0026quot;V:\u0026quot;)\rprint(v)\rS:\rtf.Tensor([18.604359 5.459675 2.4636664], shape=(3,), dtype=float32)\rU:\rtf.Tensor(\r[[ 0.2936678 0.40458775 0.7340845 ]\r[ 0.48711583 -0.7956307 0.01849233]\r[ 0.34406567 0.418864 -0.67870086]\r[ 0.7470583 0.16683212 0.01195723]], shape=(4, 3), dtype=float32)\rV:\rtf.Tensor(\r[[ 0.4678568 0.5231253 0.71235543]\r[ 0.6254436 -0.76545155 0.15134181]\r[ 0.62444425 0.3747316 -0.685307 ]], shape=(3, 3), dtype=float32)\rThe result is a truncated SVD. To get full SVD decomposition, we have to set full_matrices = True in the argument.\ns_full,u_full,v_full = tf.linalg.svd(mat, full_matrices = True)\rprint(\u0026quot;S full:\u0026quot;)\rprint(s_full)\rprint()\rprint(\u0026quot;U full:\u0026quot;)\rprint(u_full)\rprint()\rprint(\u0026quot;V full:\u0026quot;)\rprint(v_full)\rS full:\rtf.Tensor([18.604359 5.459675 2.4636664], shape=(3,), dtype=float32)\rU full:\rtf.Tensor(\r[[ 0.2936678 0.40458775 0.7340845 -0.45955172]\r[ 0.48711583 -0.7956307 0.01849233 -0.35964906]\r[ 0.34406567 0.418864 -0.67870086 -0.4955166 ]\r[ 0.7470583 0.16683212 0.01195723 0.6433724 ]], shape=(4, 4), dtype=float32)\rV full:\rtf.Tensor(\r[[ 0.4678568 0.5231253 0.71235543]\r[ 0.6254436 -0.76545155 0.15134181]\r[ 0.62444425 0.3747316 -0.685307 ]], shape=(3, 3), dtype=float32)\rIf only singular values are of interest, it can be computed without computing singular vectors. In this way, computations can be much faster.\ntf.linalg.svd(mat, compute_uv=False).numpy()\rarray([18.604359 , 5.459675 , 2.4636664], dtype=float32)\r\n\r\rEigenvalues and eigenvectors\rSymmetry is an important consideration while computing eigenvalues and eigenvectors. For symmetric matrices, different set of algorithms are used for eigen analysis that exploit the symmetry of the matrix. Therefore, two functions are available in tensorflow for eigen analysis. eig is used to compute eigenvalues and eigenvectors of a dense matrix without any special structure. eigh is used for eigen analysis of Hermitian matrices. If only eigenvalues are of interest, eigvals and eigvalsh can be used compute just eigenvalues.\n\nEigen-analysis of Hermitian matrices\rA = tf.constant([[3,1,1],\r[1,2,1],\r[1,1,2.]])\rvalues, vectors = tf.linalg.eigh(A)\rprint(\u0026quot;Eigenvalues:\u0026quot;)\rprint(values.numpy())\rprint()\rprint(\u0026quot;Eigenvectors:\u0026quot;)\rprint(vectors.numpy())\rEigenvalues:\r[1. 1.5857866 4.4142137]\rEigenvectors:\r[[ 0. -0.7071068 0.70710665]\r[-0.70710677 0.49999994 0.50000006]\r[ 0.7071068 0.4999999 0.5 ]]\rEach row is an eigenvector.\ntf.linalg.matvec(A, vectors[0,:]).numpy()\rarray([-1.7881393e-07, -7.0710701e-01, 7.0710647e-01], dtype=float32)\rResults are accurate up to 5 decimal digits.\ntf.linalg.eigvalsh(A).numpy() # Just eigenvalues\rarray([1. , 1.5857866, 4.4142137], dtype=float32)\rWhat happens if you pass a nonsymmetric matrix to eigh by mistake?\nWell, while using eigh, tensorflow assumes the matrix to be symmetric. Tensorflow doesn’t check whether the matrix is symmetric or not. It just takes the lower triangular part, assumes that the upper triangular part is same because of symmetry and performs the computations. So be prepared to get a wrong result!\n\n\rEigen-analysis of other matrices\rA = tf.constant([[1,-5,3],\r[2,4,-7],\r[3,9,-2.]])\rvalues, vectors = tf.linalg.eig(A)\rprint(\u0026quot;Eigenvalues:\u0026quot;)\rprint(values.numpy())\rprint()\rprint(\u0026quot;Eigenvectors:\u0026quot;)\rprint(vectors.numpy())\rEigenvalues:\r[2.7560833 -7.9942424e-08j 0.12195918-7.5705280e+00j\r0.12195931+7.5705280e+00j]\rEigenvectors:\r[[ 0.06142625+0.95019215j 0.16093381-0.34744066j 0.13818482+0.35709903j]\r[-0.01236177-0.19122148j 0.3560446 +0.53046155j 0.38952374-0.5063876j ]\r[ 0.01535368+0.23750281j -0.33046415+0.5796737j -0.29238018-0.59978485j]]\rOnly eigenvalues.\ntf.linalg.eigvals(A).numpy()\rarray([2.7560833 -7.9942424e-08j, 0.12195918-7.5705280e+00j,\r0.12195931+7.5705280e+00j], dtype=complex64)\rWhat happens when you pass a symmetric matrix to eig?\nNothing! We will still get the correct answer. Tensorflow will use more operations to compute results when it could have been done using less computations.\n\n\r\rSolving linear systems\rA linear system can be written as \\[ Ax = b\\]\rIn general, \\(A\\) can be square or rectangular. Right hand side \\(b\\) is a vector in this case. If we have to solve the linear system for multiple right hand side vectors involving same \\(A\\), the RHS can be replaced by a matrix whose columns are different RHS vectors.\nDepending on the structure of \\(A\\) (whether triangular, or tri-diagonal, or positive definite), suitable algorithm is chosen to solve the linear system. Tensorflow has a function tf.linalg.solve to solve linear systems. But this function doesn’t take into account the special structure of \\(A\\).\nA = tf.constant([[1,1,1],\r[1,5,5],\r[1,5,13]], dtype = tf.float32)\rb = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)\rtf.linalg.solve(A,b)\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[1. ],\r[0.875],\r[1.125]], dtype=float32)\u0026gt;\r\nUsing LU decomposition\rIf \\(LU\\) decomposition factors of \\(A\\) are known, those can be used to solve the linear system.\rFor example, solve:\r\\[\\begin{pmatrix}\r1 \u0026amp; 1 \u0026amp; 1\\\\\r1 \u0026amp; 5 \u0026amp; 5\\\\\r1 \u0026amp; 5 \u0026amp; 13\r\\end{pmatrix} x= \\begin{pmatrix}\r3\\\\\r11\\\\\r20\\\\\r\\end{pmatrix}\\]\nA = tf.constant([[1,1,1],\r[1,5,5],\r[1,5,13]], dtype = tf.float32)\rb = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)\rlu, p = tf.linalg.lu(A) # Factoriztion result of LU\rx_sol_lu = tf.linalg.lu_solve(lu,p,b)\rx_sol_lu\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[1. ],\r[0.875],\r[1.125]], dtype=float32)\u0026gt;\rtf.matmul(A,x_sol_lu)\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[ 3.],\r[11.],\r[20.]], dtype=float32)\u0026gt;\rOnce we have obtained factors \\(L\\) and \\(U\\), we can use tf.linalg.triangular_solve to solve the linear system by solving following two triangular system. \\[Ly = b\\]\rand \\[Ux = y\\]\nL = tf.linalg.band_part(lu,-1,0) - tf.linalg.diag(tf.linalg.diag_part(lu)) + tf.linalg.diag(tf.ones(shape = lu.shape[0],))\ry = tf.linalg.triangular_solve(L,b) # Solves Ly = b\rx = tf.linalg.triangular_solve(lu,y, lower = False) # Solves Ux = y\rx\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[1. ],\r[0.875],\r[1.125]], dtype=float32)\u0026gt;\r\n\rUsing Cholesky decomposition\rIf \\(A\\) is positive definite, Cholesky decomposition is an efficient method for solving the linear system. For positive definite \\(A\\), Cholesky decomposition requires fewer computations that \\(LU\\) decomposition. This is because it exploits the symmetry of the matrix \\(A\\). Once the Cholesky factor \\(L\\) is found, we solve the linear system by solving two triangular systems. Solving triangular systems only requires \\(O(n^2)\\) operations.\r\\[ LL^Tx = b\\]\rTwo triangular systems are:\r\\[ Ly = b\\]\rThis gives us \\(y\\). Using \\(y\\) we solve for \\(x\\) using the following equation.\r\\[L^Tx = y\\]\nIn tensorflow, we solve the system using tf.linalg.cholesky_solve. It takes cholesky factor \\((L)\\) and right hand side \\(b\\) as input.\nFor example, solve:\r\\[\\begin{pmatrix}\r1 \u0026amp; 1 \u0026amp; 1\\\\\r1 \u0026amp; 5 \u0026amp; 5\\\\\r1 \u0026amp; 5 \u0026amp; 13\r\\end{pmatrix} x= \\begin{pmatrix}\r3\\\\\r11\\\\\r20\\\\\r\\end{pmatrix}\\]\nA = tf.constant([[1,1,1],\r[1,5,5],\r[1,5,13]], dtype = tf.float32)\rb = tf.constant([3,11,20], shape = (3,1), dtype = tf.float32)\rL = tf.linalg.cholesky(A)\rsol_chol = tf.linalg.cholesky_solve(L, b)\rsol_chol\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[1.0000001],\r[0.8750001],\r[1.1249999]], dtype=float32)\u0026gt;\rtf.matmul(A,sol_chol)\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[ 3. ],\r[11. ],\r[19.999998]], dtype=float32)\u0026gt;\r\n\rSolving tri-diagonal systems\rIf matrix \\(A\\) is tri-diagonal, tf.linalg.tridiagonal_solve can be used to solve the linear system efficiently. For example, we will solve the following tri-diagonal system.\r\\[\\begin{pmatrix}\r2 \u0026amp; -1 \u0026amp; 0 \u0026amp;0 \u0026amp; 0\\\\\r-1 \u0026amp; 2 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0\\\\\r0 \u0026amp; -1 \u0026amp; 2\u0026amp; -1\u0026amp; 0\\\\\r0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 2 \u0026amp; -1\\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 2\r\\end{pmatrix}x=\r\\begin{pmatrix}\r3\\\\\r4\\\\\r-5\\\\\r7\\\\\r9\\end{pmatrix}\\]\ndiags = tf.constant([[-1,-1,-1,-1, 0],\r[ 2, 2, 2, 2, 2],\r[ 0,-1,-1,-1,-1]], dtype = tf.float32)\rb = tf.constant([3,4,-5,7,9.],shape = (5,1))\rx = tf.linalg.tridiagonal_solve(diagonals = diags, rhs = b)\rx\r\u0026lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\rarray([[ 6.5000005],\r[10.000001 ],\r[ 9.500001 ],\r[14. ],\r[11.5 ]], dtype=float32)\u0026gt;\rtf.linalg.tridiagonal_matmul(diagonals=diags, rhs = x)\r\u0026lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\rarray([[ 3. ],\r[ 4.000001],\r[-4.999999],\r[ 7. ],\r[ 9. ]], dtype=float32)\u0026gt;\r\n\r\rSolving least squares problems\r\nOrdinary least squares\rBoth over determined and under determined least squares problem can be solved using the command tf.linalg.lstsq. In the underdetermined case, the output is the least norm solution. Least squares problem can be written as\r\\[arg\\min_{x}\\|Ax-b\\|_2^2\\]\rThat is, we try to find an \\(x\\) such that the residual error is as small as possible.\rFor example, we will solve following two problems.\r\\[\\begin{pmatrix}\r1 \u0026amp; 2\\\\\r2 \u0026amp; 0.5\\\\\r3 \u0026amp; 1\\\\\r4 \u0026amp; 5\\\\\r\\end{pmatrix}x_{over}=\r\\begin{pmatrix}\r3\\\\\r4\\\\\r5\\\\\r6\\\\\r\\end{pmatrix}\\]\r\\[\\begin{pmatrix}\r3 \u0026amp; 1 \u0026amp; 2 \u0026amp; 5\\\\\r7 \u0026amp; 9 \u0026amp; 1 \u0026amp; 4\r\\end{pmatrix}x_{under}=\r\\begin{pmatrix}\r7.2\\\\\r-5.8\\\\\r\\end{pmatrix}\\]\nA_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])\rA_under = tf.constant([[3,1,2,5],[7,9,1,4.]])\rb_over = tf.constant([3,4,5,6.], shape = (4,1))\rb_under = tf.constant([7.2,-5.8], shape = (2,1))\rx_over = tf.linalg.lstsq(A_over, b_over)\rx_over\r\u0026lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\rarray([[ 1.704103 ],\r[-0.04319588]], dtype=float32)\u0026gt;\rThough it is not advisable, for this simple case, we will directly apply normal equation to get the solution.\ntf.matmul(tf.linalg.inv(tf.matmul(A_over,A_over, transpose_a = True)), tf.matmul(A_over,b_over, transpose_a = True))\r\u0026lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\rarray([[ 1.704104 ],\r[-0.04319668]], dtype=float32)\u0026gt;\rx_under = tf.linalg.lstsq(A_under, b_under)\rx_under\r\u0026lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=\rarray([[-0.04100358],\r[-1.3355565 ],\r[ 0.699703 ],\r[ 1.4518324 ]], dtype=float32)\u0026gt;\rWe will computer the least norm solution for underdetermined case using the closed form solution. However, it should be remembered that it is not advisable to do so in practice for large systems.\ntf.matmul(A_under,tf.matmul(tf.linalg.inv(tf.matmul(A_under, A_under, transpose_b = True)), b_under), transpose_a = True)\r\u0026lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=\rarray([[-0.04100358],\r[-1.3355561 ],\r[ 0.6997029 ],\r[ 1.4518325 ]], dtype=float32)\u0026gt;\r\n\rRegularized least squares\rOnly \\(l_2\\) regularization is supported. The following regularized problem is solved.\r\\[arg\\min_{x}\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2\\]\nHere, \\(\\lambda\\) is a hyperparameter. Usually several values of \\(\\lambda\\) are tried over a logarithmic scale before choosing the best one.\nx_over_reg = tf.linalg.lstsq(A_over, b_over, l2_regularizer= 2.0)\rx_over_reg.numpy()\rarray([[1.3890449 ],\r[0.21348318]], dtype=float32)\rx_under_reg = tf.linalg.lstsq(A_under, b_under, l2_regularizer=2.)\rx_under_reg\r\u0026lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=\rarray([[-0.04763567],\r[-1.214508 ],\r[ 0.62748903],\r[ 1.299031 ]], dtype=float32)\u0026gt;\r\n\r\rSome specialized operations\r\nNorm\rNorm can be defined for vectors as well as matrices. \\(p\\) norm of vector is defined as\r\\[\\|x\\|_p = (\\Sigma_{i=1}^{n}|x_i|^p)^\\frac{1}{p}\\]\nMatrix is norm is defined as\r\\[\\|A\\|_p= \\max_{x\\neq 0}\\frac{\\|Ax\\|_p}{\\|x\\|_p}\\]\nTensorflow supports all the usual vector and matrix norms that are used in practice. Using only tensorflow we can calculate all norms except infinity norm. To calculate infinity norm we have to use ord = np.inf.\ntf.linalg.norm(tf.constant([1,-2,3.]), ord = \u0026quot;euclidean\u0026quot;).numpy()\r3.7416575\rtf.linalg.norm(tf.constant([1,-2,3]), ord = 1).numpy()\r6\rFractional norms for vectors are also supported.\ntf.linalg.norm(tf.constant([1,-2,3.]), ord = 0.75).numpy()\r8.46176\rA = tf.constant([[1,8, 2,3],\r[2,7,6,5],\r[0,3,2,8.]])\rmat_norm_2 = tf.linalg.norm(A, ord = 2, axis = [0,1])\rprint(\u0026quot;2 norm of matrix A = \u0026quot;, mat_norm_2.numpy())\rWARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_22/lib/python3.7/site-packages/tensorflow/python/ops/linalg_ops.py:721: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\rInstructions for updating:\rThis op will be removed after the deprecation date. Please switch to tf.sets.difference().\r2 norm of matrix A = 15.294547\r2 norm of a matrix is equivalent to the largest singular value of the matrix. We will verify that.\nvals,_ ,_ = tf.linalg.svd(A)\rtf.math.reduce_max(vals).numpy()\r15.294547\r\n\rNormalizing a tensor\rComputes the norm and normalizes the tensor using that norm. By normalize we mean, divide the entries of the tensor by the norm. Here, we will consider a matrix. But the method can be extended to multi-dimensional tensor.\nIf computed norm is a single number, all the entries of the matrix will be divided by that number. If norm is calculated along some axis, normalization happens along that axis using individual norms. Here are some examples.\nA\r\u0026lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\rarray([[1., 8., 2., 3.],\r[2., 7., 6., 5.],\r[0., 3., 2., 8.]], dtype=float32)\u0026gt;\rnormalized_mat, norm = tf.linalg.normalize(A, ord = 2, axis = [0,1])\rprint(\u0026quot;Normalized matrix: \u0026quot;)\rprint(normalized_mat.numpy())\rprint()\rprint(\u0026quot;Norm = \u0026quot;, norm.numpy())\rNormalized matrix: [[0.06538278 0.5230622 0.13076556 0.19614834]\r[0.13076556 0.45767945 0.39229667 0.3269139 ]\r[0. 0.19614834 0.13076556 0.5230622 ]]\rNorm = [[15.294547]]\rWe will get the same normalized matrix by dividing the entries of the matrix by the norm.\nA/norm\r\u0026lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\rarray([[0.06538278, 0.5230622 , 0.13076556, 0.19614834],\r[0.13076556, 0.45767945, 0.39229667, 0.3269139 ],\r[0. , 0.19614834, 0.13076556, 0.5230622 ]], dtype=float32)\u0026gt;\rnorm_mat_by_col, norms_col = tf.linalg.normalize(A, ord = 2, axis = 0)\rprint(\u0026quot;Normalized matrix:\u0026quot;)\rprint(norm_mat_by_col)\rprint()\rprint(\u0026quot;Norms of columns of A:\u0026quot;)\rprint(norms_col)\rNormalized matrix:\rtf.Tensor(\r[[0.4472136 0.724286 0.30151135 0.30304575]\r[0.8944272 0.63375026 0.904534 0.5050763 ]\r[0. 0.27160725 0.30151135 0.80812204]], shape=(3, 4), dtype=float32)\rNorms of columns of A:\rtf.Tensor([[ 2.236068 11.045361 6.6332498 9.899495 ]], shape=(1, 4), dtype=float32)\rtf.linalg.norm(A[:,0], ord = 2).numpy() # 2 Norm of first column of A\r2.236068\rA/norms_col\r\u0026lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\rarray([[0.4472136 , 0.724286 , 0.30151135, 0.30304575],\r[0.8944272 , 0.63375026, 0.904534 , 0.5050763 ],\r[0. , 0.27160725, 0.30151135, 0.80812204]], dtype=float32)\u0026gt;\rnorm_mat_by_row, norms_row = tf.linalg.normalize(A, ord = 2, axis = 1)\rprint(\u0026quot;Normalized matrix:\u0026quot;)\rprint(norm_mat_by_row)\rprint()\rprint(\u0026quot;Norms of rows:\u0026quot;)\rprint(norms_row)\rNormalized matrix:\rtf.Tensor(\r[[0.11322771 0.9058217 0.22645542 0.33968312]\r[0.18731716 0.6556101 0.56195146 0.4682929 ]\r[0. 0.34188172 0.22792116 0.91168463]], shape=(3, 4), dtype=float32)\rNorms of rows:\rtf.Tensor(\r[[ 8.83176 ]\r[10.677078]\r[ 8.774964]], shape=(3, 1), dtype=float32)\rtf.linalg.norm(A[0,:], ord = 2).numpy() # 2 norm of first row of A\r8.83176\rA/norms_row\r\u0026lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\rarray([[0.11322771, 0.9058217 , 0.22645542, 0.33968312],\r[0.18731716, 0.6556101 , 0.56195146, 0.4682929 ],\r[0. , 0.34188172, 0.22792116, 0.91168463]], dtype=float32)\u0026gt;\r\n\rGlobal norm\rGiven two or more tensors, tf.linalg.global_norm computes the 2 norm of a vector generated by resizing all the tensors to one dimensional arrays.\na = tf.constant([1, 2, 3.])\rb = tf.constant([[4,5],\r[6,7.]])\rtf.linalg.global_norm([a,b]).numpy()\r11.83216\rtf.linalg.norm([1,2,3,4,5,6,7.], ord = 2).numpy()\r11.83216\r\n\rCross product of vectors\rIt is defined for 3-element vectors. For two vectors \\(a = (a_1, a_2, a_3)^T\\), and \\(b = (b_1, b_2, b_3)^T\\), cross product is defined as the determinant of following matrix\r\\[\\begin{pmatrix}\ri \u0026amp; j \u0026amp; k\\\\\ra_1 \u0026amp; a_2 \u0026amp; a_3\\\\\rb_1 \u0026amp; b_2 \u0026amp; b_3\r\\end{pmatrix}\\]\rWhere \\(i, j\\), and \\(k\\) are unit direction vectors along three perpendicular right handed system.\na = tf.constant([1,2,3])\rb = tf.constant([2,3,4])\rtf.linalg.cross(a,b).numpy()\rarray([-1, 2, -1], dtype=int32)\rFirst element in the output corresponds to the value along \\(i\\)th direction. Similarly for other outputs.\nIt is also possible to calculate cross product of more that one pair of vectors simultaneously.\nc = tf.random.normal(shape = (5,3))\rd = tf.random.normal(shape = (5,3))\rtf.linalg.cross(c,d).numpy()\rarray([[ 1.2073451 , 0.64969957, -0.02847651],\r[-0.4979881 , 1.3546165 , -0.591327 ],\r[-0.11196035, 2.9687622 , 1.8474071 ],\r[-0.13938189, -3.3894374 , -2.557374 ],\r[-0.00778121, -3.5212047 , -3.4708867 ]], dtype=float32)\rFirst row of output is the cross product of first rows of \\(c\\) and \\(d\\). Similarly for other rows.\n\n\rMatrix square root\rSquare root of a matrix is defined for invertible matrices whose real eigenvalues are positive.\nmat = tf.constant([[5,2,3],\r[2,9,4],\r[3,2,6.]])\rmat_root = tf.linalg.sqrtm(mat)\rmat_root\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[2.1185937 , 0.35412252, 0.6189322 ],\r[0.30147368, 2.9409115 , 0.7257953 ],\r[0.6540313 , 0.33657336, 2.3132057 ]], dtype=float32)\u0026gt;\rtf.matmul(mat_root, mat_root)\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[4.9999986, 2.0000007, 3.0000038],\r[2.0000005, 9.000003 , 4.0000057],\r[3.0000033, 2.000003 , 6.0000052]], dtype=float32)\u0026gt;\r\n\rMatrix exponential\rExponential of a matrix \\(A\\) is defined as\r\\[ e^A = \\sum_{n=0}^\\infty \\frac{A^n}{n!}\\]\nIn practice, the sum is not taken to infinity. Rather, approximations are used to compute matrix exponential. Tensorflow implementation is based on this paper.\nWhen the matrix has a full set of independent eigenvectors, the formula can be simplified to\r\\[e^A = Se^{\\Lambda}S^{-1}\\]\nwhere, \\(S\\) is the eigenvector matrix and \\(e^\\Lambda\\) is a diagonal matrix whose diagonal entries are exponentials of eigenvalues of the matrix \\(A\\).\nA = tf.constant([[0,1],\r[1,0.]])\rtf.linalg.expm(A)\r\u0026lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\rarray([[1.5430806, 1.1752012],\r[1.1752012, 1.5430806]], dtype=float32)\u0026gt;\r\n\rMatrix logarithm\rComputes logarithm of the matrix such that matrix exponential of the result gives back the original matrix. Refer to the documentation for further details. It is defined only for complex matrices.\nmat = tf.constant([[5,2,3],\r[2,9,4],\r[3,2,6.]], dtype = tf.complex64)\rmat_log = tf.linalg.logm(mat)\rmat_log\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=\rarray([[1.4031177 +0.j, 0.25731087+0.j, 0.53848237+0.j],\r[0.16580153+0.j, 2.1160111 +0.j, 0.54512537+0.j],\r[0.5994888 +0.j, 0.2268081 +0.j, 1.5622762 +0.j]], dtype=complex64)\u0026gt;\rtf.linalg.expm(mat_log)\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=complex64, numpy=\rarray([[4.999999 +0.j, 1.9999989+0.j, 2.9999986+0.j],\r[1.9999995+0.j, 9.000006 +0.j, 4.0000005+0.j],\r[2.9999995+0.j, 2.000001 +0.j, 5.9999995+0.j]], dtype=complex64)\u0026gt;\r\n\rLog-determinant of a matrix\rComputes the natural logarithm of the determinant of a matrix. There are two functions in tensorflow to calculate this.\r* If matrix is symmetric positive definite, use tf.linalg.logdet (Uses Cholesky decomposition)\r* For other matrices, use tf.linalg.slogdet (Uses \\(LU\\) decomposition)\nslogdet computes the sign of the determinant as well as the log of the absolute value of the determinant.\nmat = tf.constant([[5,2,3],\r[2,9,2],\r[3,2,6.]]) # Symmetric positive definite\rtf.linalg.logdet(mat).numpy()\r5.1298985\rtf.math.log(tf.linalg.det(mat)).numpy()\r5.1298985\rmat_2 = tf.constant([[5,2,3],\r[0,-2,2],\r[0,0,6.]])\rsign, log_abs_det = tf.linalg.slogdet(mat_2)\rprint(\u0026quot;Sign of determinant = \u0026quot;, sign.numpy())\rprint(\u0026quot;Log of absolute value of determinant = \u0026quot;, log_abs_det.numpy())\rSign of determinant = -1.0\rLog of absolute value of determinant = 4.0943446\rtf.math.log(tf.abs(tf.linalg.det(mat_2))).numpy()\r4.0943446\r\n\rPseudo inverse of a matrix\rWhile matrix inverse is defined only for square matrices, pseudo inverse is defined for matrices of any shape. It is also defined for singular matrices. Pseudo inverse can also be used to solve ordinary least squares problem. For the problem \\[arg\\min_{x}\\|Ax-b\\|_2\\]\rthe approximate least squares solution can be written as\r\\[x_{ls} = A^{\\dagger}b\\]\nWhere, \\(A^{\\dagger}\\) is the pseudo inverse of \\(A\\).\ntf.linalg.pinv(tf.constant([[2,0,0],[0,3,0],[5,0,0.]]))\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[0.06896552, 0. , 0.17241378],\r[0. , 0.33333334, 0. ],\r[0. , 0. , 0. ]], dtype=float32)\u0026gt;\rA_over = tf.constant([[1,2],[2,0.5],[3, 1],[4,5.]])\rA_under = tf.constant([[3,1,2,5],[7,9,1,4.]])\rb_over = tf.constant([3,4,5,6.], shape = (4,1))\rb_under = tf.constant([7.2,-5.8], shape = (2,1))\rx_ls_over = tf.linalg.lstsq(A_over,b_over)\rx_ls_over\r\u0026lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\rarray([[ 1.704103 ],\r[-0.04319588]], dtype=float32)\u0026gt;\rtf.matmul(tf.linalg.pinv(A_over),b_over)\r\u0026lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\rarray([[ 1.7041038 ],\r[-0.04319668]], dtype=float32)\u0026gt;\rx_ls_under = tf.linalg.lstsq(A_under, b_under)\rx_ls_under\r\u0026lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=\rarray([[-0.04100358],\r[-1.3355565 ],\r[ 0.699703 ],\r[ 1.4518324 ]], dtype=float32)\u0026gt;\rtf.matmul(tf.linalg.pinv(A_under), b_under)\r\u0026lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=\rarray([[-0.04100376],\r[-1.3355565 ],\r[ 0.69970286],\r[ 1.4518324 ]], dtype=float32)\u0026gt;\r\n\r\rLinear operators\rLinear operators are a powerful way of defining matrices and associated operators without even doing actual computations. What does this mean? Do we ever get result of our computations using operators? Well, the computations are done only when we ask for the results. Before that the operators just act on each other (like chaining of operators) without doing any computation. We will show this by examples.\nNote: We will mainly use operators to form dense matrices. But the scope of applicability of operators is far bigger than that.\nTo define a matrix as an operator, we use tf.linalg.LinearOperatorFullMatrix.\noperator = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],\r[2,3,5],\r[7,8,9.]]))\roperator\r\u0026lt;tensorflow.python.ops.linalg.linear_operator_full_matrix.LinearOperatorFullMatrix at 0x7fdc6dadfd90\u0026gt;\rWe get only the memory location. No result is shown. To see the actual matrix we have to call the method to_dense on this operator. There are many methods that can be called on an operator. For the full list, refer the documentation.\nBefore seeing the result, we will apply adjoint operator to our old operator and apply to_dense to the adjoint operator. If everything works well, we should see the transpose of the matrix as result.\nadj_operator = tf.linalg.LinearOperatorAdjoint(operator)\radj_operator\rWARNING:tensorflow:From /home/biswajit/anaconda3/envs/tf_cpu_22/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_adjoint.py:145: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\rInstructions for updating:\rDo not call `graph_parents`.\r\u0026lt;tensorflow.python.ops.linalg.linear_operator_adjoint.LinearOperatorAdjoint at 0x7fdc61140cd0\u0026gt;\rAgain no result. At this point we want to see the result. So we will apply to_dense method to adj_operator.\nadj_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 7.],\r[2., 3., 8.],\r[3., 5., 9.]], dtype=float32)\u0026gt;\rTo compare it with our original matrix, we will also show the original matrix.\noperator.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 3.],\r[2., 3., 5.],\r[7., 8., 9.]], dtype=float32)\u0026gt;\rAs expected, the adjoint operator gives the correct answer.\n\nCommon methods on linear operators\rThere are many methods that can be called on the operator. Depending on the operator, the methods vary. In this section we will discuss some of the methods of LinearOperatorFullMatrix operator. Some of the methods are:\n\rcond (To find condition number)\rdeterminant\rcholesky (To compute Cholesky factors of operator)\reigvals (Compute eigenvalues only for self-adjoint (Hermitian) matrices)\rtrace\rinverse\rsolve (Solve linear system using operator)\radjoint\rand many others.\r\roperator.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 3.],\r[2., 3., 5.],\r[7., 8., 9.]], dtype=float32)\u0026gt;\roperator.cond()\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=68.21983\u0026gt;\roperator.trace()\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=13.0\u0026gt;\roperator.determinant()\rWARNING:tensorflow:Using (possibly slow) default implementation of determinant. Requires conversion to a dense matrix and O(N^3) operations.\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0\u0026gt;\roperator.adjoint().to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 7.],\r[2., 3., 8.],\r[3., 5., 9.]], dtype=float32)\u0026gt;\roperator.inverse().to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[-2.1666667 , 1. , 0.16666669],\r[ 2.8333333 , -2. , 0.16666669],\r[-0.83333325, 1. , -0.16666669]], dtype=float32)\u0026gt;\roperator.matmul(operator.inverse().to_dense())\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\r[-2.3841858e-07, 1.0000000e+00, 0.0000000e+00],\r[-2.3841858e-07, 0.0000000e+00, 1.0000000e+00]], dtype=float32)\u0026gt;\r\n\rSpecial matrices using operators\r\nToeplitz matrix\rcol = tf.constant([1,2,3,4,5.])\rrow = tf.constant([1,6,7,8,9.])\rtoeplitz_operator = tf.linalg.LinearOperatorToeplitz(col = col, row = row )\rtoeplitz_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 6., 7., 8., 9.],\r[2., 1., 6., 7., 8.],\r[3., 2., 1., 6., 7.],\r[4., 3., 2., 1., 6.],\r[5., 4., 3., 2., 1.]], dtype=float32)\u0026gt;\r\n\rCirculant matrix\rkernel = [1,2,3,4,5]\rspectrum = tf.signal.fft(tf.cast(kernel, dtype = tf.complex64))\rcirc_operator = tf.linalg.LinearOperatorCirculant(spectrum = spectrum, input_output_dtype = tf.float32)\rcirc_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1.0000002 , 4.9999995 , 3.9999993 , 3. , 2.0000005 ],\r[2.0000005 , 0.99999934, 4.9999995 , 3.9999993 , 2.9999998 ],\r[3.0000002 , 2. , 0.9999998 , 4.9999995 , 3.9999998 ],\r[3.9999993 , 2.9999993 , 2. , 1. , 4.9999995 ],\r[4.9999995 , 3.9999993 , 3. , 2.0000005 , 1.0000002 ]],\rdtype=float32)\u0026gt;\rcirc_operator.convolution_kernel()\r\u0026lt;tf.Tensor: shape=(5,), dtype=float32, numpy=\rarray([1. , 2.0000002, 3. , 3.9999993, 4.9999995],\rdtype=float32)\u0026gt;\r\n\rBlock diagonal matrix\roperator_1 = tf.linalg.LinearOperatorFullMatrix(tf.constant([[1,2,3],\r[4,5,6],\r[7,8,9]], dtype = tf.float32))\roperator_2 = tf.linalg.LinearOperatorFullMatrix(-1*tf.constant([[9,8],\r[7,6]], dtype = tf.float32))\rblk_diag_operator = tf.linalg.LinearOperatorBlockDiag([operator_1,operator_2])\rblk_diag_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 1., 2., 3., 0., 0.],\r[ 4., 5., 6., 0., 0.],\r[ 7., 8., 9., 0., 0.],\r[ 0., 0., 0., -9., -8.],\r[ 0., 0., 0., -7., -6.]], dtype=float32)\u0026gt;\r\n\rBlock lower triangular matrix\roperator_3 = tf.linalg.LinearOperatorFullMatrix(tf.constant(tf.repeat(6.,repeats = 6), shape = (2,3)))\rblk_lower = tf.linalg.LinearOperatorBlockLowerTriangular([[operator_1], [operator_3, operator_2]])\rblk_lower.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 1., 2., 3., 0., 0.],\r[ 4., 5., 6., 0., 0.],\r[ 7., 8., 9., 0., 0.],\r[ 6., 6., 6., -9., -8.],\r[ 6., 6., 6., -7., -6.]], dtype=float32)\u0026gt;\r\n\rHouseholder matrix\rHouseholder matrix can be used to triangularize a matrix using orthogonal matrices (the process is called orthogonal triangularization). But we will not pursue that point here. We will only show the method using only a column vector. Given a vector \\(v = [1, 4, 7]^T\\), Householder transform can transform the vector into \\(v = \\|v\\|_2[1,0,0]^T\\). It is achieved by multiplying the vector \\(v\\) by an orthogonal Householder matrix \\(H\\).\r\\[H\\begin{pmatrix}\rv_1\\\\\rv_2\\\\\rv_3\\end{pmatrix}=\\|v\\|_2\\begin{pmatrix}\r1\\\\\r0\\\\\r0\\end{pmatrix}\\]\rThis process can be repeated with other columns of a matrix to transform it into an upper triangular one. For more details, have a look at the references.\nfirst_column_of_operator_1 = operator_1.to_dense()[:,0]\rnorm = tf.linalg.norm(first_column_of_operator_1)\rvec = first_column_of_operator_1 - norm*tf.linalg.eye(3)[:,0] # Whether to take positive or neagtive sign? See references.\rhouseholder = tf.linalg.LinearOperatorHouseholder(reflection_axis = vec)\rhouseholder.matmul(tf.reshape(first_column_of_operator_1, shape = (3,1))).numpy()\rarray([[8.1240387e+00],\r[4.7683716e-07],\r[4.7683716e-07]], dtype=float32)\rtf.matmul(householder.to_dense(), tf.reshape(first_column_of_operator_1, shape = (3,1)))\r\u0026lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=\rarray([[8.1240387e+00],\r[4.7683716e-07],\r[7.1525574e-07]], dtype=float32)\u0026gt;\r\n\rKronecker matrix\roperator_1.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 3.],\r[4., 5., 6.],\r[7., 8., 9.]], dtype=float32)\u0026gt;\roperator_2.to_dense()\r\u0026lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\rarray([[-9., -8.],\r[-7., -6.]], dtype=float32)\u0026gt;\rkron_operator = tf.linalg.LinearOperatorKronecker([operator_1,operator_2])\rkron_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(6, 6), dtype=float32, numpy=\rarray([[ -9., -8., -18., -16., -27., -24.],\r[ -7., -6., -14., -12., -21., -18.],\r[-36., -32., -45., -40., -54., -48.],\r[-28., -24., -35., -30., -42., -36.],\r[-63., -56., -72., -64., -81., -72.],\r[-49., -42., -56., -48., -63., -54.]], dtype=float32)\u0026gt;\r\n\rPermutation matrix\rperm_operator = tf.linalg.LinearOperatorPermutation(tf.constant([2,4,0,3,1]))\rperm_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[0., 0., 1., 0., 0.],\r[0., 0., 0., 0., 1.],\r[1., 0., 0., 0., 0.],\r[0., 0., 0., 1., 0.],\r[0., 1., 0., 0., 0.]], dtype=float32)\u0026gt;\r\n\r\rCommon matrices using operators\r\nIdentity matrix\riden_operator = tf.linalg.LinearOperatorIdentity(num_rows = 5)\riden_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[1., 0., 0., 0., 0.],\r[0., 1., 0., 0., 0.],\r[0., 0., 1., 0., 0.],\r[0., 0., 0., 1., 0.],\r[0., 0., 0., 0., 1.]], dtype=float32)\u0026gt;\r\n\rScaled identity matrix\rscaled_iden_operator = tf.linalg.LinearOperatorScaledIdentity(num_rows = 5, multiplier = 5.)\rscaled_iden_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[5., 0., 0., 0., 0.],\r[0., 5., 0., 0., 0.],\r[0., 0., 5., 0., 0.],\r[0., 0., 0., 5., 0.],\r[0., 0., 0., 0., 5.]], dtype=float32)\u0026gt;\rscaled_iden_operator_2 = tf.linalg.LinearOperatorScaledIdentity(num_rows = 3, multiplier = tf.constant([-5,7]))\rscaled_iden_operator_2.to_dense()\r\u0026lt;tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=\rarray([[[-5, 0, 0],\r[ 0, -5, 0],\r[ 0, 0, -5]],\r[[ 7, 0, 0],\r[ 0, 7, 0],\r[ 0, 0, 7]]], dtype=int32)\u0026gt;\r\n\rDiagonal matrix\rdiag_operator = tf.linalg.LinearOperatorDiag(tf.constant([1,2,3,4.]))\rdiag_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\rarray([[1., 0., 0., 0.],\r[0., 2., 0., 0.],\r[0., 0., 3., 0.],\r[0., 0., 0., 4.]], dtype=float32)\u0026gt;\r\n\rTri-diagonal matrix\rdiags = tf.constant([[-1, -1, -1, -1, 0],\r[ 2, 2, 2, 2, 2],\r[ 0, -1, -1, -1, -1.]])\rtridiag_operator = tf.linalg.LinearOperatorTridiag(diagonals = diags)\rtridiag_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy=\rarray([[ 2., -1., 0., 0., 0.],\r[-1., 2., -1., 0., 0.],\r[ 0., -1., 2., -1., 0.],\r[ 0., 0., -1., 2., -1.],\r[ 0., 0., 0., -1., 2.]], dtype=float32)\u0026gt;\r\n\rLower triangular matrix\rmat = tf.constant([[2,4,7,8],\r[1,2,3,4],\r[5,8,9,6],\r[4,2,3,1]], dtype = tf.float32)\rlower_tri_operator = tf.linalg.LinearOperatorLowerTriangular(mat)\rlower_tri_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\rarray([[2., 0., 0., 0.],\r[1., 2., 0., 0.],\r[5., 8., 9., 0.],\r[4., 2., 3., 1.]], dtype=float32)\u0026gt;\r\n\rMatrix of zeros\rzeros_operator = tf.linalg.LinearOperatorZeros(num_rows = 4, num_columns = 5, is_square = False, is_self_adjoint=False)\rzeros_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy=\rarray([[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.],\r[0., 0., 0., 0., 0.]], dtype=float32)\u0026gt;\r\n\r\rMatrix operations using operators\r\nLow-rank update\rWhen a low rank matrix is added to a given matrix, the resulting matrix is called a low-rank update of the original matrix. Let’s suppose our original matrix was \\(A\\) and we add a rank 1 update to it. The resulting matrix is \\(B\\). So\r\\[B = A + uv^T\\]\rWhere, \\(u\\), and \\(v\\) are column vectors. It should be noted that low-rank matrix update doesn’t always increase the rank of the original matrix. For example, if rank of \\(A\\) was 2, updating it with a rank 1 matrix will not always make its rank 3. Here is an example.\nA = tf.constant([[1,2,3],\r[2,4,6],\r[3,4,5]], dtype = tf.float32)\ru = tf.constant([[5],\r[6],\r[7]], dtype = tf.float32)\rv = tf.constant([[7],\r[8],\r[9]], dtype = tf.float32)\rB = A + tf.matmul(u,v, transpose_b=True)\rprint(\u0026quot;Rank of A = \u0026quot;, tf.linalg.matrix_rank(A).numpy())\rprint(\u0026quot;Rank of B = \u0026quot;, tf.linalg.matrix_rank(B).numpy())\rRank of A = 2\rRank of B = 2\rWhy is it useful?\nIt turns out that this low rank update appears in many applications. One of the applications is in least squares. Imagine that you have solved the least squares problem using the available data. And now you get some new data. The problem is to get the new least squares fit. Well, you can start form scratch by including the new data to your old data and then fit the model on the whole data. But this is a wasteful approach. A better alternative is to use the new data to modify your old fit. If you do some mathematics, you will arrive at matrix update equation. We will not do the math here. Interested readers can check references.\nComputations can be much faster if we use low-rank matrix update equation. In tensorflow it is done using tf.linalg.OperatorLowRankUpdate operator. Though the operator can handle more than rank 1 update, we will use it only for rank 1 update.\nAs an example, let’s suppose we want to compute the inverse of \\(B\\). We can do so by modifying the inverse of \\(A\\) that we have previously computed. The result is the famous Sherman–Morrison-Woodbury formula.\r\\[B^{-1} = (A+uv^T)^{-1} = A^{-1}-\\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}\\]\nProvided that the denominator is not equal to zero. Note that denominator is a scalar for rank 1 update. This equation show that we can compute new inverse from the old inverse by using matrix-vector and vector-vector multiplications.\noperator.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[1., 2., 3.],\r[2., 3., 5.],\r[7., 8., 9.]], dtype=float32)\u0026gt;\rlow_rank_update = tf.linalg.LinearOperatorLowRankUpdate(operator,u = u, diag_update = None, v = v)\rlow_rank_update.inverse().to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[-2.1666667 , 1. , 0.625 ],\r[ 2.8333333 , -2. , -0.24999982],\r[-0.83333325, 1. , -0.25000012]], dtype=float32)\u0026gt;\rUsing Sherman–Morrison-Woodbury formula.\noperator_inv = operator.inverse()\rsecond_factor_numer = tf.matmul(operator_inv.matmul(u), tf.matmul(v,operator_inv.to_dense(), transpose_a=True))\rsecond_factor_denom = 1 + tf.matmul(v,operator_inv.matmul(u), transpose_a = True)\rupdate_inv = operator.inverse().to_dense() - (1/second_factor_denom)*second_factor_numer\rprint(update_inv)\rtf.Tensor(\r[[-2.1666667 1. 0.6250001 ]\r[ 2.8333333 -2. -0.25 ]\r[-0.83333325 1. -0.25000006]], shape=(3, 3), dtype=float32)\rlow_rank_update.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[36., 42., 48.],\r[44., 51., 59.],\r[56., 64., 72.]], dtype=float32)\u0026gt;\roperator.to_dense() + tf.matmul(u, v, transpose_b = True)\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[36., 42., 48.],\r[44., 51., 59.],\r[56., 64., 72.]], dtype=float32)\u0026gt;\rAlong with inverse, other methods can be applied to LinearOperatorLowRankUpdate.\nlow_rank_update.diag_part().numpy()\rarray([36., 51., 72.], dtype=float32)\r\n\rOperator inversion\rComputes inverse operator of a given operator.\ninv_operator = tf.linalg.LinearOperatorInversion(operator = operator)\rinv_operator.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[-2.1666667 , 1. , 0.16666669],\r[ 2.8333333 , -2. , 0.16666669],\r[-0.83333325, 1. , -0.16666669]], dtype=float32)\u0026gt;\r\n\rOperator composition\rLike composition of function, this operator applies one operator over another. In terms of matrices, it just means matrix multiplication. But the result of composition is another operator. That new operator can be used for further analysis.\noperator_1 = tf.linalg.LinearOperatorFullMatrix([[1,2],\r[2,5],\r[7,-3.]])\roperator_2 = tf.linalg.LinearOperatorFullMatrix([[2,-1,3],\r[-1,4,5.]])\roperator_comp = tf.linalg.LinearOperatorComposition([operator_1,operator_2])\roperator_comp.to_dense()\r\u0026lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\rarray([[ 0., 7., 13.],\r[ -1., 18., 31.],\r[ 17., -19., 6.]], dtype=float32)\u0026gt;\r\n\r\r\rConclusion\rAs we have seen, using only tensorflow we can do quite a bit of linear algebra. In this post, we have only glossed over some of the functionalities. Clever use of the functions and operators will enable us to do much more than what has been covered here. At times, it might feel a little verbose. But the flexibility that it offers will make the exploration a rewarding experience.\n\rReferences\rTensorflow documentation\rDatta, Biswa Nath. Numerical linear algebra and applications. Vol. 116. Siam, 2010.\r(The Book) Golub, Gene H., and Charles F. Van Loan. Matrix computations. Vol. 3. JHU press, 2012.\r\r\r","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"325d2d914f6f0d741e97bd8e60dbc120","permalink":"/post/doing-linear-algebra-using-tensorflow-2/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/post/doing-linear-algebra-using-tensorflow-2/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will explore the ways of doing linear algebra only using tensorflow. We will only import tensorflow and nothing else. As we will see, we can do all the common linear algebra operations without using any other library. This post is very long as it covers almost all the functions that are there in the linear algebra library tf.","tags":["Linear Algebra","Tensorflow"],"title":"Doing Linear Algebra using Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog"],"content":"\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple .csv files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 .csv files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of .csv files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.\nThis post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following codes. Jupyter notebook of this post can be found here.\nWe will use Tensorflow 2 to run our deep learning model. Tensorflow is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.\nOutline:\rCreate 500 \".csv\" files and save it in the folder “random_data” in current directory.\rWrite a generator that reads data from the folder in chunks and preprocesses it.\rFeed the chunks of data to a CNN model and train it for several epochs.\r\r\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.\r* Each .csv file will have one column of data with 1024 entries.\r* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”\rcategory will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.\nimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026quot;./random_data/\u0026quot;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026quot;./random_data/\u0026quot; + eval(\u0026quot;fault_class\u0026quot;) + \u0026quot;_\u0026quot; + \u0026quot;{0:03}\u0026quot;.format(i+1) + \u0026quot;.csv\u0026quot; # This creates file_name\rnp.savetxt(eval(\u0026quot;file_name\u0026quot;), data, delimiter = \u0026quot;,\u0026quot;, header = \u0026quot;V1\u0026quot;, comments = \u0026quot;\u0026quot;)\rprint(str(eval(\u0026quot;number_of_files_in_each_class\u0026quot;)) + \u0026quot; \u0026quot; + eval(\u0026quot;fault_class\u0026quot;) + \u0026quot; files\u0026quot; + \u0026quot; created.\u0026quot;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = glob.glob(\u0026quot;./random_data/*\u0026quot;)\rprint(\u0026quot;Total number of files: \u0026quot;, len(files))\rprint(\u0026quot;Showing first 10 files...\u0026quot;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\r[\u0026#39;./random_data/Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_010.csv\u0026#39;]\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data/Fault_1_001.csv\rprint(files[0][14:21])\rFault_1\rNow that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.\n\r2. Write a generator that reads data in chunks and preprocesses it\rGenerator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. yield keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For our application, we will use a generator that runs indefinitely.\nThe following generator takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there is no need for generators. In case our data size is huge, we can process chunks of it.\nAs we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.\n\r\rClass\rLabel\r\r\r\rFault_1\r0\r\rFault_2\r1\r\rFault_3\r2\r\rFault_4\r3\r\rFault_5\r4\r\r\r\rThe generator will yield both data and labels.\nimport pandas as pd\rimport re # To match regular expression for extracting labels\rdef data_generator(file_list, batch_size = 20):\ri = 0\rwhile True:\rif i*batch_size \u0026gt;= len(file_list): # This loop is used to run the generator indefinitely.\ri = 0\rnp.random.shuffle(file_list)\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size] data = []\rlabels = []\rlabel_classes = [\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]\rfor file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;)) # Change this line to read any other type of file\rdata.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel\rpattern = \u0026quot;^\u0026quot; + eval(\u0026quot;file[14:21]\u0026quot;) # Pattern extracted from file_name\rfor j in range(len(label_classes)):\rif re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes\rlabels.append(j) data = np.asarray(data).reshape(-1,32,32,1)\rlabels = np.asarray(labels)\ryield data, labels\ri = i + 1\rTo read any other file format, inside the generator change the line that reads files. This will enable us to read different file formats, be it .txt or .npz or any other. Preprocessing of data, different from what we have done in this blog, can be done within the generator loop.\nNow we will check whether the generator works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.\nIn the demonstration, we will read files from an ordered list. This will help us check any errors in the code.\ngenerated_data = data_generator(files, batch_size = 10)\rnum = 0\rfor data, labels in generated_data:\rprint(data.shape, labels.shape)\rprint(labels, \u0026quot;\u0026lt;--Labels\u0026quot;) # Just to see the lables\rprint()\rnum = num + 1\rif num \u0026gt; 5: break\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\rRun the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.\nNow we will create a tensorflow dataset using the generator. Tensorflow datasets can conveniently be used to train tensorflow models.\nA tensorflow dataset can be created form numpy arrays or from generators.Here, we will create it using a generator. Use of the previously created generator as it is in tensorflow datasets doesn’t work (Readers can verify this). This happens because of the inability of regular expression to compare a “string” with a “byte string”. “byte strings” are generated by default in tensorflow. As a way around, we will make modifications to the earlier generator and use it with tensorflow datasets. Note that we will only modified three lines. Modified lines are accompanied by commented texts beside it.\nimport tensorflow as tf\rprint(tf.__version__)\r2.2.0\rdef tf_data_generator(file_list, batch_size = 20):\ri = 0\rwhile True:\rif i*batch_size \u0026gt;= len(file_list): i = 0\rnp.random.shuffle(file_list)\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size] data = []\rlabels = []\rlabel_classes = tf.constant([\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]) # This line has changed.\rfor file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;))\rdata.append(temp.values.reshape(32,32,1)) pattern = tf.constant(eval(\u0026quot;file[14:21]\u0026quot;)) # This line has changed\rfor j in range(len(label_classes)):\rif re.match(pattern.numpy(), label_classes[j].numpy()): # This line has changed.\rlabels.append(j)\rdata = np.asarray(data).reshape(-1,32,32,1)\rlabels = np.asarray(labels)\ryield data, labels\ri = i + 1\rTest whether modified generator works or not.\ncheck_data = tf_data_generator(files, batch_size = 10)\rnum = 0\rfor data, labels in check_data:\rprint(data.shape, labels.shape)\rprint(labels, \u0026quot;\u0026lt;--Labels\u0026quot;)\rprint()\rnum = num + 1\rif num \u0026gt; 5: break\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\r(10, 32, 32, 1) (10,)\r[0 0 0 0 0 0 0 0 0 0] \u0026lt;--Labels\rNote that the new generator created by using a few tensorflow commands works just fine as our previous generator. This new generator can now be integrated with a tensorflow dataset.\nbatch_size = 15\rdataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\routput_shapes = ((None,32,32,1),(None,)))\rCheck whether dataset works or not.\nnum = 0\rfor data, labels in dataset:\rprint(data.shape, labels.shape)\rprint(labels)\rprint()\rnum = num + 1\rif num \u0026gt; 7: break\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)\r(15, 32, 32, 1) (15,)\rtf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)\rThis also works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the tf_data_generator create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don’t expect to see good performance. The aim is only to create a pipeline.\n\r3. Building data pipeline and training CNN model\rBefore building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact.\nimport shutil\rCreate five different folders.\nfault_folders = [\u0026quot;Fault_1\u0026quot;, \u0026quot;Fault_2\u0026quot;, \u0026quot;Fault_3\u0026quot;, \u0026quot;Fault_4\u0026quot;, \u0026quot;Fault_5\u0026quot;]\rfor folder_name in fault_folders:\ros.mkdir(os.path.join(\u0026quot;./random_data\u0026quot;, folder_name))\rMove files into those folders.\nfor file in files:\rpattern = \u0026quot;^\u0026quot; + eval(\u0026quot;file[14:21]\u0026quot;)\rfor j in range(len(fault_folders)):\rif re.match(pattern, fault_folders[j]):\rdest = os.path.join(\u0026quot;./random_data/\u0026quot;,eval(\u0026quot;fault_folders[j]\u0026quot;))\rshutil.move(file, dest)\rglob.glob(\u0026quot;./random_data/*\u0026quot;)\r[\u0026#39;./random_data/Fault_1\u0026#39;,\r\u0026#39;./random_data/Fault_2\u0026#39;,\r\u0026#39;./random_data/Fault_3\u0026#39;,\r\u0026#39;./random_data/Fault_4\u0026#39;,\r\u0026#39;./random_data/Fault_5\u0026#39;]\rglob.glob(\u0026quot;./random_data/Fault_1/*\u0026quot;)[:10] # Showing first 10 files of Fault_1 folder\r[\u0026#39;./random_data/Fault_1/Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1/Fault_1_010.csv\u0026#39;]\rglob.glob(\u0026quot;./random_data/Fault_3/*\u0026quot;)[:10] # Showing first 10 files of Falut_3 folder\r[\u0026#39;./random_data/Fault_3/Fault_3_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_3/Fault_3_010.csv\u0026#39;]\rPrepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing.\nfault_1_files = glob.glob(\u0026quot;./random_data/Fault_1/*\u0026quot;)\rfault_2_files = glob.glob(\u0026quot;./random_data/Fault_2/*\u0026quot;)\rfault_3_files = glob.glob(\u0026quot;./random_data/Fault_3/*\u0026quot;)\rfault_4_files = glob.glob(\u0026quot;./random_data/Fault_4/*\u0026quot;)\rfault_5_files = glob.glob(\u0026quot;./random_data/Fault_5/*\u0026quot;)\rfrom sklearn.model_selection import train_test_split\rfault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)\rfault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)\rfault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)\rfault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)\rfault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)\rfault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)\rfault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)\rfault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)\rfault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)\rfault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)\rtrain_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train\rvalidation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val\rtest_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test\r# Shuffle data (We don\u0026#39;t need to shuffle validation and test data)\rnp.random.shuffle(train_file_names)\rprint(\u0026quot;Number of train_files:\u0026quot; ,len(train_file_names))\rprint(\u0026quot;Number of validation_files:\u0026quot; ,len(validation_file_names))\rprint(\u0026quot;Number of test_files:\u0026quot; ,len(test_file_names))\rNumber of train_files: 350\rNumber of validation_files: 50\rNumber of test_files: 100\rbatch_size = 10\rtrain_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], output_shapes = ((None,32,32,1),(None,)),\routput_types = (tf.float32, tf.float32))\rvalidation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\routput_shapes = ((None,32,32,1),(None,)),\routput_types = (tf.float32, tf.float32))\rtest_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\routput_shapes = ((None,32,32,1),(None,)),\routput_types = (tf.float32, tf.float32))\rNow create the model.\nfrom tensorflow.keras import layers\rmodel = tf.keras.Sequential([\rlayers.Conv2D(16, 3, activation = \u0026quot;relu\u0026quot;, input_shape = (32,32,1)),\rlayers.MaxPool2D(2),\rlayers.Conv2D(32, 3, activation = \u0026quot;relu\u0026quot;),\rlayers.MaxPool2D(2),\rlayers.Flatten(),\rlayers.Dense(16, activation = \u0026quot;relu\u0026quot;),\rlayers.Dense(5, activation = \u0026quot;softmax\u0026quot;)\r])\rmodel.summary()\rModel: \u0026quot;sequential\u0026quot;\r_________________________________________________________________\rLayer (type) Output Shape Param # =================================================================\rconv2d (Conv2D) (None, 30, 30, 16) 160 _________________________________________________________________\rmax_pooling2d (MaxPooling2D) (None, 15, 15, 16) 0 _________________________________________________________________\rconv2d_1 (Conv2D) (None, 13, 13, 32) 4640 _________________________________________________________________\rmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 32) 0 _________________________________________________________________\rflatten (Flatten) (None, 1152) 0 _________________________________________________________________\rdense (Dense) (None, 16) 18448 _________________________________________________________________\rdense_1 (Dense) (None, 5) 85 =================================================================\rTotal params: 23,333\rTrainable params: 23,333\rNon-trainable params: 0\r_________________________________________________________________\rCompile the model.\nmodel.compile(loss = \u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer = \u0026quot;adam\u0026quot;, metrics = [\u0026quot;accuracy\u0026quot;])\rBefore we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments steps_per_epoch and validation_steps to desired numbers in model.fit(). Similarly while evaluating model, we need to set the argument steps to a desired number in model.evaluate().\nThere are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set steps_per_epoch to 35. Similarly, validation_steps = 5 and in model.evaluate(), steps = 10.\nsteps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))\rvalidation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))\rsteps = np.int(np.ceil(len(test_file_names)/batch_size))\rprint(\u0026quot;steps_per_epoch = \u0026quot;, steps_per_epoch)\rprint(\u0026quot;validation_steps = \u0026quot;, validation_steps)\rprint(\u0026quot;steps = \u0026quot;, steps)\rsteps_per_epoch = 35\rvalidation_steps = 5\rsteps = 10\rmodel.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\rvalidation_steps = validation_steps, epochs = 10)\rEpoch 1/10\r35/35 [==============================] - 1s 40ms/step - loss: 1.6268 - accuracy: 0.2029 - val_loss: 1.6111 - val_accuracy: 0.2000\rEpoch 2/10\r35/35 [==============================] - 1s 36ms/step - loss: 1.6101 - accuracy: 0.2114 - val_loss: 1.6079 - val_accuracy: 0.2600\rEpoch 3/10\r35/35 [==============================] - 1s 35ms/step - loss: 1.6066 - accuracy: 0.2343 - val_loss: 1.6076 - val_accuracy: 0.2000\rEpoch 4/10\r35/35 [==============================] - 1s 34ms/step - loss: 1.5993 - accuracy: 0.2143 - val_loss: 1.6085 - val_accuracy: 0.2400\rEpoch 5/10\r35/35 [==============================] - 1s 34ms/step - loss: 1.5861 - accuracy: 0.2657 - val_loss: 1.6243 - val_accuracy: 0.2000\rEpoch 6/10\r35/35 [==============================] - 1s 35ms/step - loss: 1.5620 - accuracy: 0.3514 - val_loss: 1.6363 - val_accuracy: 0.2000\rEpoch 7/10\r35/35 [==============================] - 1s 36ms/step - loss: 1.5370 - accuracy: 0.2857 - val_loss: 1.6171 - val_accuracy: 0.2600\rEpoch 8/10\r35/35 [==============================] - 1s 35ms/step - loss: 1.5015 - accuracy: 0.4057 - val_loss: 1.6577 - val_accuracy: 0.2000\rEpoch 9/10\r35/35 [==============================] - 1s 35ms/step - loss: 1.4415 - accuracy: 0.5086 - val_loss: 1.6484 - val_accuracy: 0.1400\rEpoch 10/10\r35/35 [==============================] - 1s 36ms/step - loss: 1.3363 - accuracy: 0.6143 - val_loss: 1.6672 - val_accuracy: 0.2200\r\u0026lt;tensorflow.python.keras.callbacks.History at 0x7fcab40f6150\u0026gt;\rtest_loss, test_accuracy = model.evaluate(test_dataset, steps = 10)\r10/10 [==============================] - 0s 25ms/step - loss: 1.6974 - accuracy: 0.1500\rprint(\u0026quot;Test loss: \u0026quot;, test_loss)\rprint(\u0026quot;Test accuracy:\u0026quot;, test_accuracy)\rTest loss: 1.6973648071289062\rTest accuracy: 0.15000000596046448\rAs expected, model performs terribly.\n\rHow to make predictions?\rUntil now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But often times, for test set, we don’t have access to true labels. Rather we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitray but we will stick with it.\nIf the whole of our prediction set fits into memory, we can just call model.predict() on this data and then use np.argmax() to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.\nYet another pedantic way of doing this is to write a generator to read files from the prediciton set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set.\ndef create_prediction_set(num_files = 20):\ros.mkdir(\u0026quot;./random_data/prediction_set\u0026quot;)\rfor i in range(num_files):\rdata = np.random.randn(1024,)\rfile_name = \u0026quot;./random_data/prediction_set/\u0026quot; + \u0026quot;file_\u0026quot; + \u0026quot;{0:03}\u0026quot;.format(i+1) + \u0026quot;.csv\u0026quot; # This creates file_name\rnp.savetxt(eval(\u0026quot;file_name\u0026quot;), data, delimiter = \u0026quot;,\u0026quot;, header = \u0026quot;V1\u0026quot;, comments = \u0026quot;\u0026quot;)\rprint(str(eval(\u0026quot;num_files\u0026quot;)) + \u0026quot; \u0026quot;+ \u0026quot; files created in prediction set.\u0026quot;)\rCreate some files for prediction set.\ncreate_prediction_set(num_files = 55)\r55 files created in prediction set.\rprediction_files = glob.glob(\u0026quot;./random_data/prediction_set/*\u0026quot;)\rprint(\u0026quot;Total number of files: \u0026quot;, len(prediction_files))\rprint(\u0026quot;Showing first 10 files...\u0026quot;)\rprediction_files[:10]\rTotal number of files: 55\rShowing first 10 files...\r[\u0026#39;./random_data/prediction_set/file_001.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_002.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_003.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_004.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_005.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_006.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_007.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_008.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_009.csv\u0026#39;,\r\u0026#39;./random_data/prediction_set/file_010.csv\u0026#39;]\rNow, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don’t want the generator to run indefinitely. Secondly, we don’t have any labels. So this generator should only yield data. This is how we achieve that.\ndef generator_for_prediction(file_list, batch_size = 20):\ri = 0\rwhile i \u0026lt;= (len(file_list)/batch_size):\rif i == np.floor(len(file_list)/batch_size):\rfile_chunk = file_list[i*batch_size:len(file_list)]\rif len(file_chunk)==0:\rbreak\relse:\rfile_chunk = file_list[i*batch_size:(i+1)*batch_size] data = []\rfor file in file_chunk:\rtemp = pd.read_csv(open(file,\u0026#39;r\u0026#39;))\rdata.append(temp.values.reshape(32,32,1)) data = np.asarray(data).reshape(-1,32,32,1)\ryield data\ri = i + 1\rCheck whether the generator works or not.\npred_gen = generator_for_prediction(prediction_files, batch_size = 10)\rfor data in pred_gen:\rprint(data.shape)\r(10, 32, 32, 1)\r(10, 32, 32, 1)\r(10, 32, 32, 1)\r(10, 32, 32, 1)\r(10, 32, 32, 1)\r(5, 32, 32, 1)\rCreate a tensorflow dataset.\nbatch_size = 10\rprediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],\routput_shapes=(None,32,32,1), output_types=(tf.float32))\rsteps = np.int(np.ceil(len(prediction_files)/batch_size))\rpredictions = model.predict(prediction_dataset,steps = steps)\rprint(\u0026quot;Shape of prediction array: \u0026quot;, predictions.shape)\rpredictions\rShape of prediction array: (55, 5)\rarray([[0.28138927, 0.3383776 , 0.17806269, 0.18918239, 0.01298801],\r[0.16730548, 0.20139892, 0.32996896, 0.16305783, 0.13826886],\r[0.08079846, 0.35669118, 0.4091237 , 0.13286887, 0.02051783],\r[0.01697877, 0.79075295, 0.17063092, 0.01676028, 0.00487713],\r[0.19006915, 0.02615157, 0.39364284, 0.09650648, 0.29362988],\r[0.05416911, 0.682985 , 0.19086388, 0.0668761 , 0.00510592],\r[0.21325852, 0.27782622, 0.10314588, 0.39539766, 0.01037181],\r[0.23633875, 0.3308002 , 0.30727112, 0.09573858, 0.02985144],\r[0.06442448, 0.34153524, 0.47356713, 0.08497778, 0.03549532],\r[0.37901744, 0.32311487, 0.12875995, 0.16359715, 0.00551067],\r[0.12227482, 0.49774405, 0.26021793, 0.1060346 , 0.01372868],\r[0.07139122, 0.17324339, 0.5490784 , 0.10136751, 0.10491937],\r[0.18757634, 0.2833261 , 0.3367256 , 0.14390293, 0.04846917],\r[0.23564269, 0.2800771 , 0.19150141, 0.2686058 , 0.02417296],\r[0.4835618 , 0.03908279, 0.09785527, 0.31918615, 0.06031401],\r[0.03285189, 0.5866938 , 0.3362034 , 0.0313101 , 0.01294078],\r[0.31367007, 0.05583594, 0.24806198, 0.2707511 , 0.1116809 ],\r[0.11204866, 0.05982558, 0.44611645, 0.16678827, 0.21522103],\r[0.04504926, 0.7100154 , 0.16532828, 0.0747861 , 0.00482096],\r[0.22441828, 0.01738338, 0.36729604, 0.0961706 , 0.29473177],\r[0.22392808, 0.23958267, 0.11669649, 0.41423568, 0.00555711],\r[0.11768451, 0.16422512, 0.49695587, 0.13158153, 0.08955302],\r[0.04941175, 0.31670955, 0.46190843, 0.12606393, 0.04590632],\r[0.19507076, 0.03239974, 0.3885634 , 0.14447391, 0.23949222],\r[0.3530666 , 0.08613478, 0.11636773, 0.4088019 , 0.03562902],\r[0.12874755, 0.3140329 , 0.3858064 , 0.1278494 , 0.0435637 ],\r[0.3001929 , 0.02791574, 0.11502622, 0.5044482 , 0.05241694],\r[0.0929171 , 0.1467541 , 0.6005069 , 0.06660035, 0.09322156],\r[0.10712272, 0.5518521 , 0.2632791 , 0.06340495, 0.01434106],\r[0.27723876, 0.25847596, 0.18952209, 0.25228631, 0.02247689],\r[0.12578863, 0.44461673, 0.25048074, 0.14304985, 0.03606399],\r[0.09593316, 0.06914104, 0.49921316, 0.1389045 , 0.19680816],\r[0.22185169, 0.0878747 , 0.33703303, 0.23808932, 0.11515129],\r[0.0850782 , 0.06328611, 0.57307494, 0.08615369, 0.19240707],\r[0.41479778, 0.07033634, 0.22154689, 0.2007963 , 0.09252268],\r[0.22052608, 0.10761442, 0.33570328, 0.25846007, 0.07769614],\r[0.03679338, 0.4369671 , 0.42453632, 0.07080499, 0.03089818],\r[0.17414902, 0.3666445 , 0.26953018, 0.16861232, 0.02106389],\r[0.04334973, 0.04427214, 0.5819794 , 0.02825493, 0.30214384],\r[0.23099631, 0.31964707, 0.31392127, 0.11803907, 0.01739628],\r[0.03072637, 0.6739159 , 0.25826213, 0.0309101 , 0.00618558],\r[0.20030826, 0.05058228, 0.42536664, 0.14415787, 0.17958501],\r[0.25894472, 0.0410106 , 0.25135538, 0.15487678, 0.29381245],\r[0.31544876, 0.05200702, 0.20838396, 0.31984535, 0.10431487],\r[0.10788545, 0.31769663, 0.44471365, 0.08522549, 0.04447879],\r[0.01864015, 0.35556656, 0.551683 , 0.02805553, 0.04605483],\r[0.20043266, 0.1211144 , 0.26670808, 0.33885604, 0.07288874],\r[0.29432756, 0.19128233, 0.19503927, 0.2826192 , 0.03673161],\r[0.2151616 , 0.05391361, 0.34218988, 0.11304423, 0.27569073],\r[0.241943 , 0.05663572, 0.23858468, 0.36390153, 0.09893499],\r[0.24665013, 0.22702417, 0.33673155, 0.11996701, 0.06962712],\r[0.05448309, 0.33466634, 0.49283266, 0.07876839, 0.03924957],\r[0.3060696 , 0.03565398, 0.33453086, 0.12989788, 0.19384763],\r[0.1417291 , 0.40642622, 0.20021752, 0.22896914, 0.02265806],\r[0.10395318, 0.20624556, 0.46823606, 0.12000521, 0.10156006]],\rdtype=float32)\rOutputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use np.argmax() command.\nnp.argmax(predictions, axis = 1)\rarray([1, 2, 2, 1, 2, 1, 3, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 3, 2,\r2, 2, 3, 2, 3, 2, 1, 0, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 4, 3,\r2, 2, 3, 0, 2, 3, 2, 2, 2, 1, 2])\rThe data are randomly generated. So we should not be surprised by this result. Also note that the for each new data, softmax outputs are close to each other. This means that the network is not very sure about the classification result.\nThis brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a generator, and trained a model using that generator. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.\nAs a final note, I want to stress that, this is not the only approach to do the task. As I have mentioned previously, in Tensorflow, you can do the same thing in several different ways. The approach I have chosen seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.\nI hope, this blog will be of help to readers. Please bring any errors or omissions to my notice.\nUpdate: If along with reading, one has to perform complex transformations on extracted data (say, doing spectrogram on each segment of data, etc.), the naive approach presented in this blog may turn out to be slow. But there are ways to make these computations faster. One such speedup technique can be found at this blog.\nLast modified: 27th April, 2020.\n\r","date":1578528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578528000,"objectID":"eef9f04d25b7db2b29a4a4ca2c3f753e","permalink":"/post/reading-multiple-files-in-tensorflow-2/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/post/reading-multiple-files-in-tensorflow-2/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple .csv files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 .csv files. These files have been created using random numbers. Each file contains only 1024 numbers in one column.","tags":["Machine Learning","Deep Learning","Tensorflow"],"title":"Reading multiple files in Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog"],"content":"\r\r(Jupyter notebook for this post can be found here)\r\r\r(Check out this post for an end-to-end data pipeline and training using generators in Tensorflow 2)\r\rIn this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful to the beginner.\nWhile functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It’s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.\nFirst let’s create artificial data that we will extract later batch by batch.\nimport numpy as np\rdata = np.random.randint(100,150, size = (10,2,2))\rlabels = np.random.permutation(10)\rprint(data)\r[[[102 146]\r[141 125]]\r[[120 128]\r[114 119]]\r[[143 110]\r[132 148]]\r[[133 105]\r[126 140]]\r[[116 108]\r[125 103]]\r[[121 125]\r[102 107]]\r[[146 126]\r[130 138]]\r[[136 145]\r[103 135]]\r[[148 100]\r[128 106]]\r[[144 118]\r[149 143]]]\rprint(\u0026quot;labels:\u0026quot;, labels)\rlabels: [6 2 0 9 5 8 3 7 4 1]\rLet’s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.\ndef my_gen(data, labels, batch_size = 2):\ri = 0\rwhile True:\rif i*batch_size \u0026gt;= len(labels):\ri = 0\ridx = np.random.permutation(len(labels))\rdata, labels = data[idx], labels[idx]\rcontinue\relse:\rX = data[i*batch_size:(i+1)*batch_size,:]\ry = labels[i*batch_size:(i+1)*batch_size]\ri += 1\ryield X,y\rNote that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found here. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found here.\nNow that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using “np.shuffle()”.\nget_data = my_gen(data,labels)\rfor i in range(10):\rX,y = next(get_data)\rprint(X,y)\rprint(X.shape, y.shape)\rprint(\u0026quot;=========================\u0026quot;)\r[[[102 146]\r[141 125]]\r[[120 128]\r[114 119]]] [6 2]\r(2, 2, 2) (2,)\r=========================\r[[[143 110]\r[132 148]]\r[[133 105]\r[126 140]]] [0 9]\r(2, 2, 2) (2,)\r=========================\r[[[116 108]\r[125 103]]\r[[121 125]\r[102 107]]] [5 8]\r(2, 2, 2) (2,)\r=========================\r[[[146 126]\r[130 138]]\r[[136 145]\r[103 135]]] [3 7]\r(2, 2, 2) (2,)\r=========================\r[[[148 100]\r[128 106]]\r[[144 118]\r[149 143]]] [4 1]\r(2, 2, 2) (2,)\r=========================\r[[[120 128]\r[114 119]]\r[[133 105]\r[126 140]]] [2 9]\r(2, 2, 2) (2,)\r=========================\r[[[116 108]\r[125 103]]\r[[146 126]\r[130 138]]] [5 3]\r(2, 2, 2) (2,)\r=========================\r[[[136 145]\r[103 135]]\r[[148 100]\r[128 106]]] [7 4]\r(2, 2, 2) (2,)\r=========================\r[[[143 110]\r[132 148]]\r[[121 125]\r[102 107]]] [0 8]\r(2, 2, 2) (2,)\r=========================\r[[[102 146]\r[141 125]]\r[[144 118]\r[149 143]]] [6 1]\r(2, 2, 2) (2,)\r=========================\rIn the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:\nimport tensorflow as tf\rimport math\rclass my_new_gen(tf.keras.utils.Sequence):\rdef __init__(self, data, labels, batch_size= 2 ):\rself.x, self.y = data, labels\rself.batch_size = batch_size\rself.indices = np.arange(self.x.shape[0])\rdef __len__(self):\rreturn math.floor(self.x.shape[0] / self.batch_size)\rdef __getitem__(self, idx):\rinds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\rbatch_x = self.x[inds]\rbatch_y = self.y[inds]\rreturn batch_x, batch_y\rdef on_epoch_end(self):\rnp.random.shuffle(self.indices)\rIn this case we must add len method and getitem method within the class and if we want to shuffle data between epochs, we have to add on_epoch_end() method. len finds out the number of batches possible in an epoch and getitem extracts batches one by one. When one epoch is complete, on_epoch_end() shuffles the data and the process continues. We will test it with an example.\nget_new_data = my_new_gen(data, labels)\rfor i in range(10):\rif i == 5:\rget_new_data.on_epoch_end()\ri = 0\relif i \u0026gt; 5:\ri = i-5\rdat,labs = get_new_data.__getitem__(i)\rprint(dat,labs)\rprint(dat.shape, labs.shape)\rprint(\u0026quot;===========================\u0026quot;)\r[[[102 146]\r[141 125]]\r[[120 128]\r[114 119]]] [6 2]\r(2, 2, 2) (2,)\r===========================\r[[[143 110]\r[132 148]]\r[[133 105]\r[126 140]]] [0 9]\r(2, 2, 2) (2,)\r===========================\r[[[116 108]\r[125 103]]\r[[121 125]\r[102 107]]] [5 8]\r(2, 2, 2) (2,)\r===========================\r[[[146 126]\r[130 138]]\r[[136 145]\r[103 135]]] [3 7]\r(2, 2, 2) (2,)\r===========================\r[[[148 100]\r[128 106]]\r[[144 118]\r[149 143]]] [4 1]\r(2, 2, 2) (2,)\r===========================\r[[[143 110]\r[132 148]]\r[[136 145]\r[103 135]]] [0 7]\r(2, 2, 2) (2,)\r===========================\r[[[102 146]\r[141 125]]\r[[148 100]\r[128 106]]] [6 4]\r(2, 2, 2) (2,)\r===========================\r[[[133 105]\r[126 140]]\r[[144 118]\r[149 143]]] [9 1]\r(2, 2, 2) (2,)\r===========================\r[[[146 126]\r[130 138]]\r[[116 108]\r[125 103]]] [3 5]\r(2, 2, 2) (2,)\r===========================\r[[[120 128]\r[114 119]]\r[[121 125]\r[102 107]]] [2 8]\r(2, 2, 2) (2,)\r===========================\rWe have also used generators to train MNIST example. The code can be found here. The example might seem bit stretched as we don’t need generators for small data sets like MNIST. The aim of the example is just to show different implementation using generators.\nPerhaps the most detailed blog about using generators for deep learning is this one. I also found these comments helpful.\nUpdate: With the release of tensorflow-2.0, it is much easier to use tf.data.Dataset API for handling large datasets. Generators can still be used for training using tf.keras. As a final note, use generators if it is absolutely essential to do so. Otherwise, use tf.data.Dataset API. Check out this post for an end-to-end data pipeline and training using generators in Tensorflow 2.\n","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"8a8ff85778a3ea2a7dc201941fd20da8","permalink":"/post/using-python-generators/","publishdate":"2019-06-29T00:00:00Z","relpermalink":"/post/using-python-generators/","section":"post","summary":"(Jupyter notebook for this post can be found here)\r\r\r(Check out this post for an end-to-end data pipeline and training using generators in Tensorflow 2)\r\rIn this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it.","tags":["Python","Deep Learning"],"title":"Using Python Generators","type":"post"},{"authors":null,"categories":["Blog"],"content":"\r\rThis story was originally written for “Augmenting Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. Full list of awardees and their stories can be found here.\n\rPrelude\rRising sun with its gentle light marks the arrival of morning. Birds’ chirp as well as time on our clock, sometimes with a blaring alarm, confirm the arrival of morning. Each of these, among several others, is an indicator of the morning. But can we know about morning by following only one indicator? Let’s deliberate. What if the sky is cloudy and we don’t see the sun rising, will this mean that morning is yet to come? Of course not! Our alarm will remind us of morning irrespective of whether there is sun or not. But what if, on some occasion, our clock doesn’t work. In that case, birds may chirp or sun may rise or our near and dear ones may remind us that it’s morning already. So in essence, we usually don’t look for only one indicator, rather we consider several indicators. If one indicator fails, we can check another and thus be sure. It is very unlikely that all the indicators will fail simultaneously.\nSo the best way to get an idea about an event, it seems, is not to rely on only one indicator. Rather, observe several indicators and depending on their collective state, arrive at some conclusion. In this way, we deliberately add redundancy in order to get reliable results. This is exactly what we do in fault diagnosis of machines. Fault diagnosis is a broad term that addresses mainly three questions. First, find out whether fault is there in the machine or not. If fault is present, next question is to find the location of the fault. Once location of the fault is found, finally, find out the type of fault and its severity. In this article, we will only limit ourselves to the last aspect. But for simplicity, we will still use the term fault diagnosis to address that particular problem.\n\rThe method\rTo determine the health of a machine, we collect a set of indicators that best explain the condition of the machine. In scientific jargon, we call those features. Before discussing further let’s first discuss what are those features and how they are calculated.\nFirst, data needs to be collected from a machine whose health needs to be assessed. Data might pertain to vibration level of the machine or its temperature distribution or the sound produced by the machine or something else. Sensors are needed to collect each type of data. By analogy, a thermometer, which is used to measure body temperature of humans, is a sensor that measures temperature. Likewise different types of sensors are available to measure different quantities of interest related to the machine. From research it has been found that vibration based data are more suitable for fault diagnosis as compared to other types of data, say temperature or sound. So in this article, we will limit our attention to vibration based fault diagnosis. And the sensor that is most commonly used to measure the vibration of a machine is called an accelerometer. Form the data collected by accelerometer(s) we calculate features like the maximum level of vibration, similarly, the minimum level and other statistical features like skewness, kurtosis, etc. It is not uncommon to collect 10-15 features.\nAfter feature collection, the next task is to find out what type of faults are present by using those features. One way to do this is by comparing the obtained feature values to pre-existing standards. But standards are available for few specialized cases when each feature is considered in isolation. For multiple features, no concrete information can be obtained from standards. The way out of this problem is to come up with an algorithm that takes all feature values as input and produces the output related to the type of fault present.\nConstruction of such an algorithm requires prior faulty and non-faulty data of similar machine be fed to it. The algorithm should ideally work well on this prior data. Once fine-tuning of its parameters are done, new data are fed into the algorithm and from its output, we infer the fault type. If the algorithm is carefully constructed, error in prediction of fault type will be very small. In some cases, it is also possible to get perfect accuracy. The problem just considered is a sub-class of a broad field called pattern recognition. In pattern recognition, we try to find underlying patterns in features that correspond to different fault types. This type of pattern recognition tasks are best performed by machine learning algorithms. The simple technique just described works fine for a large class of problems. But there exist some problems for which the features previously calculated are not sufficient to identify fault. However, it is possible to modify the technique by using transformation of data as well as features. Transformations are a way of converting the original data into another type such that after transformation more insight is gained out of it. This is similar to using logarithms in mathematics to do complex calculations. While direct computation of complex multiplications and divisions is difficult, using logarithm we transform the original problem into a simpler form that can be solved easily in less time. The transformation trick along with pattern recognition methods are surprisingly effective for most fault diagnosis task.\n\rSome recent advances\rUp to this point, we have argued that redundancy is important. It helps us take reliable decisions. However, it requires collection of huge amounts of data. Thus, continuous monitoring of machine, also known as online monitoring, becomes infeasible. So we seek an algorithm that is capable of finding fault types using only a few measurements. One way to do this is to select a few important features that can perform fault diagnosis. Research shows that it is indeed possible. But merely finding best features is not enough. Because to calculate the features, even though small in number, we need to collect all data. Hence issues related to online monitoring will still exist. A way around this problem is not to collect all data but only a fraction of it randomly in time. And the data should be collected in such a way that all information regarding the machine can be extracted from these limited observations. An even optimistic goal is to reconstruct the original data from the limited collected data. By analogy, this is similar to reconstructing the speech of a person, who speaks, say, 3000 words, from 300 random words that you have remembered of their entire speech. The problem just described is known as compressed sensing. And no matter how much counter-intuitive it may seem, encouraging results for this problem have been obtained in signal processing and these methods are beginning to get applied to problems of fault diagnosis. The problem is still in its infancy in fault diagnosis field.\n\rWhat we learned (and what we didn’t!)\rIn summary, we have learned that to diagnose faults, we need multiple features and sometimes we have to transform the data into different domains for better accuracy. We then observed that we can get rid of the redundancy inherent in this method by using compressed sensing methods. All these techniques come under data-driven methods. It is called data-driven because all analyses are done after we collect relevant data from the machine. These methods are quite general purpose and can be used to diagnose faults in different components, say detecting faults in cars or in other machines.\nApart from data-driven methods there also exists another class of techniques that go by the broad name of model-based methods. In model-based methods, we formulate a full mathematical model of the machine and then try to find out how the response of the model changes if a fault is introduced and using this fact, try to find the nature of fault for a new problem. Though model-based techniques are important in their own right, in some cases it becomes very difficult to find an accurate model of the system. In contrast, data-driven methods are more robust against external noise and flexible, meaning we can perform different analysis using the same data and obtain different insights. Another advantage of using data-driven methods is that the whole process of fault diagnosis can easily be automated.\nIn this article, we have only considered the field of fault diagnosis. In fault diagnosis, faults are already present and we wish to either detect them or segregate them depending on fault type. But there exists another branch that deals with ways to predict the time of occurrence of fault in future, given the present state. Basically, they determine the remaining useful life of the machine. This sub-branch is called fault prognosis which is also an active area of research.\nGiven the advancement of research and scope for automation, it may be possible, in not so distant future, to get updates on your phone about possible malfunction of a part of your car while driving your car or while enjoying a ride in a self-driving car, maybe!!\nPublished story can be found at this link\n\r","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"d3115ef77297a658af8e830ead2c1e11","permalink":"/post/fault-diagnosis-of-machines/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/fault-diagnosis-of-machines/","section":"post","summary":"This story was originally written for “Augmenting Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. Full list of awardees and their stories can be found here.\n\rPrelude\rRising sun with its gentle light marks the arrival of morning.","tags":["Story"],"title":"Fault Diagnosis of Machines","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rAlmost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable. Then the final solution is obtained by back substituting the obtained value of the variable into remaining equations. This method, in mathematical jargon, is called Gaussian elimination and back substitution.\nIt turns out (surprisingly) that linear systems form the basis of many interesting engineering applications. Ultimately the problem boils down to solution (or approximate solution) of a system of linear equations. So a thorough understanding of linear systems is essential to appreciate the applications. In this post we will outline all possible cases of finding solutions to linear systems and briefly outline two most important applications.\nWe will use matrix notation to represent the equations succinctly. It also gives us better insight into their solution. Using matrix notation the system can be represented as\r\\[\\textbf{Ax}=\\textbf{b}\\]\rWhere \\(\\textbf{A}\\) is the matrix of coefficients of size \\((m\\times n)\\), \\(\\textbf{x}\\) is a vector of variables of size \\((n\\times 1)\\), and \\(\\textbf{b}\\)\ris a vector of size \\((m\\times 1)\\) representing constant right hand sides. Note that \\(\\textbf{b}\\) can be a vector of all zeros, i.e., \\(\\textbf{b} = \\textbf{0}\\) or it can be any arbitrary vector with some nonzero values, i.e.,\\(\\textbf{b}\\neq \\textbf{0}\\). The solution(s) of linear systems depend to a large extent on what the right hand side is as we will see shortly.\nApart from notation, we need two other concepts from matrix theory. One is of rank and other is the range space (or column space) of a matrix. Rank \\((Rank(\\textbf{A}))\\) of a matrix (say, \\(\\textbf{A}\\)) is defined as number of independent rows or columns of a matrix. It is a well known result in matrix theory that row rank (number of independent rows) is equal to column rank (number of independent columns) and \\(Rank(\\textbf{A})\\leq min(m,n)\\).\nRange space \\((\\mathcal{R}(A))\\)(in short, Range) of a matrix is the vector space of all possible linear combinations of columns of the matrix. As we take all possible linear combination of columns, it is also known as column space. Readers who are slightly more familiar with linear algebra may know that Range is the span of columns of \\(\\textbf{A}\\). Zero vector \\((\\textbf{0})\\) is always in the range of \\(\\textbf{A}\\) because if we take linear combination of columns of \\(\\textbf{A}\\) with all coefficients as 0’s, we get zero vector. Hence \\(\\textbf{b}=0 \\in \\mathcal{R}(\\textbf{A})\\) is always true.\nLet’s now discuss different cases separately and their solutions. We will assume that our system of equations has real entries.\n\rCase - I: \\((m = n)\\)\n\r\\(Rank(\\textbf{A}) = m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution (for any \\(\\textbf{b}\\)). For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 8 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : Impossible (This case will never happen because \\(Rank(\\textbf{A})=m\\))\r\r\\(Rank(\\textbf{A}) \u0026lt; m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r8\r\\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r1 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\rCase - II: \\((m \u0026gt; n)\\)\n\r\\(Rank(\\textbf{A}) = n\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution. But this case is immensely useful from application point of view. Sometimes it is not desirable to obtain the exact solution. Rather an approximate solution suffices for all practical purposes. Finding an approximate solution to an overdetermined system leads to the famous Least Squares problem.\r\r\\(Rank(\\textbf{A}) \u0026lt; n\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 4 \\\\\r3 \u0026amp; 6 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r9\r\\end{bmatrix}\r\\end{equation}\\] It has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 4 \\\\\r3 \u0026amp; 6 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r8\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\rCase - III: \\((m \u0026lt; n)\\)\n\r\\(Rank(\\textbf{A}) = m\\) :\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 5 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r2 \\\\\r3 \\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions. This case is also used in many applications. As there are infinitely many solutions, a natural choice is to choose the best solution. The qualifier ‘best’ determines what application we have in our mind. If we seek minimum \\((l_2)\\) norm, we get the so called minimum energy solution, a concept used in signal processing. Yet another concern is to seek for the sparsest solution (a solution with only a few nonzero entries and all other entries being zero). This idea is used in Compressed Sensing, an active research area with many interesting applications.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : Impossible. This case will never happen since \\(Rank(\\textbf{A})=m\\).\r\r\\(Rank(\\textbf{A}) \u0026lt; m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r4 \\\\\r8 \\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example, \\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r1 \\\\\r5 \\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\r\rHope this post gives a clear overview of linear systems of equations. Interested reader may explore further applications. Comments and clarifications are welcome.\n","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"e805d2cc09051cefef839cab06b96815","permalink":"/post/revisiting-systems-of-linear-equations/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/post/revisiting-systems-of-linear-equations/","section":"post","summary":"Almost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable.","tags":["Linear Algebra"],"title":"Revisiting Systems of Linear Equations","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rRun Python codes in Google Colab\r\rDownload Python codes\r\rDownload R codes (R Markdown)\r\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).\nThis post is Part-III of a three part series on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\r\rThis post contains code snippets in R. Equivalent MATLAB codes can be written using commands of Part-II. For figures, the reader has to write his/her own code in MATLAB.\nStructure of the paper\rAlong with basic theory, the paper contains three examples on PCA, one example on correspondence analysis, and one example on multiple factor analysis. We will only focus on PCA examples in this post.\nTo run following R codes seamlessly, readers have to load following packages. If these packages have not been installed previously, use install.packages(\"package_name\") to install those.\nlibrary(ggplot2)\rlibrary(ggrepel)\r\rHow to get data\rData for the examples have been taken from the paper [1]. The datasets are pretty small. So one way to read the data is to create a dataframe itself in R using the values given in paper. Otherwise, the values can first be stored in a csv file and then read into R. To make this post self-sufficient, we will adopt the former approach.\nNote: Throughout this article, additional comments have been made beside code segments. It would be a good idea to read those commented lines along with the codes.\n# Table 1\r# Create a dataframe\rWords = c(\u0026quot;Bag\u0026quot;, \u0026quot;Across\u0026quot;, \u0026quot;On\u0026quot;, \u0026quot;Insane\u0026quot;, \u0026quot;By\u0026quot;, \u0026quot;Monastery\u0026quot;, \u0026quot;Relief\u0026quot;, \u0026quot;Slope\u0026quot;, \u0026quot;Scoundrel\u0026quot;, \u0026quot;With\u0026quot;, \u0026quot;Neither\u0026quot;, \u0026quot;Pretentious\u0026quot;, \u0026quot;Solid\u0026quot;, \u0026quot;This\u0026quot;, \u0026quot;For\u0026quot;, \u0026quot;Therefore\u0026quot;, \u0026quot;Generality\u0026quot;, \u0026quot;Arise\u0026quot;, \u0026quot;Blot\u0026quot;, \u0026quot;Infectious\u0026quot;)\rWord_length = c(3, 6, 2, 6, 2, 9, 6, 5, 9, 4, 7, 11, 5, 4, 3, 9, 10, 5, 4, 10)\rLines_in_dict = c(14, 7, 11, 9, 9, 4, 8, 11, 5, 8, 2, 4, 12, 9, 8, 1, 4, 13, 15, 6)\rwords = data.frame(Words, Word_length, Lines_in_dict, stringsAsFactors = F)\rwords\r Words Word_length Lines_in_dict\r1 Bag 3 14\r2 Across 6 7\r3 On 2 11\r4 Insane 6 9\r5 By 2 9\r6 Monastery 9 4\r7 Relief 6 8\r8 Slope 5 11\r9 Scoundrel 9 5\r10 With 4 8\r11 Neither 7 2\r12 Pretentious 11 4\r13 Solid 5 12\r14 This 4 9\r15 For 3 8\r16 Therefore 9 1\r17 Generality 10 4\r18 Arise 5 13\r19 Blot 4 15\r20 Infectious 10 6\r(words_centered = scale(words[,2:3],scale = F)) # Centering after reemoving the first column\r Word_length Lines_in_dict\r[1,] -3 6\r[2,] 0 -1\r[3,] -4 3\r[4,] 0 1\r[5,] -4 1\r[6,] 3 -4\r[7,] 0 0\r[8,] -1 3\r[9,] 3 -3\r[10,] -2 0\r[11,] 1 -6\r[12,] 5 -4\r[13,] -1 4\r[14,] -2 1\r[15,] -3 0\r[16,] 3 -7\r[17,] 4 -4\r[18,] -1 5\r[19,] -2 7\r[20,] 4 -2\rattr(,\u0026quot;scaled:center\u0026quot;)\rWord_length Lines_in_dict 6 8 \r\rCovariance PCA\rCovariance PCA uses centered data matrix. But data matrix is not scaled. prcomp() centers data by default.\npca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA\rfactor_scores_words = pca_words_cov$x\rround(factor_scores_words,2)\r PC1 PC2\r[1,] -6.67 0.69\r[2,] 0.84 -0.54\r[3,] -4.68 -1.76\r[4,] -0.84 0.54\r[5,] -2.99 -2.84\r[6,] 4.99 0.38\r[7,] 0.00 0.00\r[8,] -3.07 0.77\r[9,] 4.14 0.92\r[10,] -1.07 -1.69\r[11,] 5.60 -2.38\r[12,] 6.06 2.07\r[13,] -3.91 1.30\r[14,] -1.92 -1.15\r[15,] -1.61 -2.53\r[16,] 7.52 -1.23\r[17,] 5.52 1.23\r[18,] -4.76 1.84\r[19,] -6.98 2.07\r[20,] 3.83 2.30\rObserver that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as principal directions are orthogonal.\n\rPrincipal directions are orthogonal\r# It can also be checked that both the principal components are orthogonal.\rsum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal\r[1] -4.773959e-15\r\rContribution of each factor\rIt is defined as square of factor score divided by sum of squares of factor scores in that column.\nround(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)\r [1] 11.36 0.18 5.58 0.18 2.28 6.34 0.00 2.40 4.38 0.29 8.00 9.37\r[13] 3.90 0.94 0.66 14.41 7.78 5.77 12.43 3.75\rround(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)\r [1] 0.92 0.55 5.98 0.55 15.49 0.28 0.00 1.13 1.63 5.48 10.87 8.25\r[13] 3.27 2.55 12.32 2.90 2.90 6.52 8.25 10.18\rThe calculations in above two lines can be done in a single line\nround(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)\r PC1 PC2\r[1,] 11.36 0.92\r[2,] 0.18 0.55\r[3,] 5.58 5.98\r[4,] 0.18 0.55\r[5,] 2.28 15.49\r[6,] 6.34 0.28\r[7,] 0.00 0.00\r[8,] 2.40 1.13\r[9,] 4.38 1.63\r[10,] 0.29 5.48\r[11,] 8.00 10.87\r[12,] 9.37 8.25\r[13,] 3.90 3.27\r[14,] 0.94 2.55\r[15,] 0.66 12.32\r[16,] 14.41 2.90\r[17,] 7.78 2.90\r[18,] 5.77 6.52\r[19,] 12.43 8.25\r[20,] 3.75 10.18\r\rSquared distance to center of gravity\r(dist = rowSums(factor_scores_words^2))\r [1] 45 1 25 1 17 25 0 10 18 4 37 41 17 5 9 58 32 26 53 20\r\rSquared cosine of observations\r(sq_cos = round(factor_scores_words^2/rowSums(factor_scores_words^2)*100))\r PC1 PC2\r[1,] 99 1\r[2,] 71 29\r[3,] 88 12\r[4,] 71 29\r[5,] 53 47\r[6,] 99 1\r[7,] NaN NaN\r[8,] 94 6\r[9,] 95 5\r[10,] 29 71\r[11,] 85 15\r[12,] 90 10\r[13,] 90 10\r[14,] 74 26\r[15,] 29 71\r[16,] 97 3\r[17,] 95 5\r[18,] 87 13\r[19,] 92 8\r[20,] 74 26\rNan’s are produced because of division by zero.\n# Figue 1\rp = ggplot(words,aes(x = Lines_in_dict,y = Word_length,label = Words))+\rgeom_point()+ geom_text_repel()+ geom_hline(yintercept = 6)+geom_vline(xintercept = 8)+\rlabs(x = \u0026quot;Lines in dictionary\u0026quot;,y = \u0026quot;Word length\u0026quot;)\rprint(p)\r# Show directions of PCs\r# Note that intercept argument in geom_abline considers the line to be at the origin. In our case the data are mean shifted.\r# So we have to adjust the intercept taking new origin into consideration. These adjustments have been made below.\rslope1 = pca_words_cov$rotation[1,1]/pca_words_cov$rotation[2,1] # Slope of first PC\rslope2 = pca_words_cov$rotation[1,2]/pca_words_cov$rotation[2,2] # Slope of second PC\r(new_origin = c(mean(words$Lines_in_dict),mean(words$Word_length)))\r[1] 8 6\rintercept1 = 6 - slope1*8\rintercept2 = 6 - slope2*8\rp+geom_abline(slope = slope1,intercept = intercept1,linetype = \u0026quot;dashed\u0026quot;,size = 1.2,col = \u0026quot;red\u0026quot;)+\rgeom_abline(slope = slope2,intercept = intercept2,linetype = \u0026quot;dashed\u0026quot;,size = 1.2,col = \u0026quot;blue\u0026quot;)\rIn the above figure red dashed line is the 1st principal component (PC) and blue dashed line is the 2nd PC.\n\rRotated PCs\rThis figure is obtained by plotting factor scores. Note that we will plot negative of the factor scores of 1st PC to make the figure consistent with the paper.\nggplot(as.data.frame(pca_words_cov$x),aes(-pca_words_cov$x[,1],pca_words_cov$x[,2],label = words$Words))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Factor score along PC1\u0026quot;)+ylab(\u0026quot;Factor score along PC2\u0026quot;)\r\rWith supplementary data\rGiven a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix.\n\rFactor score of the new word ‘sur’\rsur = c(3,12) # It has 3 letter and 12 lines of dictionary entry\r(sur_centered = sur - colMeans(words[,2:3]))\r Word_length Lines_in_dict -3 4 \r(factor_scores_sur = round(sur_centered %*% pca_words_cov$rotation,2))\r PC1 PC2\r[1,] -4.99 -0.38\r\rEigenvalues and variance\rSee Part-II for details.\n\rTotal variance before transformation\r(total_var_before = round(sum(diag(var(words_centered))),3))\r[1] 23.368\r\rTotal variance after transformation\r(total_var_after = round(sum(diag(var(pca_words_cov$x))),3))\r[1] 23.368\rCorrelation between principal components and original variables\r(In the paper,this correlation is also termed loading. But we will strictly reserve the loading term to mean loading matrix \\(\\textbf{P}\\) (see Part-I)\nThe sum of correlation coefficients between variables and principal components is 1. Intuitively, this means that variables are orthogonally projected onto the principal components.\n\rCorrelation matrix\r# Correlation between PCs and original variables\r(cor(pca_words_cov$x,words_centered))\r Word_length Lines_in_dict\rPC1 0.8679026 -0.9741764\rPC2 0.4967344 0.2257884\rNote that the answers for correlation coefficients don’t match with that of the paper. Readers who get actual answers as given in paper are encouraged to send me an email using my contact details. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below.\n\rSquared correlation\r(cor(pca_words_cov$x,words_centered)^2)\r Word_length Lines_in_dict\rPC1 0.7532549 0.94901961\rPC2 0.2467451 0.05098039\rSum of correlation coefficients between variables and principal components is 1.\ncolSums((cor(pca_words_cov$x,words_centered)^2))\r Word_length Lines_in_dict 1 1 \r\rLoading matrix\r(loading_matrix = pca_words_cov$rotation)\r PC1 PC2\rWord_length 0.5368755 0.8436615\rLines_in_dict -0.8436615 0.5368755\r\rCorrelation score for supplementary variables\r# Supplementary variable (Table 4)\rFrequency = c(8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1)\rNum_entries = c(6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2)\rsupp_data = data.frame(Frequency,Num_entries) # Supplementary data\rsupp_data\r Frequency Num_entries\r1 8 6\r2 230 3\r3 700 12\r4 1 2\r5 500 7\r6 1 1\r7 9 1\r8 2 6\r9 1 1\r10 700 5\r11 7 2\r12 1 1\r13 4 5\r14 500 9\r15 900 7\r16 3 1\r17 1 1\r18 10 4\r19 1 4\r20 1 2\r\rCentered supplementary data\rsupp_data_cent = scale(supp_data,scale = F) # Centered supplementary data\r\rCorrelation score for supplementary data\r(corr_score_supp = round(cor(pca_words_cov$x,supp_data),4))\r Frequency Num_entries\rPC1 -0.3012 -0.6999\rPC2 -0.7218 -0.4493\rNote that correlation score doesn’t depend on whether supplementary data is centered or not.\n(round(cor(pca_words_cov$x,supp_data_cent),4))\r Frequency Num_entries\rPC1 -0.3012 -0.6999\rPC2 -0.7218 -0.4493\r\rSquared correlation\r(round(cor(pca_words_cov$x,supp_data_cent)^2,4))\r Frequency Num_entries\rPC1 0.0907 0.4899\rPC2 0.5210 0.2019\r\rColumn sums of squared correlation for support data\r(round(colSums(cor(pca_words_cov$x,supp_data_cent)^2),4))\r Frequency Num_entries 0.6118 0.6918 \r\rCorrelation circle plot\r# First plot correlation circle\rx = seq(0,2*pi,length.out = 300)\rcircle = ggplot() + geom_path(data = data.frame(a = cos(x),b = sin(x)),\raes(cos(x),sin(x)),alpha = 0.3, size = 1.5)+\rgeom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rannotate(\u0026quot;text\u0026quot;,x = c(1.08,0.05),y = c(0.05,1.08),label = c(\u0026quot;PC1\u0026quot;,\u0026quot;PC2\u0026quot;),angle = c(0,90))+\rxlab(NULL)+ylab(NULL)\r# Plotting original variables\rcor_score = as.data.frame(cor(words_centered,pca_words_cov$x))\rvariable_plot_original = circle + geom_point(data = cor_score, aes(cor_score[,1],cor_score[,2]))+\rgeom_text_repel(aes(cor_score[,1],cor_score[,2],\rlabel = c(\u0026quot;Length of words\u0026quot;,\u0026quot;Number of lines in Dict.\u0026quot;))) print(variable_plot_original)\r\rPlotting supplementary variables\rvariable_plot_original+\rgeom_point(data = as.data.frame(corr_score_supp),\raes(corr_score_supp[,1],corr_score_supp[,2]))+\rgeom_text_repel(aes(corr_score_supp[,1],corr_score_supp[,2],\rlabel = c(\u0026quot;Frequency\u0026quot;,\u0026quot;Number of entries\u0026quot;))) \rObserve that our correlation circle plot is flipped about y-axis (i.e., PC2) when compared to the plot given in paper. This is because our first principal component is negative of the one given in paper. So while computing correlation score, this negative principal component results in negative correlation scores. Hence, our plot flips about y-axis.\n\rExample 2 (Wine example)\rCorrelation PCA with wine data\r# Table 6\rwine_type = c(paste(\u0026quot;wine\u0026quot;, 1:5, sep = \u0026quot;_\u0026quot;))\rhedonic = c(14, 10, 8, 2, 6)\rfor_meat = c(7, 7, 5, 4, 2)\rfor_dessert = c(8, 6, 5, 7, 4)\rprice = c(7, 4, 10, 16, 13)\rsugar = c(7, 3, 5, 7, 3)\ralcohol = c(13, 14, 12, 11, 10)\racidity = c(7, 7, 5, 3, 3)\rwine = data.frame(wine_type, hedonic, for_meat, for_dessert, price, sugar, alcohol, acidity, stringsAsFactors = F)\rwine\r wine_type hedonic for_meat for_dessert price sugar alcohol acidity\r1 wine_1 14 7 8 7 7 13 7\r2 wine_2 10 7 6 4 3 14 7\r3 wine_3 8 5 5 10 5 12 5\r4 wine_4 2 4 7 16 7 11 3\r5 wine_5 6 2 4 13 3 10 3\rpca_wine_cor = prcomp(wine[2:8],scale = T)\rggplot(as.data.frame(pca_wine_cor$x),aes(x = pca_wine_cor$x[,1],y = pca_wine_cor$x[,2], label = paste0(\u0026quot;wine \u0026quot;,1:5)))+\rgeom_point()+geom_text_repel()+ geom_vline(xintercept = 0)+ geom_hline(yintercept = 0)+\rxlab(\u0026quot;Factor score along PC1\u0026quot;)+ylab(\u0026quot;Factor score along PC2\u0026quot;)\rAgain our figure seems upside down than that of the paper. This is a minor discrepancy. Our 2nd eigenvector is negative of the one considered in paper. We can match the plot with that of the paper by just flipping the second principal component but we will not do that here.\n\rFactor scores along 1st and 2nd PC\r# Table 7\r(pca_wine_cor$x[,1:2])\r PC1 PC2\r[1,] -2.3301649 1.095284\r[2,] -2.0842419 -1.223185\r[3,] 0.1673228 -0.370258\r[4,] 1.7842392 1.712563\r[5,] 2.4628448 -1.214405\r\rContribution of each observation to principal component\rround(pca_wine_cor$x[,1:2]^2/matrix(rep(colSums(pca_wine_cor$x[,1:2]^2),nrow(wine)),ncol = 2,byrow = T)*100,2)\r PC1 PC2\r[1,] 28.50 16.57\r[2,] 22.80 20.66\r[3,] 0.15 1.89\r[4,] 16.71 40.51\r[5,] 31.84 20.37\r\rSquared cosine of observations of first PC\r(sq_cos = round(pca_wine_cor$x[,1:2]^2/rowSums(pca_wine_cor$x^2)*100))\r PC1 PC2\r[1,] 77 17\r[2,] 69 24\r[3,] 7 34\r[4,] 50 46\r[5,] 78 19\r\rLoading scores corresponding to first two principal components\r(round(pca_wine_cor$rotation[,1:2],2))\r PC1 PC2\rhedonic -0.40 -0.11\rfor_meat -0.45 0.11\rfor_dessert -0.26 0.59\rprice 0.42 0.31\rsugar -0.05 0.72\ralcohol -0.44 -0.06\racidity -0.45 -0.09\r\rCorrelation score variables with first two principal components\r(corr_score_wine = round(cor(pca_wine_cor$x,wine[,2:8])[1:2,],2))\r hedonic for_meat for_dessert price sugar alcohol acidity\rPC1 -0.87 -0.97 -0.58 0.91 -0.11 -0.96 -0.99\rPC2 -0.15 0.15 0.79 0.42 0.97 -0.07 -0.12\r\rCorrelation circle for wine data\r# Figure 6\rcorr_score_wine = t(corr_score_wine)\rcircle + geom_point(data = as.data.frame(corr_score_wine),\raes(corr_score_wine[,1],corr_score_wine[,2]))+\rgeom_text_repel(aes(corr_score_wine[,1],corr_score_wine[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))\r\rVarimax rotation\rRotation is applied to loading matrix such that after rotation principal components are interpretable. By interpretable, we mean, some of the loading scores will have higher values and some other loading scores will have lower values. So it can be said that the variables whose loading scores have higher value, contribute significantly towards principal components as compared to other variables with lesser loading scores. Though rotation works in certain cases, it must be remembered that it is no magic wand for principal component interpretability. One of the popular rotations is Varimax rotation. R has a built-in command to perform varimax rotation.\nVarimax rotation can be performed on the whole loading matrix or on a few components only. In the paper, varimax has been applied to first two principal components.\n\rLoading scores of first two principal components\r(round(pca_wine_cor$rotation[,1:2],2))\r PC1 PC2\rhedonic -0.40 -0.11\rfor_meat -0.45 0.11\rfor_dessert -0.26 0.59\rprice 0.42 0.31\rsugar -0.05 0.72\ralcohol -0.44 -0.06\racidity -0.45 -0.09\r\rVarimax applied to first two principal components\rrotated_loading_scores = varimax(pca_wine_cor$rotation[,1:2])\r\rLoading scores after rotation\r# Table 10\r(round(rotated_loading_scores$loadings[,1:2],2))\r PC1 PC2\rhedonic -0.41 -0.02\rfor_meat -0.41 0.21\rfor_dessert -0.12 0.63\rprice 0.48 0.21\rsugar 0.12 0.72\ralcohol -0.44 0.05\racidity -0.46 0.02\rThe same result can also be obtained by multiplying the original loading matrix by the rotation matrix obtained from varimax.\n(round(pca_wine_cor$rotation[,1:2] %*% rotated_loading_scores$rotmat,2))\r [,1] [,2]\rhedonic -0.41 -0.02\rfor_meat -0.41 0.21\rfor_dessert -0.12 0.63\rprice 0.48 0.21\rsugar 0.12 0.72\ralcohol -0.44 0.05\racidity -0.46 0.02\r\rPlot of loading scores before rotation\r#Figure 7\rggplot(as.data.frame(pca_wine_cor$rotation[,1:2]),aes(x = pca_wine_cor$rotation[,1],y = pca_wine_cor$rotation[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Loading score along PC1\u0026quot;)+ylab(\u0026quot;Loading score along PC2\u0026quot;)\r\rPlot of loading scores after rotation\rggplot(as.data.frame(rotated_loading_scores$loadings[,1:2]),\raes(x = rotated_loading_scores$loadings[,1],\ry = rotated_loading_scores$loadings[,2],\rlabel = c(\u0026quot;Hedonic\u0026quot;,\u0026quot;For Meat\u0026quot;,\u0026quot;For dessert\u0026quot;,\u0026quot;Price\u0026quot;,\u0026quot;Sugar\u0026quot;,\u0026quot;Alcohol\u0026quot;,\u0026quot;Acidity\u0026quot;)))+\rgeom_point()+geom_text_repel()+geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+\rxlab(\u0026quot;Loading score along PC1 after rotation\u0026quot;)+\rylab(\u0026quot;Loading score along PC2 after rotation\u0026quot;)\r\r\rExample 3\rFrench food example (Covariance PCA example)\r# Table 11 class = rep(c(\u0026quot;Blue_collar\u0026quot;, \u0026quot;White_collar\u0026quot;, \u0026quot;Upper_class\u0026quot;), times = 4)\rchildren = rep(c(2,3,4,5), each = 3)\rbread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)\rvegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)\rfruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)\rmeat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)\rpoultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)\rmilk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)\rwine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)\rfood = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)\rfood\r class children bread vegetables fruit meat poultry milk wine\r1 Blue_collar 2 332 428 354 1437 526 247 427\r2 White_collar 2 293 559 388 1527 567 239 258\r3 Upper_class 2 372 767 562 1948 927 235 433\r4 Blue_collar 3 406 563 341 1507 544 324 407\r5 White_collar 3 386 608 396 1501 558 319 363\r6 Upper_class 3 438 843 689 2345 1148 243 341\r7 Blue_collar 4 534 660 367 1620 638 414 407\r8 White_collar 4 460 699 484 1856 762 400 416\r9 Upper_class 4 385 789 621 2366 1149 304 282\r10 Blue_collar 5 655 776 423 1848 759 495 486\r11 White_collar 5 584 995 548 2056 893 518 319\r12 Upper_class 5 515 1097 887 2630 1167 561 284\rpca_food_cov = prcomp(food[,3:9],scale = F)\r\rFactor scores\r# Table 12\r(factor_scores_food = round(pca_food_cov$x[,1:2],2))\r PC1 PC2\r[1,] -635.05 120.89\r[2,] -488.56 142.33\r[3,] 112.03 139.75\r[4,] -520.01 -12.05\r[5,] -485.94 -1.17\r[6,] 588.17 188.44\r[7,] -333.95 -144.54\r[8,] -57.51 -42.86\r[9,] 571.32 206.76\r[10,] -39.38 -264.47\r[11,] 296.04 -235.92\r[12,] 992.83 -97.15\r\rContribution of each observation to principal component\rround(pca_food_cov$x[,1:2]^2/matrix(rep(colSums(pca_food_cov$x[,1:2]^2),nrow(food)),ncol = 2,byrow = T)*100,2)\r PC1 PC2\r[1,] 13.34 5.03\r[2,] 7.90 6.97\r[3,] 0.42 6.72\r[4,] 8.94 0.05\r[5,] 7.81 0.00\r[6,] 11.44 12.22\r[7,] 3.69 7.19\r[8,] 0.11 0.63\r[9,] 10.80 14.71\r[10,] 0.05 24.07\r[11,] 2.90 19.15\r[12,] 32.61 3.25\rdist = pca_food_cov$x[,1]^2+pca_food_cov$x[,2]^2\r\rSquared cosine of observations\r(sq_cos = round(pca_food_cov$x[,1:2]^2/rowSums(pca_food_cov$x^2)*100))\r PC1 PC2\r[1,] 95 3\r[2,] 86 7\r[3,] 26 40\r[4,] 100 0\r[5,] 98 0\r[6,] 89 9\r[7,] 83 15\r[8,] 40 22\r[9,] 86 11\r[10,] 2 79\r[11,] 57 36\r[12,] 97 1\r\rSquared loading scores\r# Table 13\r(round(pca_food_cov$rotation[,1:2]^2,2))\r PC1 PC2\rbread 0.01 0.33\rvegetables 0.11 0.17\rfruit 0.09 0.01\rmeat 0.57 0.01\rpoultry 0.22 0.06\rmilk 0.01 0.40\rwine 0.00 0.02\rNote that this table doesn’t match with that of the paper. We will stick to our analysis.\n\rCorrelation score\r(corr_score_food = round((cor(pca_food_cov$x,food[,3:9])[1:2,]),2))\r bread vegetables fruit meat poultry milk wine\rPC1 0.36 0.91 0.96 1.00 0.98 0.41 -0.43\rPC2 -0.87 -0.35 0.10 0.04 0.16 -0.88 -0.33\r\rSquared correlation score\r(round((cor(pca_food_cov$x,food[,3:9])[1:2,])^2,2))\r bread vegetables fruit meat poultry milk wine\rPC1 0.13 0.83 0.92 1 0.96 0.17 0.18\rPC2 0.76 0.12 0.01 0 0.03 0.77 0.11\r\rCorrelation circle for food data\r# Figure 9\rcorr_score_food = t(corr_score_food)\rcircle + geom_point(data = as.data.frame(corr_score_food), aes(x = corr_score_food[,1],y = corr_score_food[,2]))+\rgeom_text_repel(data = as.data.frame(corr_score_food), aes(x = corr_score_food[,1],y = corr_score_food[,2],\rlabel = c(\u0026quot;Bread\u0026quot;,\u0026quot;Vegetables\u0026quot;,\u0026quot;Fruit\u0026quot;,\u0026quot;Meat\u0026quot;,\u0026quot;Poultry\u0026quot;,\u0026quot;Milk\u0026quot;,\u0026quot;Wine\u0026quot;)))\rNow observe that our correlation circle plot is almost close to that of the papers (though in opposite quadrants. But this is not a problem as we have previously mentioned).\n\rEigenvalues\rEigenvalues of data covariance matrix is square of singular values of centered data matrix. Hence eigenvalues of data covariance matrix can be obtained as below.\n## Table 14\rcent_food = food[,3:9]-matrix(rep(colMeans(food[,3:9]),times = 12),nrow = 12,\rbyrow = T)\rsvd_food = svd(cent_food)\r(Eigenvalues = (svd_food$d)^2)\r[1] 3023141.2354 290575.8390 68795.2333 25298.9496 22992.2474\r[6] 3722.3214 723.9238\rImportant Note: These eigenvalues are not the same as variance of factor scores in principal components. Variance of principal component factor scores can be obtained by dividing the eigenvalues by \\((n-1)\\), where \\(n\\) is number of data points (in this case \\(n = 12\\)). If this point is still not clear, refer to Part-II.\n\rPercentage contribution of each principal component\r(round(Eigenvalues/sum(Eigenvalues),2))\r[1] 0.88 0.08 0.02 0.01 0.01 0.00 0.00\r\rCumulative sum of eigenvalues\r(round(cumsum(Eigenvalues),2))\r[1] 3023141 3313717 3382512 3407811 3430804 3434526 3435250\r\rCumulative percentage contribution\r(round(cumsum(Eigenvalues)/sum(Eigenvalues),2))\r[1] 0.88 0.96 0.98 0.99 1.00 1.00 1.00\r\rRESS (Refer to the paper for a description)\rRESS = array(rep(0,7))\rfor (i in 1:7){\rRESS[i] = sum(Eigenvalues)-sum(Eigenvalues[1:i])\r}\rRESS\r[1] 412108.5146 121532.6756 52737.4423 27438.4927 4446.2453 723.9238\r[7] 0.0000\r\rRatio of RESS and sum of eigenvalues\rround(RESS/sum(Eigenvalues),2)\r[1] 0.12 0.04 0.02 0.01 0.00 0.00 0.00\rWe will not calculate the value of PRESS in this post as it requires us to consider random models. We will not pursue that here.\nsessionInfo()\rR version 4.0.2 (2020-06-22)\rPlatform: x86_64-w64-mingw32/x64 (64-bit)\rRunning under: Windows 7 x64 (build 7601) Service Pack 1\rMatrix products: default\rlocale:\r[1] LC_COLLATE=English_United Kingdom.1252 [2] LC_CTYPE=English_United Kingdom.1252 [3] LC_MONETARY=English_United Kingdom.1252\r[4] LC_NUMERIC=C [5] LC_TIME=English_United Kingdom.1252 attached base packages:\r[1] stats graphics grDevices utils datasets methods base other attached packages:\r[1] ggrepel_0.8.2 ggplot2_3.3.0\rloaded via a namespace (and not attached):\r[1] Rcpp_1.0.4.6 knitr_1.28 magrittr_1.5 tidyselect_1.0.0\r[5] munsell_0.5.0 colorspace_1.4-1 R6_2.4.1 rlang_0.4.5 [9] dplyr_0.8.5 stringr_1.4.0 tools_4.0.2 grid_4.0.2 [13] gtable_0.3.0 xfun_0.13 withr_2.2.0 htmltools_0.4.0 [17] ellipsis_0.3.0 assertthat_0.2.1 yaml_2.2.1 digest_0.6.25 [21] tibble_3.0.1 lifecycle_0.2.0 crayon_1.3.4 bookdown_0.19 [25] farver_2.0.3 purrr_0.3.4 vctrs_0.2.4 glue_1.4.0 [29] evaluate_0.14 rmarkdown_2.1 blogdown_0.18 labeling_0.3 [33] stringi_1.4.6 compiler_4.0.2 pillar_1.4.3 scales_1.1.0 [37] pkgconfig_2.0.3 \rThough unusually long, I hope, this post will be of help to (courageous) readers who work there way through it till end. Comments regarding any errors or omissions may be sent to the author’s email.\nR Markdown file for this post\n\r\rReference\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\rLast updated: 19th January, 2020\n\r","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"d862839c3611a99e119cc5e51a673237","permalink":"/post/principal-component-analysis-part-iii/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-iii/","section":"post","summary":"Run Python codes in Google Colab\r\rDownload Python codes\r\rDownload R codes (R Markdown)\r\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).","tags":["PCA","Machine Learning","R"],"title":"Principal Component Analysis - Part III","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rThis post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-III: Reproducing results of a published paper on PCA\r\rIn this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions. Rather, in our opinion, it enhances understanding by knowing what happens under the hood when a built-in function is called. In actual applications, readers should always use built functions as they are robust(almost always) and tested for efficiency.\nThis post is written in R. Equivalent MATLAB codes for the same can be obtained from this link.\nWe will use French food data form reference [2]. Refer to the paper to know about the original source of the data. We will apply different methods to this data and compare the result. As the dataset is pretty small, one way to load the data into R is to create a dataframe in R using the values in the paper. Another way is to first create a csv file and then read the file into R/MATLAB. We have used the later approach.\nLoad Data\r#Create a dataframe of food data\rclass = rep(c(\u0026quot;Blue_collar\u0026quot;, \u0026quot;White_collar\u0026quot;, \u0026quot;Upper_class\u0026quot;), times = 4)\rchildren = rep(c(2,3,4,5), each = 3)\rbread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)\rvegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)\rfruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)\rmeat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)\rpoultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)\rmilk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)\rwine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)\rfood = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)\rfood\r class children bread vegetables fruit meat poultry milk wine\r1 Blue_collar 2 332 428 354 1437 526 247 427\r2 White_collar 2 293 559 388 1527 567 239 258\r3 Upper_class 2 372 767 562 1948 927 235 433\r4 Blue_collar 3 406 563 341 1507 544 324 407\r5 White_collar 3 386 608 396 1501 558 319 363\r6 Upper_class 3 438 843 689 2345 1148 243 341\r7 Blue_collar 4 534 660 367 1620 638 414 407\r8 White_collar 4 460 699 484 1856 762 400 416\r9 Upper_class 4 385 789 621 2366 1149 304 282\r10 Blue_collar 5 655 776 423 1848 759 495 486\r11 White_collar 5 584 995 548 2056 893 518 319\r12 Upper_class 5 515 1097 887 2630 1167 561 284\r# Centerd data matrix\rcent_food = scale(food[,3:9],scale = F)\r# Scaled data matrix\rscale_food = scale(food[,3:9],scale = T)\r\rCovariance PCA\rUsing built-in function\r# Using built-in function\rpca_food_cov = prcomp(food[,3:9],scale = F)\r# Loading scores (we have printed only four columns out of seven)\r(round(pca_food_cov$rotation[,1:4],2))\r PC1 PC2 PC3 PC4\rbread 0.07 -0.58 -0.40 0.11\rvegetables 0.33 -0.41 0.29 0.61\rfruit 0.30 0.10 0.34 -0.40\rmeat 0.75 0.11 -0.07 -0.29\rpoultry 0.47 0.24 -0.38 0.33\rmilk 0.09 -0.63 0.23 -0.41\rwine -0.06 -0.14 -0.66 -0.31\r# Factor score (we have printed only four PCs out of seven)\rWe have printed only four columns of loading scores out of seven.\n(round(pca_food_cov$x[,1:4],2))\r PC1 PC2 PC3 PC4\r[1,] -635.05 120.89 -21.14 -68.97\r[2,] -488.56 142.33 132.37 34.91\r[3,] 112.03 139.75 -61.86 44.19\r[4,] -520.01 -12.05 2.85 -13.70\r[5,] -485.94 -1.17 65.75 11.51\r[6,] 588.17 188.44 -71.85 28.56\r[7,] -333.95 -144.54 -34.94 10.07\r[8,] -57.51 -42.86 -26.26 -46.55\r[9,] 571.32 206.76 -38.45 3.69\r[10,] -39.38 -264.47 -126.43 -12.74\r[11,] 296.04 -235.92 58.84 87.43\r[12,] 992.83 -97.15 121.13 -78.39\rWe have printed only four principal components out of seven.\n# Variances using built-in function\r(round(pca_food_cov$sdev^2,2))\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\r# Total variance\r(sum(round(pca_food_cov$sdev^2,2)))\r[1] 312295.4\r\r\rComparison of variance before and after transformation\r# Total variance before transformation\rsum(diag(cov(food[,3:9])))\r[1] 312295.4\r# Total variance after transformation\rsum(diag(cov(pca_food_cov$x)))\r[1] 312295.4\rAnother important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.\n# Variance along variables before transformation\rround(diag(cov(food[,3:9])),2)\r bread vegetables fruit meat poultry milk wine 11480.61 35789.09 27255.45 156618.39 62280.52 13718.75 5152.63 \rNote that calculation of variance is unaffected by centering data matrix. So variance of original data matrix as well as centered data matrix is same. Check it for yourself. Now see how PCA transforms these variance.\n# Variance along principal compoennts\rround(diag(cov(pca_food_cov$x)),2)\r PC1 PC2 PC3 PC4 PC5 PC6 PC7 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81 \r# We can obtain the same result using built-in fucntion\rround(pca_food_cov$sdev^2,2)\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rPerforming covariance PCA manually using SVD\rsvd_food_cov = svd(cent_food)\r# Loading scores\rround(svd_food_cov$v[,1:4],2) # We have printed only four columns\r [,1] [,2] [,3] [,4]\r[1,] 0.07 -0.58 -0.40 0.11\r[2,] 0.33 -0.41 0.29 0.61\r[3,] 0.30 0.10 0.34 -0.40\r[4,] 0.75 0.11 -0.07 -0.29\r[5,] 0.47 0.24 -0.38 0.33\r[6,] 0.09 -0.63 0.23 -0.41\r[7,] -0.06 -0.14 -0.66 -0.31\r# Factor scores\rround((cent_food %*% svd_food_cov$v)[,1:4],2) # only 4 columns printed\r [,1] [,2] [,3] [,4]\r[1,] -635.05 120.89 -21.14 -68.97\r[2,] -488.56 142.33 132.37 34.91\r[3,] 112.03 139.75 -61.86 44.19\r[4,] -520.01 -12.05 2.85 -13.70\r[5,] -485.94 -1.17 65.75 11.51\r[6,] 588.17 188.44 -71.85 28.56\r[7,] -333.95 -144.54 -34.94 10.07\r[8,] -57.51 -42.86 -26.26 -46.55\r[9,] 571.32 206.76 -38.45 3.69\r[10,] -39.38 -264.47 -126.43 -12.74\r[11,] 296.04 -235.92 58.84 87.43\r[12,] 992.83 -97.15 121.13 -78.39\r# Variance of principal components\rround(svd_food_cov$d^2/11,2)\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rOur data matrix contains 12 data points. So to find variance of principal components we have to divide the square of the diagonal matrix by 11. To know the theory behind it, refer Part-I\n\rPerforming covariance PCA using Eigen-decomoposition(Not recommended)\rThis procedure is not recommended because forming a covariance matrix is computationally not efficient for large matrices if data matrix contains smaller entries. So doing eigen analysis on covariance matrix may give erroneous results. However, for our example we can use it to obtain results.\neigen_food_cov = eigen(cov(cent_food))\r# Loading scores\rround(eigen_food_cov$vectors[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] -0.07 0.58 -0.40 -0.11\r[2,] -0.33 0.41 0.29 -0.61\r[3,] -0.30 -0.10 0.34 0.40\r[4,] -0.75 -0.11 -0.07 0.29\r[5,] -0.47 -0.24 -0.38 -0.33\r[6,] -0.09 0.63 0.23 0.41\r[7,] 0.06 0.14 -0.66 0.31\r# Factor scores\rround((cent_food %*% eigen_food_cov$vectors)[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] 635.05 -120.89 -21.14 68.97\r[2,] 488.56 -142.33 132.37 -34.91\r[3,] -112.03 -139.75 -61.86 -44.19\r[4,] 520.01 12.05 2.85 13.70\r[5,] 485.94 1.17 65.75 -11.51\r[6,] -588.17 -188.44 -71.85 -28.56\r[7,] 333.95 144.54 -34.94 -10.07\r[8,] 57.51 42.86 -26.26 46.55\r[9,] -571.32 -206.76 -38.45 -3.69\r[10,] 39.38 264.47 -126.43 12.74\r[11,] -296.04 235.92 58.84 -87.43\r[12,] -992.83 97.15 121.13 78.39\r# Variance along principal components\rround(eigen_food_cov$values,2)\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rInstead of using the ‘cov()’ command to find the covariance matrix manually and perform its eigen analysis.\ncov_matrix_manual_food = (1/11)*t(cent_food) %*% cent_food\reigen_food_new = eigen(cov_matrix_manual_food)\r# Loading scores\rround(eigen_food_new$vectors[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] -0.07 0.58 -0.40 0.11\r[2,] -0.33 0.41 0.29 0.61\r[3,] -0.30 -0.10 0.34 -0.40\r[4,] -0.75 -0.11 -0.07 -0.29\r[5,] -0.47 -0.24 -0.38 0.33\r[6,] -0.09 0.63 0.23 -0.41\r[7,] 0.06 0.14 -0.66 -0.31\r# Variance along principal components\rround(eigen_food_new$values,2)\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\rThere are also different ways to find total variance of the data matrix. We will explore some of the options.\n# Total varaiance before transformation\rsum(diag(cov(cent_food)))\r[1] 312295.4\rNote that total variance is invariant to translations. So calculating the total variance on raw data will also give the same answer. Check it to convince yourself.\n\r\rCorrelation PCA\rWhen PCA is performed on a scaled data matrix (each variable is centered as well as variance of each variable is one), it is called correlation PCA. Before discussing correlation PCA we will take some time to see different ways in which we can obtain correlation matrix.\nDifferent ways to obtain correlation matrix.\r# Using built-in command\rround(cor(food[,3:9]),2)[,1:4] # We have printed only four columns\r bread vegetables fruit meat\rbread 1.00 0.59 0.20 0.32\rvegetables 0.59 1.00 0.86 0.88\rfruit 0.20 0.86 1.00 0.96\rmeat 0.32 0.88 0.96 1.00\rpoultry 0.25 0.83 0.93 0.98\rmilk 0.86 0.66 0.33 0.37\rwine 0.30 -0.36 -0.49 -0.44\r# manually\rround((1/11)*t(scale_food) %*% scale_food,2)[,1:4]\r bread vegetables fruit meat\rbread 1.00 0.59 0.20 0.32\rvegetables 0.59 1.00 0.86 0.88\rfruit 0.20 0.86 1.00 0.96\rmeat 0.32 0.88 0.96 1.00\rpoultry 0.25 0.83 0.93 0.98\rmilk 0.86 0.66 0.33 0.37\rwine 0.30 -0.36 -0.49 -0.44\r\r\rPerforming correlation PCA using built-in function\rpca_food_cor = prcomp(food[,3:9],scale = T)\r# Loading scores\rround(pca_food_cor$rotation[,1:4],2) # Printed only four\r PC1 PC2 PC3 PC4\rbread 0.24 -0.62 0.01 -0.54\rvegetables 0.47 -0.10 0.06 -0.02\rfruit 0.45 0.21 -0.15 0.55\rmeat 0.46 0.14 -0.21 -0.05\rpoultry 0.44 0.20 -0.36 -0.32\rmilk 0.28 -0.52 0.44 0.45\rwine -0.21 -0.48 -0.78 0.31\r# Factor scores\rround(pca_food_cor$x[,1:4],2)\r PC1 PC2 PC3 PC4\r[1,] -2.86 0.36 -0.40 0.36\r[2,] -1.89 1.79 1.31 -0.16\r[3,] -0.12 0.73 -1.42 0.20\r[4,] -2.04 -0.32 0.11 0.10\r[5,] -1.69 0.16 0.51 0.16\r[6,] 1.69 1.35 -0.99 -0.43\r[7,] -0.93 -1.37 0.28 -0.26\r[8,] -0.25 -0.63 -0.27 0.29\r[9,] 1.60 1.74 -0.10 -0.40\r[10,] 0.22 -2.78 -0.57 -0.25\r[11,] 1.95 -1.13 0.99 -0.32\r[12,] 4.32 0.10 0.57 0.72\r# Variances along principal componentes\rround(pca_food_cor$sdev^2,2)\r[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\r# Sum of vairances\rsum(pca_food_cor$sdev^2)\r[1] 7\r\rComparison of variance before and after transformation\r# Total variance before transformation\rsum(diag(cov(scale_food)))\r[1] 7\r# Total variance after transformation\rsum(diag(cov(pca_food_cor$x)))\r[1] 7\rAnother important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.\n# Variance along variables before transformation\rround(diag(cov(scale_food)),2)\r bread vegetables fruit meat poultry milk wine 1 1 1 1 1 1 1 \rThis is obvious as we have scaled the matrix. Now see how PCA transforms these variance.\n# Variance along principal compoennts\rround(diag(cov(pca_food_cor$x)),2)\r PC1 PC2 PC3 PC4 PC5 PC6 PC7 4.33 1.83 0.63 0.13 0.06 0.02 0.00 \r# We can obtain the same result using built-in fucntion\rround(pca_food_cor$sdev^2,2)\r[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\rPerforming correlation PCA manually using SVD\rsvd_food_cor = svd(scale_food)\r# Loading scores\rround(svd_food_cor$v[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] 0.24 -0.62 0.01 -0.54\r[2,] 0.47 -0.10 0.06 -0.02\r[3,] 0.45 0.21 -0.15 0.55\r[4,] 0.46 0.14 -0.21 -0.05\r[5,] 0.44 0.20 -0.36 -0.32\r[6,] 0.28 -0.52 0.44 0.45\r[7,] -0.21 -0.48 -0.78 0.31\r# Factor scores\rround((scale_food %*% svd_food_cor$v)[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] -2.86 0.36 -0.40 0.36\r[2,] -1.89 1.79 1.31 -0.16\r[3,] -0.12 0.73 -1.42 0.20\r[4,] -2.04 -0.32 0.11 0.10\r[5,] -1.69 0.16 0.51 0.16\r[6,] 1.69 1.35 -0.99 -0.43\r[7,] -0.93 -1.37 0.28 -0.26\r[8,] -0.25 -0.63 -0.27 0.29\r[9,] 1.60 1.74 -0.10 -0.40\r[10,] 0.22 -2.78 -0.57 -0.25\r[11,] 1.95 -1.13 0.99 -0.32\r[12,] 4.32 0.10 0.57 0.72\r# Variance along each principcal component\rround(svd_food_cor$d^2/11,2)\r[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\r# Sum of variances\rsum(svd_food_cor$d^2/11)\r[1] 7\rAgain we have to divide by 11 to get eigenvalues of correlation matrix. Check the formulation of correlation matrix using scaled data matrix to convince yourself.\n\rUsing eigen-decomposition (Not Recommended)\reigen_food_cor = eigen(cor(food[,3:9]))\r# Loading scores\rround(eigen_food_cor$vectors)\r [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r[1,] 0 1 0 -1 0 1 0\r[2,] 0 0 0 0 1 0 0\r[3,] 0 0 0 1 0 1 0\r[4,] 0 0 0 0 0 0 -1\r[5,] 0 0 0 0 0 0 1\r[6,] 0 1 0 0 0 0 0\r[7,] 0 0 -1 0 0 0 0\r# Factor scores\rround((scale_food %*% eigen_food_cor$vectors)[,1:4],2)\r [,1] [,2] [,3] [,4]\r[1,] 2.86 -0.36 -0.40 0.36\r[2,] 1.89 -1.79 1.31 -0.16\r[3,] 0.12 -0.73 -1.42 0.20\r[4,] 2.04 0.32 0.11 0.10\r[5,] 1.69 -0.16 0.51 0.16\r[6,] -1.69 -1.35 -0.99 -0.43\r[7,] 0.93 1.37 0.28 -0.26\r[8,] 0.25 0.63 -0.27 0.29\r[9,] -1.60 -1.74 -0.10 -0.40\r[10,] -0.22 2.78 -0.57 -0.25\r[11,] -1.95 1.13 0.99 -0.32\r[12,] -4.32 -0.10 0.57 0.72\r# Variances along each principal component\rround(eigen_food_cor$values,2)\r[1] 4.33 1.83 0.63 0.13 0.06 0.02 0.00\rI hope this post would help clear some of the confusions that a beginner might have while encountering PCA for the first time. Please send me a note if you find any errors.\nR Markdown file for this post\n\r\rReferences\rI.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\r\r","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549238400,"objectID":"b37d1d5f68fa7976b26279b7dfbcecbb","permalink":"/post/principal-component-analysis-part-ii/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-ii/","section":"post","summary":"This post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-III: Reproducing results of a published paper on PCA\r\rIn this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions.","tags":["PCA","Machine Learning","R","MATLAB"],"title":"Principal Component Analysis - Part II","type":"post"},{"authors":null,"categories":["Blog"],"content":"\rIn this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function. This will give us an idea as to what happens under the hood when a built-in function is called in any software environment. Simultaneously, we will also show how to use built-in commands to obtain results. Finally, we will reproduce the results of a popular paper on PCA. Including all this in a single post will make it very very long. Therefore, the post has been divided into three parts. Readers totally familiar with PCA should read none and leave this page immediately to save their precious time. Other readers, who have a passing knowledge of PCA and want to see different implementations, should pick and choose material from different parts as per their need. Absolute beginners should start with Part-I and work their way through gradually. Beginners are also encouraged to explore the references at the end of this post for further information. Here is the outline of different parts:\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\rPart-III: Reproducing results of a published paper on PCA\r\rFor Part-II, both MATLAB and R codes are available to reproduce all the results. Part-III contains both R and Python codes to reproduce results of the paper. In this post, we will discuss only the theory behind PCA.\nPrincipal Component Analysis\rTheory:\rGiven a data matrix, we apply PCA to transform it in a way such that the transformed data reveals maximum information. So we have to first get the data on which we want to perform PCA. The usual convention in storing data is to place variables as columns and different observations as rows (Data frames in R follow this convention by default). For example, let’s suppose we are collecting data about daily weather for a year. Our variables of interest may include maximum temperature in a day, minimum temperature, humidity, max. wind speed, etc. For every day, we have to collect observations for each of these variables. In vector form, our data point for one day will contain number of observations equal to the number of variables under study and this becomes one row of our data matrix. Assuming that we are observing 10 variables everyday, our data matrix for one year (assuming it’s not a leap year) will contain 365 rows and 10 columns. Once data matrix is obtained, further analysis is done on this data matrix to obtain important hidden information regarding the data. We will use notations from matrix theory to simplify our analysis.\nLet \\(\\textbf{X}\\) be the data matrix of size \\(n\\times p\\), where \\(n\\) is the number of data points and \\(p\\) is the number of variables. We can assume without any loss of generality that data is centered, meaning its column means are zero. This only shifts the data towards the origin without changing their relative orientation. So if originally not centered, it is first centered before doing PCA. From now onward we will assume that data is always centered.\nVariance of a variable (a column)in \\(\\textbf{X}\\) is equal to sum of squares of entries (because the column is centered) of that column divided by (n - 1) (to make it unbiased). So sum of variance of all variables is \\(\\frac{1}{n - 1}\\) times sum of squares of all elements of the matrix . Readers who are familiar with matrix norms would instantly recognize that total variance is \\(\\frac{1}{n - 1}\\) times the square of Frobenius norm of \\(\\textbf{X}\\). Frobenius norm is nothing but square root of sum of squares of all elements of a matrix.\r\\[ \\|\\textbf{X}\\|_{F} = (\\sum_{i,j}{x_{ij}^2})^{\\frac{1}{2}}=trace(\\textbf{X}^T\\textbf{X})=trace(\\textbf{X}\\textbf{X}^T)\\]\nUsing this definition, total variance before transformation =\r\\[\\frac{1}{n-1}\\sum_{i,j}{x_{ij}^2}=trace(\\frac{1}{n-1}\\textbf{X}^T\\textbf{X})=\\frac{1}{n-1}\\|\\textbf{X}\\|_{F}^2\\]\rWhere trace of a matrix is sum of its diagonal entries and \\(\\|\\textbf{X}\\|_{F}^2\\) is the square of Frobenius norm.\nThe aim of PCA is to transform the data in such a way that along first principal direction, variance of transformed data is maximum. It subsequently finds second principal direction orthogonal to the first one in such a way that it explains maximum of the remaining variance among all possible direction in the orthogonal subspace.\nIn matrix form the transformation can be written as\r\\[\\textbf{Y}_{n\\times p}=\\textbf{X}_{n\\times p}\\textbf{P}_{p\\times p}\\]\rWhere \\(\\textbf{Y}\\) is the transformed data matrix. The columns of \\(\\textbf{Y}\\) are called principal components and \\(\\textbf{P}\\) is usually called loading matrix. Our aim is to find matrix \\(\\textbf{P}\\). Once we find \\(\\textbf{P}\\) we can then find \\(\\textbf{Y}\\) just by a matrix multiplication. Though we will not go into to proof here, it can be easily proved (see references), that matrix \\(\\textbf{P}\\) is the eigenvector matrix of the covariance matrix. Let’s first define covariance matrix.\nGiven a data matrix \\(\\textbf{X}\\)(centered), its covariance matrix \\((\\textbf{S})\\) is defined as\r\\[\\textbf{S} = \\frac{1}{n-1}\\textbf{X}^T\\textbf{X}\\]\rAs principal directions are orthogonal, we will also require \\(\\textbf{P}\\) to be an orthogonal matrix.\nNow, it is straightforward to form the covariance matrix and by placing its eigenvectors as columns, we can find matrix \\(\\textbf{P}\\) and consequently the principal components. The eigenvectors are arranged in such a way that first column is the eigenvector corresponding to largest eigenvector, second column (second eigenvector) corresponds to second largest eigenvalue and so on. Here we have assumed that we will always be able to find all the \\(p\\) orthogonal eigenvectors. In fact, we will always be able to find \\(p\\) orthogonal eigenvectors as the matrix is symmetric. It can also be shown that the transformed matrix \\(\\textbf{Y}\\) is centered and more remarkably, total variance of columns of \\(\\textbf{Y}\\) is same as total variance of columns of \\(\\textbf{X}\\). We will prove these two propositions as the proof are short.\nLet \\(\\textbf{1}\\) be a column vector of all ones of size \\((n\\times 1)\\). To prove that columns of \\(\\textbf{Y}\\) are centered, just premultiply it by \\(\\textbf{1}^T\\) (this finds column sum for each column). So\r\\[\\textbf{1}^T \\textbf{Y} = \\textbf{1}^T\\textbf{X}\\textbf{P}\\]\rBut columns of \\(\\textbf{X}\\) are already centered, so \\(\\textbf{1}^T\\textbf{X}=\\textbf{0}\\). Thus \\(\\textbf{1}^T \\textbf{Y}= \\textbf{0}\\). Hence columns of \\(\\textbf{Y}\\) are centered.\nTo prove that total variance of \\(\\textbf{Y}\\) also remains same, observe that\ntotal covariance of \\(\\textbf{Y}\\) =\r\\[trace(\\frac{1}{n-1}\\textbf{Y}^{T}\\textbf{Y})=\\frac{1}{n-1}trace((\\textbf{P}^T\\textbf{X}^{T}\\textbf{X})\\textbf{P})=\\\\\\frac{1}{n-1}trace((\\textbf{P}\\textbf{P}^T)\\textbf{X}^{T}\\textbf{X})=trace(\\frac{1}{n-1}\\textbf{X}^T\\textbf{X})\\]\rThe previous equation uses the fact that trace is commutative(i.e.\\(trace(\\textbf{AB})=trace(\\textbf{BA})\\)) and \\(\\textbf{P}\\) is orthogonal (i.e. \\(\\textbf{P}\\textbf{P}^T=\\textbf{I}\\)).\nLink between total variance and eigenvalues\rTotal variance is sum of eigenvalues of covariance matrix \\((\\textbf{S})\\). We will further discuss this point in Part-II.\n\rVariations in PCA\rSometimes our data matrix contains variables that are measured in different units. So we might have to scale the centered matrix to reduce the effect of variables with large variation. So depending on the matrix on which PCA is performed, it is divided into two types.\n\rCovariance PCA (Data matrix is centered but not scaled)\rCorrelation PCA (Data matrix is centered and scaled)\r\rExamples of these two types can be found in Part-II.\n\rSome common terminology associated with PCA\r\rFactor scores corresponding to a principal component: Values of that column of \\(\\textbf{Y}\\) that corresponds to the desired principal component.\n\rLoading score: Values corresponding to a column of \\(\\textbf{P}\\). For example,loading scores of variables corresponding to first principal component are the values of the first column of \\(\\textbf{P}\\).\n\rInertia: Square of Frobenius norm of the matrix.\n\r\r\rHow actually are principal components computed\rThe previously stated method of finding eigenvectors of covariance matrix is not computationally efficient. In practice, singular value decomposition (SVD) is used to find the matrix \\(\\textbf{P}\\). SVD theorem tells that any real matrix \\(\\textbf{X}\\) can be decomposed into three matrices such that\r\\[ \\textbf{X} = \\textbf{U}\\Sigma\\textbf{V}^T\\]\rWhere, \\(\\textbf{X}\\) is of size \\(n\\times p\\). \\(\\textbf{U}\\) and \\(\\textbf{V}\\) are orthogonal matrices of size \\(n\\times n\\) and \\(p\\times p\\) respectively. \\(\\Sigma\\) is a diagonal matrix of size \\(n\\times p\\).\nGiven the SVD decomposition of a matrix \\(\\textbf{X}\\),\r\\[\\textbf{X}^T\\textbf{X}=\\textbf{V}\\Sigma^2\\textbf{V}^T\\]\rThis is the eigen-decomposition of \\(\\textbf{X}^T\\textbf{X}\\). So \\(\\textbf{V}\\) is the eigenvector matrix of \\(\\textbf{X}^T\\textbf{X}\\). For PCA we need eigenvector matrix of covariance matrix. So converting the equation into convenient form, we get\r\\[\\textbf{S} = \\frac{1}{n-1}\\textbf{X}^T\\textbf{X}=\\textbf{V}(\\frac{1}{n-1}\\Sigma^2)\\textbf{V}^T\\]\rThus eigenvalues of S are diagonal entries of \\((\\frac{1}{n-1}\\Sigma^2)\\). As SVD is computationally efficient, all built-in functions use SVD to find the loading matrix and then use it to find principal components.\nIn the interest of keeping the post at a reasonable length, we will stop our exposition of theory here. Whatever we have discussed is only a fraction of everything. Entire books have been written on PCA. Interested readers who want to pursue further can refer to the references given here and later to the references given in the references. Please send me a note if you find any errors.\nR Markdown file for this post\n\r\rReferences\rI.T. Jolliffe, Principal component analysis, 2nd ed, Springer, New York,2002.\rAbdi, H., \u0026amp; Williams, L. J. (2010). Principal component analysis. Wiley interdisciplinary reviews: computational statistics, 2(4), 433-459.\r\r\r\r","date":1549152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549152000,"objectID":"ab28fc880c4077431666d7f4775dd934","permalink":"/post/principal-component-analysis-part-i/","publishdate":"2019-02-03T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-i/","section":"post","summary":"In this post, we will discuss about Principal Component Analysis (PCA), one of the most popular dimensionality reduction techniques used in machine learning. The applications of PCA and its variants are ubiquitous. Thus, a through understanding of PCA is considered essential to start one’s journey into machine learning. In this and subsequent posts, we will first discuss relevant theory of PCA. Then we will implement PCA from scratch without using any built-in function.","tags":["PCA","Machine Learning","R"],"title":"Principal Component Analysis - Part I","type":"post"},{"authors":null,"categories":null,"content":"  (Visit this Github Page to download all codes and preprocessed data)\nIntroduction to the problem Condition based maintenance (CBM) is the process of doing maintenance only when it is required. Adoption of this maintenance strategy leads to significant monetary gains as it precludes periodic maintenance and reduces unplanned downtime. Another term commonly used for condition based maintenance is predictive maintenance. As the name suggests, in this method we predict in advance when to perform maintenance. Maintenance is required, if fault has already occurred or is imminent. This leads us to the problem of fault diagnosis and prognosis.\nIn fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognosis, our aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:\n Model Based Approaches Data-Driven Approaches  In model based approach, a complete model of the system is formulated and it is then used for fault diagnosis and prognosis. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modelling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.\nIn data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognosis. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognosis tasks.\nAim of the project In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.\nThis is an ongoing project and modifications and additions of new techniques will be done over time. Python and R are popular programming languages that are used for machine learning applications. We will use those for our demonstrations. Tensorflow will be used for deep learning applications. This page contains results on fault diagnosis only. Results on fault prognosis will be summarized in a separate webpage.\nResults using Case Western Reserve University Bearing Data We will first apply traditional feature based methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. In feature based methods, we will extensively use wavelet packet energy features and wavelet packet entropy featues that are calculated from raw time domain data. The procedure detailing calculation of wavelet packet energy features can be found at this link and similar calculations for wavelet packet entropy features can be found at this link.\n SVM on time domain features (10 classes, sampling frequency: 48k) (Overall accuracy: 96.5%) (Python code) (R code)\n SVM on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 99.3%) (Python code) (R code)\n Visualizing High Dimensional Data Using Dimensionality Reduction Techniques (Python Code) (R Code)\n SVM on wavelet packet entropy features (10 classes, sampling frequency: 48k) (Overall accuracy: 99.3%) (Python code) (R code)\n SVM on time and wavelet packet features (12 classes, sampling frequency: 12k) (Achieves 100% test accuracy in one case) (Python code) (R code)\n Multiclass Logistic Regression on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 98.5%) (Python code) (R code)\n Multiclass Logistic Regression on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99.7%) (Python code) (R code)\n LDA on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 89.8%) (Python code) (R code)\n LDA on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99.5%) (Python code) (R code)\n QDA on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 96.5%) (Python code) (R code)\n QDA on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99%) (Python code) (R code)\n kNN on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 89.8%) (Python code) (R code)\n kNN on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99.5%) (Python code) (R code)\n Decision tree on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 94.5%) (Python code) (R code)\n Decision tree on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99.7%) (Python code) (R code)\n Bagging on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 97%) (Python code) (R code)\n Bagging on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 100%) (Python code) (R code)\n Boosting on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 99%) (Python code) (R code)\n Boosting on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 100%) (Python code) (R code)\n Random forest on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 98.1%) (Python code) (R code)\n Random forest on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 100%) (Python code) (R code)\n  Enter Deep Learning  Fault diagnosis using convolutional neural network (CNN) (10 classes, sampling frequency: 48k) (Overall accuracy: 96.2%)\n CNN based fault diagnosis using continuous wavelet transform (CWT) (10 classes, sampling frequency: 48k) (Overall accuracy: 98.2%)\n  (This list will be updated gradually.)\nSome other related stuff  Fault diagnosis of machines (A non-technical introduction)\n A quick introduction to MATLAB\n Transient vibration and shock response spectrum plots in MATLAB\n Simple examples on finding instantaneous frequency using Hilbert transform (MATLAB Code)\n  Readers who use the processed datasets of this page must cite the original data source as\nBibTeX citation @misc{casewesternbearingdata, url = {https://csegroups.case.edu/bearingdatacenter/home}, note = {This data come from Case Western Reserve University Bearing Data Center Website} }  For attribution, readers may cite this project as\nBibTeX citation @misc{sahoo2016datadriven, author = {Sahoo, Biswajit}, title = {Data-Driven Machinery Fault Diagnosis}, url = {https://biswajitsahoo1111.github.io/cbm_codes_open/}, year = {2016} }  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fff49bc595c6f4bb0b2abd821deb85a2","permalink":"/project/personal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/personal-project/","section":"project","summary":"The main aim of this project is to produce reproducible results. We will apply some of the standard machine learning techniques to publicly available machinery data sets and show the results with code. This is an ongoing project and will evolve over time. Related notebooks and data can be found at this [github page](https://biswajitsahoo1111.github.io/cbm_codes_open/).","tags":["Machine Learning","Deep Learning","Condition Monitoring"],"title":"Data-Driven Machinery Fault Diagnosis","type":"project"}]