[{"authors":null,"categories":null,"content":"I am a PhD student in the Department of Mechanical Engineering at IIT Kharagpur, India. Presently, I am working on machinery fault diagnosis and prognosis using deep learning. My PhD supervisor is Prof. A. R. Mohanty. During my PhD, I developed an interest in machine learning and deep learning in particular. Though my training is in mechanical engineering, I have acquired machine learning skills by self-study and from MOOCs through online certifications. Beyond research, I like literature and music.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD student in the Department of Mechanical Engineering at IIT Kharagpur, India. Presently, I am working on machinery fault diagnosis and prognosis using deep learning. My PhD supervisor is Prof.","tags":null,"title":"Biswajit Sahoo","type":"authors"},{"authors":null,"categories":["Blog","Tensorflow","Deep Learning","Attention Mechanism"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will discuss about IndexedSlices class of Tensorflow. We will try to answer the following questions in this blog:\n\rWhat are IndexedSlices?\rWhere do we get it?\rHow to convert from IndexedSlices to tensors?\r\r\nWhat are IndexedSlices?\rAccording to Tensorflow documentation, IndexedSlices are sparse representation of a set of tensor slices at a given index. At an high level it appears to be some kind of sparse representation. Let’s try to understand it with examples.\n\n\rWhere do we get it?\rWe get IndexedSlices while taking gradients of an Embedding layer. Embedding matrices can be huge (depending on vocabulary size). But each batch only contains a small fraction of tokens. So while computing the gradient of loss with respect to embedding layer, in each pass we have to only consider the corresponding token embeedings of the present batch. Naturally a sparse tensor seems to be a better option to record those gradients. Tensorflow does that using IndexedSlices. We will show that below using a contrived example.\nimport tensorflow as tf\rprint(\u0026#34;Tensorflow version: \u0026#34;, tf.__version__)\rTensorflow version: 2.4.0\rmodel = tf.keras.models.Sequential([\r# Vocab size: 10, Embedding dimension: 4, Input_shape size: (batch_size, num_words). As usual, batch_size is omitted.\rtf.keras.layers.Embedding(10, 4, input_shape = (5,)),\rtf.keras.layers.Flatten(), tf.keras.layers.Dense(1)\r])\rmodel.summary()\rModel: \u0026#34;sequential\u0026#34;\r_________________________________________________________________\rLayer (type) Output Shape Param # =================================================================\rembedding (Embedding) (None, 5, 4) 40 _________________________________________________________________\rflatten (Flatten) (None, 20) 0 _________________________________________________________________\rdense (Dense) (None, 1) 21 =================================================================\rTotal params: 61\rTrainable params: 61\rNon-trainable params: 0\r_________________________________________________________________\rdata = tf.random.uniform(shape = (1, 5), minval = 0, maxval = 10, dtype = tf.int32) # Batch size is 1.\rdata\r\u0026lt;tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[6, 1, 1, 4, 8]])\u0026gt;\rmodel.variables # Is a list of 3 tensors. 1 from Embedding layer and 2 from Dense layer (Kernel and bias)\r[\u0026lt;tf.Variable \u0026#39;embedding/embeddings:0\u0026#39; shape=(10, 4) dtype=float32, numpy=\rarray([[ 4.10897247e-02, -2.48962641e-03, 1.26880072e-02,\r3.39310430e-02],\r[ 3.28579657e-02, 3.90318781e-03, 2.81411521e-02,\r3.09719704e-02],\r[ 1.16247907e-02, -1.41257644e-02, -3.36343870e-02,\r-4.41543460e-02],\r[-4.67238426e-02, 2.42819674e-02, -4.26802635e-02,\r-2.59207971e-02],\r[ 2.28367783e-02, -2.09717881e-02, 1.05572566e-02,\r3.33249308e-02],\r[-3.37148309e-02, -4.61939685e-02, -2.61853095e-02,\r-4.10162285e-03],\r[-3.59787717e-02, 2.78765075e-02, -3.16200405e-02,\r4.54976298e-02],\r[-4.67344411e-02, -1.30221620e-02, 1.52915232e-02,\r2.22466923e-02],\r[-1.03901625e-02, 2.40740217e-02, -1.24427900e-02,\r4.47194651e-03],\r[-3.57637033e-02, 4.28059734e-02, -2.59280205e-05,\r4.09286283e-02]], dtype=float32)\u0026gt;,\r\u0026lt;tf.Variable \u0026#39;dense/kernel:0\u0026#39; shape=(20, 1) dtype=float32, numpy=\rarray([[ 0.42870212],\r[ 0.04779923],\r[ 0.4126016 ],\r[-0.13294601],\r[-0.3175783 ],\r[-0.46080017],\r[-0.23412797],\r[ 0.30137837],\r[-0.5197849 ],\r[-0.10935467],\r[ 0.5087845 ],\r[-0.06930307],\r[ 0.10028934],\r[-0.11278141],\r[-0.21269777],\r[-0.0214209 ],\r[ 0.12959635],\r[-0.13330323],\r[-0.23972857],\r[ 0.23718971]], dtype=float32)\u0026gt;,\r\u0026lt;tf.Variable \u0026#39;dense/bias:0\u0026#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)\u0026gt;]\roptimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\rloss_object = tf.keras.losses.MeanSquaredError()\rtarget = tf.constant([2.5], shape = (1,1))\rfor _ in range(2): # Let\u0026#39;s run gradient descent for two batches of the same input data. (It\u0026#39;s a contrived examples)\rwith tf.GradientTape() as tape:\routput = model(data) # Output has shape: (batch_size, 1). Here batch_size is 1. So output shape is (1,1)\rloss_value = loss_object(target, output) # Calculating some random loss.\rgrads = tape.gradient(loss_value, model.trainable_variables)\r# Gradient descent step\roptimizer.apply_gradients(zip(grads, model.trainable_variables))\rlen(grads)\r3\rgrads[0]\r\u0026lt;tensorflow.python.framework.indexed_slices.IndexedSlices at 0x16c04d4a970\u0026gt;\rprint(grads[0])\rIndexedSlices(indices=tf.Tensor([6 1 1 4 8], shape=(5,), dtype=int32), values=tf.Tensor(\r[[-0.9495101 -0.14344962 -0.91739434 0.25398374]\r[ 0.69607455 1.0615798 0.5085497 -0.7338184 ]\r[ 1.1639317 0.24842012 -1.2103697 0.12384857]\r[-0.25895947 0.2856651 0.47968888 0.01028775]\r[-0.28760925 0.28005898 0.56933826 -0.5540699 ]], shape=(5, 4), dtype=float32), dense_shape=tf.Tensor([10 4], shape=(2,), dtype=int32))\rAn IndexedSlices object has 3 main entries.\n\rindices\rvalues, and\rdense_shape\r\r\n\rHow to convert IndexedSlices to Tensors?\rBefore we do the conversion, let’s answer a relevant question: Why do we have to do the …","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"7709c4e15d7c155934faa3699bcfb6cc","permalink":"https://biswajitsahoo1111.github.io/post/indexedslices-in-tensorflow/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/post/indexedslices-in-tensorflow/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will discuss about IndexedSlices class of Tensorflow. We will try to answer the following questions in this blog:","tags":["Tensorflow","Deep Learning","Machine Learning","Attention Mechanism"],"title":"IndexedSlices in Tensorflow","type":"post"},{"authors":[],"categories":["Blog","Tensorflow","Attention Mechanism","Transformer"],"content":"\r\r\r\r\rView GitHub Page\r\r-----\rView source on GitHub\r\rDownload code (.zip)\r\r\r\r\r\r\r\rThis code has been merged with D2L book. See PR: 1756, 1768\nThis post contains Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book. The chapter has 7 sections and code for each section can be found at the following links. We have given only code implementations. For theory, readers should refer the book.\n10.1. Attention Cues\n10.2. Attention Pooling: Nadaraya-Watson Kernel Regression\n10.3. Attention Scoring Functions\n10.4. Bahdanau Attention\n10.5. Multi-Head Attention\n10.6. Self-Attention and Positional Encoding\n10.7. Transformer\nAdditional sections:\r9.7. Sequence to Sequence Learning\n\rAdditional Chapters:\rChapter 17: Generative Adversarial Networks\n\rHow to run these code:\rThe best way (in our opinion) is to either clone the repo (or download the zipped repo) and then run each notebook from the cloned (or extracted) folder. All the notebooks will run without any issue.\nNote: We claim no originality for the code. Credit goes to the authors of this excellent book. However, all errors and omissions are my own and readers are encouraged to bring it to my notice. Finally, no TF code was available (to the best of my knowledge) for Attention Mechanisms chapter when this repo was first made public.\n\r","date":1615334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"7e7421b18e803605c92a3c6df8347ff9","permalink":"https://biswajitsahoo1111.github.io/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/","publishdate":"2021-03-10T00:00:00Z","relpermalink":"/post/tensorflow-2-code-for-attention-mechanisms-chapter-of-d2l-book/","section":"post","summary":"View GitHub Page\r\r-----\rView source on GitHub\r\rDownload code (.zip)\r\r\r\r\r\r\r\rThis code has been merged with D2L book.","tags":["Tensorflow","Machine Learning","Deep Learning","Attention Mechanism","Transformer"],"title":"Tensorflow 2 code for Attention Mechanisms chapter of Dive into Deep Learning (D2L) book","type":"post"},{"authors":[],"categories":["Blog","Tensorflow","Machine Learning","Deep Learning"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple csv files using Tensroflow Sequence. In an earlier post we had demonstrated the procedure for reading multiple csv files using a custom generator. Though generators are convenient for handling chunks of data from a large dataset, they have limited portability and scalability (see the caution here). Therefore, Tensorflow prefers Sequence over generators.\nSequence is similar to Tensorflow Dataset but provides flexibility for batched data preparation in a custom manner. Tensorflow Dataset provides a wide range of functionalities to handle different data types. But for some specific applications we might have to make some custom modifications for which built-in methods are not available in tensorflow dataset. In that case, we can use Sequence to create our own dataset equivalent. In this post, we will show how to use sequence to read multiple csv files. The method we will discuss is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 .csv files. But the method can be easily extended to huge datasets involving thousands of csv files.\nThis post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use Tensorflow 2 to run our deep learning model. Tensorflow is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.\nOutline:\rCreate 500 “.csv” files and save it in the folder “random_data” in current directory.\rWrite a sequence object that reads data from the folder in chunks and preprocesses it.\rFeed the chunks of data to a CNN model and train it for several epochs.\rMake prediction on new data for which labels are not known.\r\r\n\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.\n\rEach .csv file will have one column of data with 1024 entries.\rEach file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …,“Fault_1_100.csv”. Similarly for other classes.\r\rimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026#34;./random_data/\u0026#34;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026#34;./random_data/\u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34;_\u0026#34; + \u0026#34;{0:03}\u0026#34;.format(i+1) + \u0026#34;.csv\u0026#34; # This creates file_name\rnp.savetxt(eval(\u0026#34;file_name\u0026#34;), data, delimiter = \u0026#34;,\u0026#34;, header = \u0026#34;V1\u0026#34;, comments = \u0026#34;\u0026#34;)\rprint(str(eval(\u0026#34;number_of_files_in_each_class\u0026#34;)) + \u0026#34; \u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34; files\u0026#34; + \u0026#34; created.\u0026#34;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026#34;Fault_1\u0026#34;, \u0026#34;Fault_2\u0026#34;, \u0026#34;Fault_3\u0026#34;, \u0026#34;Fault_4\u0026#34;, \u0026#34;Fault_5\u0026#34;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = glob.glob(\u0026#34;./random_data/*\u0026#34;)\rprint(\u0026#34;Total number of files: \u0026#34;, len(files))\rprint(\u0026#34;Showing first 10 files...\u0026#34;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\r[\u0026#39;./random_data\\\\Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data\\\\Fault_1_010.csv\u0026#39;]\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data\\Fault_1_001.csv\rprint(files[0][14:21])\rFault_1\rNow that data have been created, we will go to the next step. That is, create a custom Sequence object, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to …","date":1608940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608940800,"objectID":"416d72acfb5b3843af5ce0a8a16179ba","permalink":"https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/","publishdate":"2020-12-26T00:00:00Z","relpermalink":"/post/reading-multiple-files-in-tensorflow-2-using-sequence/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple csv files using Tensroflow Sequence.","tags":["Tensorflow","Machine Learning","Deep Learning"],"title":"Reading multiple files in Tensorflow 2 using Sequence","type":"post"},{"authors":[],"categories":["Blog","PyTorch","Machine Learning","Deep Learning"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn many engineering applications data are usually stored in CSV (Comma Separated Values) files. In big data applications, it’s not uncommon to obtain thousands of csv files. As the number of files increases, at some point, we can no longer load the whole dataset into computer’s memory. In deep learning applications it is increasingly common to come across datasets that don’t fit in the computer’s memory. In that case, we have to devise a way so as to be able to read chunks of data at a time so that the model can be trained using the whole dataset.\nThere are many ways to achieve this objective. In this post, we will adopt an approach that allows us to read csv files in chunks and preprocess those files in whatever way we like. Then we can pass the processed data to train any deep learning model. Though we will use csv files in this post, the method is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 csv files. But the method can be easily extended to huge datasets involving thousands of csv files.\nThis post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use PyTorch to run our deep learning model. For efficiency in data loading, we will use PyTorch dataloaders.\nOutline:\rCreate 500 “.csv” files and save it in the folder “random_data” in current working directory.\rCreate a custom dataloader.\rFeed the chunks of data to a CNN model and train it for several epochs.\rMake prediction on new data for which labels are not known.\r\r\n\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.\n\rEach .csv file will have one column of data with 1024 entries.\rEach file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1” category will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.\r\rimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026#34;./random_data/\u0026#34;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026#34;./random_data/\u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34;_\u0026#34; + \u0026#34;{0:03}\u0026#34;.format(i+1) + \u0026#34;.csv\u0026#34; # This creates file_name\rnp.savetxt(eval(\u0026#34;file_name\u0026#34;), data, delimiter = \u0026#34;,\u0026#34;, header = \u0026#34;V1\u0026#34;, comments = \u0026#34;\u0026#34;)\rprint(str(eval(\u0026#34;number_of_files_in_each_class\u0026#34;)) + \u0026#34; \u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34; files\u0026#34; + \u0026#34; created.\u0026#34;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026#34;Fault_1\u0026#34;, \u0026#34;Fault_2\u0026#34;, \u0026#34;Fault_3\u0026#34;, \u0026#34;Fault_4\u0026#34;, \u0026#34;Fault_5\u0026#34;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = glob.glob(\u0026#34;./random_data/*\u0026#34;)\rprint(\u0026#34;Total number of files: \u0026#34;, len(files))\rprint(\u0026#34;Showing first 10 files...\u0026#34;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\r[\u0026#39;./random_data/Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_010.csv\u0026#39;]\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data/Fault_1_001.csv\rprint(files[0][14:21])\rFault_1\rNow that data have been created, we will go to the next step. That is, create a custom dataloader, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.\n\n\r2. Write a custom dataloader\rWe have to first create a Dataset class. Then we can pass the dataset to the dataloader. Every dataset class must implement the __len__ method that determines the length of the dataset and __getitem__ method that …","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"cd90469125a5e4dc30c0870034ca2c05","permalink":"https://biswajitsahoo1111.github.io/post/reading-multiple-csv-files-in-pytorch/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/post/reading-multiple-csv-files-in-pytorch/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn many engineering applications data are usually stored in CSV (Comma Separated Values) files.","tags":["PyTorch","Machine Learning","Deep Learning"],"title":"Reading multiple csv files in PyTorch","type":"post"},{"authors":null,"categories":null,"content":"\r\r\rView GitHub Page\r\rView source on GitHub\r\r----\rDownload all code (.zip)\r\r\r\r\r\r\r\rIntroduction Remaining useful life (RUL) prediction is the study of predicting when something is going to fail, given its present state. The problem has a prophetic charm associated with it. While a soothsayer can make a prediction about almost anything (including RUL of a machine) confidently, many people will not accept the prediction because of its lack of scientific basis. Here, we will try to solve the problem with scientific reasoning.\nA component (or a machine) is said to have failed when it can no longer perform its desired task to the satisfaction of the user. For example, Li-Ion battery of an electric vehicle is said to have failed when it requires frequent recharging to travel a small distance. Similarly, a bearing of a machine is said to have failed, if level of vibration produced at the bearing goes above some acceptable limit. Other examples can be thought of for different applications. The goal then is to predict beforehand when something is going to fail. Knowledge of a component’s expected time of failure will help us prepare well for the inevitable. In industrial setting, where any unplanned shutdown of a critical component has huge monetary cost, knowing when a machine is going to fail will result in significant monetary gains.\nThere are many techniques developed over the years to predict RUL of a component. All those techniques can be broadly divided into two categories.\n Model Based Methods Data-Driven Methods  In model based methods, we try to formulate a mathematical model of the system under consideration. Then using that model we try to predict RUL of the component. Though model based methods are used in some cases, there are many other applications where formulating a full mathematical model of the system is extremely difficult. In some cases, the underlying physics is so complex that we have to make many simplifying assumptions. Whether the simplifying assumptions are justified or not is determined by collecting real data from the machine. Therefore, it requires extensive domain knowledge and thus is a territory of only a select few who can actually do these things.\nIn contrast, in data-driven methods all information about a machine is gained from the data collected from it. With readily available sensors we can collect huge amounts of data for almost any application. By analyzing that data we can get an idea about the condition of the machine. That will help us in making an informed decision about the RUL of the machine. In this process we make no assumptions about the machine. Increasingly, data-driven methods are getting better at making reliable predictions. As the name of the project suggests, we will only focus on data-driven methods for RUL prediction. The problem of RUL prediction is also know as prognosis in some fields. Some people also call it prognostics. We will only use the term RUL prediction. In the beginning, we will mainly focus on predicting RUL of mechanical components. Later we will explore other application areas.\nAim of the project Like my previous project on fault diagnosis, aim of this project is to produce reproducible results for RUL prediction. RUL prediction is a broad subject that can be applied to many problems such as RUL prediction of Li-Ion batteries, RUL prediction of machinery bearings, RUL prediction of machine tool, etc. We will start with mechanical applications and then gradually move to other applications over time. As our aim is reproducibility, we will use publicly available datasets. Interested readers can download the data and use our code to get exact results as we have obtained. As we will use well known datasets, readers might observe that, in some cases, our results are in fact worse than some reported results elsewhere. Our goal is not to verify someone else’s claim. If someone else claims a better result, onus is on them to demonstrate their result. Here, whatever results I have claimed can be reproduced by readers by just running the jupyter notebooks after downloading relevant data.\nThis is an ongoing project and modifications and additions of new techniques will be done over time. Python and R are two popular programming languages that are used in machine learning applications. We will use Python to demonstrate our results. At a later stage we might add equivalent R code. To implement deep learning models, we will use Tensorflow.\nResults using NASA’s Turbofan Engine Degradation Dataset  We will first apply classical machine learning methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. Dataset description and preprocessing steps can be found at this link. We will use the same preprocessing steps, with minor changes, in all notebooks. We strongly encourage readers to first go over data preparation notebook before using results notebooks. In the table below, we report Root Mean Square Error (RMSE) …","date":1599264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599264000,"objectID":"d4d2242dfca4871d1bc47ec9da35675a","permalink":"https://biswajitsahoo1111.github.io/project/rul_codes_open/","publishdate":"2020-09-05T00:00:00Z","relpermalink":"/project/rul_codes_open/","section":"project","summary":"Aim of this project is to produce reproducible results in condition monitoring. We will apply some of the standard machine learning techniques to publicly available datasets and show the results with code for remaining useful life (RUL) prediction task. This is an ongoing project and will evolve over time. Related notebooks can be found at this [github page](https://biswajitsahoo1111.github.io/rul_codes_open/).","tags":["Machine Learning","Deep Learning","Condition Monitoring","Remaining Useful Life","RUL Prediction"],"title":"Data-Driven Remaining Useful Life (RUL) Prediction","type":"project"},{"authors":[],"categories":["Blog","Tensorflow","Deep Learning"],"content":"\r\rNote: Whether this method is efficient or not is contestable. Efficiency of a data input pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they use this method in their own problems. For the simple (and small) problem considered in this post, we got no perceivable performance improvement. But for one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope this method can be of help to others as well. The system on which we ran this notebook has 44 CPU cores. Tensorflow version was 2.4.0 and we did not use any GPU. Please note that for some weird reason, the speedup technique doesn’t work in Google Colab. But it works in GPU enabled personal systems, that I have checked.\nUpdate: Along with the method described in this post, readers should also try using Tensorflow Sequence and see if it improves input pipeline efficiecy. Define all the complex transformations inside __getitem__ method of seqeuence class and then suitably choose max_queue_size, workers, and use_multiprocessing in model.fit() to improve pipeline efficiency.\n\rView source on GitHub\r\r\rDownload notebook\r\r\rThis post is a sequel to an older post. In the previous post, we discussed ways in which we can read multiple files in Tensorflow 2. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are spectrogram and normalizing (converting each value to a standard normal value). We have chosen these transformations just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this tensorflow guide.\nAs this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on section 4 which is the main topic of this post.\nOutline:\rCreate 500 \u0026#34;.csv\u0026#34; files and save it in the folder “random_data” in current directory.\rWrite a generator that reads data from the folder in chunks and transforms it.\rBuild data pipeline and train a CNN model.\rHow to make the code run faster?\rHow to make predictions?\r\r\n\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.\r* Each .csv file will have one column of data with 1024 entries.\r* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”\rcategory will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.\nimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026#34;./random_data/\u0026#34;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026#34;./random_data/\u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34;_\u0026#34; + \u0026#34;{0:03}\u0026#34;.format(i+1) + \u0026#34;.csv\u0026#34; # This creates file_name\rnp.savetxt(eval(\u0026#34;file_name\u0026#34;), data, delimiter = \u0026#34;,\u0026#34;, header = \u0026#34;V1\u0026#34;, comments = \u0026#34;\u0026#34;)\rprint(str(eval(\u0026#34;number_of_files_in_each_class\u0026#34;)) + \u0026#34; \u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34; files\u0026#34; + \u0026#34; created.\u0026#34;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026#34;Fault_1\u0026#34;, \u0026#34;Fault_2\u0026#34;, \u0026#34;Fault_3\u0026#34;, \u0026#34;Fault_4\u0026#34;, \u0026#34;Fault_5\u0026#34;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = np.sort(glob.glob(\u0026#34;./random_data/*\u0026#34;))\rprint(\u0026#34;Total number of files: \u0026#34;, len(files))\rprint(\u0026#34;Showing first 10 files...\u0026#34;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\rarray([\u0026#39;./random_data/Fault_1_001.csv\u0026#39;, \u0026#39;./random_data/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_003.csv\u0026#39;, \u0026#39;./random_data/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_005.csv\u0026#39;, \u0026#39;./random_data/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_007.csv\u0026#39;, \u0026#39;./random_data/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_009.csv\u0026#39;, \u0026#39;./random_data/Fault_1_010.csv\u0026#39;],\rdtype=\u0026#39;\u0026lt;U29\u0026#39;)\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data/Fault_1_001.csv\rprint(files[0][14:21]) …","date":1589673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589673600,"objectID":"8443cbf8b2a271a053d98148f72e3833","permalink":"https://biswajitsahoo1111.github.io/post/efficiently-reading-multiple-files-in-tensorflow-2/","publishdate":"2020-05-17T00:00:00Z","relpermalink":"/post/efficiently-reading-multiple-files-in-tensorflow-2/","section":"post","summary":"Note: Whether this method is efficient or not is contestable. Efficiency of a data input pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done?","tags":["Tensorflow","Deep Learning"],"title":"Efficiently reading multiple files in Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog","Linear Algebra","Tensorflow"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will explore ways of doing linear algebra only using tensorflow. We will only import tensorflow and nothing else. As we will see, we can do all the common linear algebra operations without using any other library. This post is very long as it covers almost all the functions that are there in the linear algebra library tf.linalg. But this is not a copy of tensorflow documentation. Rather, the tensorflow documentation is a super set of what has been discussed here. This post also assumes that readers have a working knowledge of linear algebra. Most of the times, we will give examples to illustrate a function without going into the underlying theory. Interested readers should use the contents to browse relevant sections of their interest.\n\rBasics\r\rCreating tensors\rCreating a sequence of numbers\rSlicing\rModifying elements of a matrix\rCreating a complex matrix\rTranspose of a matrix\r\rTranspose of a real matrix\rTranspose of a complex matrix\r\rSome common matrices\r\rIdentity matrix\rDiagonal matrix\rTri-diagonal matrix\rMatrix of all zeros and ones\rRandom matrices\r\rRandom uniform matrix\rRandom normal matrix\rTruncated random normal matrix\rRandom Poisson matrix\rRandom gamma matrix\r\rSome special matrices\r\rSparse matrices\rMatrix multiplication\r\rMultiplying two column vectors\r\rInner product\rOuter product\r\rMultiplying a matrix with a vector\rMultiplying two matrices\rMultiplying two tri-diagonal matrices\r\rSome common operations on matrices\r\rTrace\rDeterminant\rRank\rMatrix inverse\rExtract diagonals of a matrix\rExtract band part of a matrix\r\r\rMatrix factorizations\r\rLU\rCholesky\rQR\rSVD\r\rEigenvalues and eigenvectors\r\rEigen-analysis of Hermitian matrices\rEigen-analysis of non-Hermitian matrices\r\rSolving dense linear systems\r\rUsing LU decomposition\rUsing Cholesky decomposition\r\rSolving structured linear systems\r\rSolving triangular systems\rSolving tri-diagonal systems\rSolving banded triangular systems\r\rSolving least squares problems\r\rOrdinary least squares\rRegularized least squares\r\rSome specialized operations\r\rNorm\rNormalizing a tensor\rGlobal norm\rCross product of vectors\rMatrix square root\rMatrix exponential\rMatrix logarithm\rLog-determinant of a matrix\rPseudo inverse of a matrix\r\rLinear operators\r\rCommon methods on linear operators\rSpecial matrices using operators\r\rToeplitz matrix\rCirculant matrix\rBlock diagonal matrix\rBlock lower triangular matrix\rHouseholder matrix\rKronecker matrix\rPermutation matrix\r\rCommon matrices using operators\r\rIdentity matrix\rScaled identity matrix\rDiagonal matrix\rTri-diagonal matrix\rLower triangular matrix\rMatrix of zeros\r\rMatrix operations using operators\r\rLow-rank update\rOperator inversion\rOperator composition\r\r\rConclusion\r\rimport tensorflow as tf\rprint(tf.__version__)\r2.3.0\rOne thing we have to keep in mind is that while accessing a function, we have to always append the function by tf.linalg. It is possible to remove the tf part by importing the linalg library from tensorflow. But even then we have to append every function by linalg. In this post, we will always use tf.linalg followed by function name. This amounts to little more typing. But we will do this to remind ourselves that we are using linalg library of tensorflow. This might seem little awkward to seasoned users of MATLAB or Julia where you just need to type the function name to use it without having to write the library name all the time. Except that, linear algebra in tensorflow seems quite natural.\nNote: In this post, we will show some of the ways in which we can handle matrix operations in Tensorflow. We will mainly use 1D or 2D arrays in our examples. But matrix operations in Tensorflow are not limited to 2D arrays. In fact, the operations can be done on multidimensional arrays. If an array has more than 2 dimensions, the matrix operation is done on the last two dimensions and the same operation is carried across other dimensions. For example, if our array has a shape of (3,5,5), it can be thought of as 3 matrices each of shape (5,5). When we call a matrix function on this array, the matrix function is applied to all 3 matrices of shape (5,5). This is also true for higher dimensional arrays.\n\nBasics\rTensorflow operates on Tensors. Tensors are characterized by their rank. Following table shows different types of tensors and their corresponding rank.\n\r\rTensors\rRank\r\r\r\rScalars\rRank 0 Tensor\r\rVectors (1D array)\rRank 1 Tensor\r\rMatrices (2D array)\rRank 2 Tensor\r\r3D array\rRank 3 Tensor\r\r\r\r\nCreating tensors\rIn this section, we will create tensors of different rank, starting from scalars to multi-dimensional arrays. Though tensors can be both real or complex, we will mainly focus on real tensors.\nA scalar contains a single (real or complex) value.\na = tf.constant(5.0)\ra\r\u0026lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0\u0026gt;\rThe output shows that the result is a tf.Tensor. As scalars are rank 0 tensors, its shape is empty. Data type of the tensor is float32. And …","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"e340d57ded61a9e2220a4e697d2c2c88","permalink":"https://biswajitsahoo1111.github.io/post/doing-linear-algebra-using-tensorflow-2/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/post/doing-linear-algebra-using-tensorflow-2/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will explore ways of doing linear algebra only using tensorflow.","tags":["Linear Algebra","Tensorflow"],"title":"Doing Linear Algebra using Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog","Machine Learning","Deep Learning","Tensorflow"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple .csv files into Tensorflow using generators. But the method we will discuss is general enough to work for other file formats as well. We will demonstrate the procedure using 500 .csv files. These files have been created using random numbers. Each file contains only 1024 numbers in one column. This method can easily be extended to huge datasets involving thousands of .csv files. As the number of files becomes large, we can’t load the whole data into memory. So we have to work with chunks of it. Generators help us do just that conveniently. In this post, we will read multiple files using a custom generator.\nThis post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells.\nWe will use Tensorflow 2 to run our deep learning model. Tensorflow is very flexible. A given task can be done in different ways in it. The method we will use is not the only one. Readers are encouraged to explore other ways of doing the same. Below is an outline of three different tasks considered in this post.\nOutline:\rCreate 500 \u0026#34;.csv\u0026#34; files and save it in the folder “random_data” in current directory.\rWrite a generator that reads data from the folder in chunks and preprocesses it.\rFeed the chunks of data to a CNN model and train it for several epochs.\r\r\r1. Create 500 .csv files of random data\rAs we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.\n\rEach .csv file will have one column of data with 1024 entries.\n\rEach file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in “Fault_1”\rcategory will have names as “Fault_1_001.csv”, “Fault_1_002.csv”, “Fault_1_003.csv”, …, “Fault_1_100.csv”. Similarly for other classes.\n\r\rimport numpy as np\rimport os\rimport glob\rnp.random.seed(1111)\rFirst create a function that will generate random files.\ndef create_random_csv_files(fault_classes, number_of_files_in_each_class):\ros.mkdir(\u0026#34;./random_data/\u0026#34;) # Make a directory to save created files.\rfor fault_class in fault_classes:\rfor i in range(number_of_files_in_each_class):\rdata = np.random.rand(1024,)\rfile_name = \u0026#34;./random_data/\u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34;_\u0026#34; + \u0026#34;{0:03}\u0026#34;.format(i+1) + \u0026#34;.csv\u0026#34; # This creates file_name\rnp.savetxt(eval(\u0026#34;file_name\u0026#34;), data, delimiter = \u0026#34;,\u0026#34;, header = \u0026#34;V1\u0026#34;, comments = \u0026#34;\u0026#34;)\rprint(str(eval(\u0026#34;number_of_files_in_each_class\u0026#34;)) + \u0026#34; \u0026#34; + eval(\u0026#34;fault_class\u0026#34;) + \u0026#34; files\u0026#34; + \u0026#34; created.\u0026#34;)\rNow use the function to create 100 files each for five fault types.\ncreate_random_csv_files([\u0026#34;Fault_1\u0026#34;, \u0026#34;Fault_2\u0026#34;, \u0026#34;Fault_3\u0026#34;, \u0026#34;Fault_4\u0026#34;, \u0026#34;Fault_5\u0026#34;], number_of_files_in_each_class = 100)\r100 Fault_1 files created.\r100 Fault_2 files created.\r100 Fault_3 files created.\r100 Fault_4 files created.\r100 Fault_5 files created.\rfiles = glob.glob(\u0026#34;./random_data/*\u0026#34;)\rprint(\u0026#34;Total number of files: \u0026#34;, len(files))\rprint(\u0026#34;Showing first 10 files...\u0026#34;)\rfiles[:10]\rTotal number of files: 500\rShowing first 10 files...\r[\u0026#39;./random_data/Fault_1_001.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_002.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_003.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_004.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_005.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_006.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_007.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_008.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_009.csv\u0026#39;,\r\u0026#39;./random_data/Fault_1_010.csv\u0026#39;]\rTo extract labels from file name, extract the part of the file name that corresponds to fault type.\nprint(files[0])\r./random_data/Fault_1_001.csv\rprint(files[0][14:21])\rFault_1\rNow that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it.\n\r2. Write a generator that reads data in chunks and preprocesses it\rGenerator are similar to functions with one important difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. yield keyword converts a function into a generator. Generators can run for a fixed number of times or indefinitely depending on the loop structure used inside it. For our application, we will use a generator that runs indefinitely.\nThe following generator takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. This is determined by how much memory do we have. If all data can be loaded into memory, there …","date":1578528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578528000,"objectID":"ef874e3055dd24f7c7184dc0a2ed01cc","permalink":"https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/post/reading-multiple-files-in-tensorflow-2/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will read multiple .csv files into Tensorflow using generators.","tags":["Machine Learning","Deep Learning","Tensorflow"],"title":"Reading multiple files in Tensorflow 2","type":"post"},{"authors":[],"categories":["Blog","Python","Deep Learning","Tensorflow"],"content":"\r\r\rRun in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful for beginners.\nWhile functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It’s an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.\nFirst let’s create artificial data that we will extract later batch by batch.\nimport numpy as np\rdata = np.random.randint(100,150, size = (10,2,2))\rlabels = np.random.permutation(10)\rprint(data)\rprint(\u0026#34;labels:\u0026#34;, labels)\r[[[132 119]\r[126 119]]\r[[133 126]\r[144 140]]\r[[126 129]\r[116 146]]\r[[145 104]\r[143 143]]\r[[114 122]\r[102 148]]\r[[122 118]\r[145 134]]\r[[131 134]\r[122 104]]\r[[145 103]\r[136 138]]\r[[128 119]\r[141 118]]\r[[106 115]\r[124 130]]]\rlabels: [3 5 8 4 0 9 1 6 7 2]\rLet’s pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely.\ndef my_gen(data, labels, batch_size = 2):\ri = 0\rwhile True:\rif i*batch_size \u0026gt;= len(labels):\ri = 0\ridx = np.random.permutation(len(labels))\rdata, labels = data[idx], labels[idx]\rcontinue\relse:\rX = data[i*batch_size:(i+1)*batch_size,:]\ry = labels[i*batch_size:(i+1)*batch_size]\ri += 1\ryield X,y\rNote that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found here. Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found here.\nNow that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using “np.shuffle()”.\nget_data = my_gen(data,labels)\rfor i in range(10):\rX,y = next(get_data)\rprint(X,y)\rprint(X.shape, y.shape)\rprint(\u0026#34;=========================\u0026#34;)\r[[[132 119]\r[126 119]]\r[[133 126]\r[144 140]]] [3 5]\r(2, 2, 2) (2,)\r=========================\r[[[126 129]\r[116 146]]\r[[145 104]\r[143 143]]] [8 4]\r(2, 2, 2) (2,)\r=========================\r[[[114 122]\r[102 148]]\r[[122 118]\r[145 134]]] [0 9]\r(2, 2, 2) (2,)\r=========================\r[[[131 134]\r[122 104]]\r[[145 103]\r[136 138]]] [1 6]\r(2, 2, 2) (2,)\r=========================\r[[[128 119]\r[141 118]]\r[[106 115]\r[124 130]]] [7 2]\r(2, 2, 2) (2,)\r=========================\r[[[132 119]\r[126 119]]\r[[145 104]\r[143 143]]] [3 4]\r(2, 2, 2) (2,)\r=========================\r[[[131 134]\r[122 104]]\r[[126 129]\r[116 146]]] [1 8]\r(2, 2, 2) (2,)\r=========================\r[[[133 126]\r[144 140]]\r[[106 115]\r[124 130]]] [5 2]\r(2, 2, 2) (2,)\r=========================\r[[[114 122]\r[102 148]]\r[[122 118]\r[145 134]]] [0 9]\r(2, 2, 2) (2,)\r=========================\r[[[128 119]\r[141 118]]\r[[145 103]\r[136 138]]] [7 6]\r(2, 2, 2) (2,)\r=========================\rIn the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:\nimport tensorflow as tf\rprint(\u0026#34;Tensorflow Version: \u0026#34;, tf.__version__)\rTensorflow Version: 2.4.0\rclass my_new_gen(tf.keras.utils.Sequence):\rdef __init__(self, data, labels, batch_size= 2 ):\rself.x, self.y = data, labels\rself.batch_size = batch_size\rself.indices = np.arange(self.x.shape[0])\rdef __len__(self):\rreturn tf.math.floor(self.x.shape[0] / self.batch_size)\rdef …","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"b70c965c7d753586edab630f37b4bf87","permalink":"https://biswajitsahoo1111.github.io/post/using-python-generators/","publishdate":"2019-06-29T00:00:00Z","relpermalink":"/post/using-python-generators/","section":"post","summary":"Run in Google Colab\r\r\rView source on GitHub\r\r\rDownload notebook\r\r\rIn this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can’t be loaded into RAM.","tags":["Python","Deep Learning","Tensorflow"],"title":"Using Python Generators","type":"post"},{"authors":null,"categories":null,"content":"\r\r\rView GitHub Page\r\rView source on GitHub\r\r----\rDownload all code (.zip)\r\r\r\r\r\r\r\rIntroduction Condition based maintenance (CBM) is the process of doing maintenance only when it is required. Adoption of this maintenance strategy leads to significant monetary gains as it precludes periodic maintenance and reduces unplanned downtime. Another term commonly used for condition based maintenance is predictive maintenance. As the name suggests, in this method we predict in advance when to perform maintenance. Maintenance is required, if fault has already occurred or is imminent. This leads us to the problem of fault diagnosis and prognosis.\nIn fault diagnosis, fault has already occurred and our aim is to find what type of fault is there and what is its severity. In fault prognosis, our aim is to predict the time of occurrence of fault in future, given its present state. These two problem are central to condition based maintenance. There are many methods to solve these problems. These methods can be broadly divided into two groups:\n Model Based Approaches Data-Driven Approaches  In model based approach, a complete model of the system is formulated and it is then used for fault diagnosis and prognosis. But this method has several limitations. Firstly, it is a difficult task to accurately model a system. Modeling becomes even more challenging with variations in working conditions. Secondly, we have to formulate different models for different tasks. For example, to diagnose bearing fault and gear fault, we have to formulate two different models. Data-driven methods provide a convenient alternative to these problems.\nIn data-driven approach, we use operational data of the machine to design algorithms that are then used for fault diagnosis and prognosis. The operational data may be vibration data, thermal imaging data, acoustic emission data, or something else. These techniques are robust to environmental variations. Accuracy obtained by data-driven methods is also at par and sometimes even better than accuracy obtained by model based approaches. Due to these reasons data-driven methods are becoming increasingly popular at diagnosis and prognosis tasks.\nAim of the project In this project we will apply some of the standard machine learning techniques to publicly available data sets and show their results with code. There are not many publicly available data sets in machinery condition monitoring. So we will manage with those that are publicly available. Unlike machine learning community where almost all data and codes are open, in condition monitoring very few things are open, though some people are gradually making codes open. This project is a step towards that direction, even though a tiny one.\nThis is an ongoing project and modifications and additions of new techniques will be done over time. Python and R are two popular programming languages that are used in machine learning applications. We will use those for our demonstrations. Tensorflow will be used for deep learning applications. This page contains results on fault diagnosis only. Results on fault prognosis can be found here.\nResults using Case Western Reserve University Bearing Data*  We will first apply classical feature based methods (so-called shallow learning methods) to obtain results and then apply deep learning based methods. In feature based methods, we will extensively use wavelet packet energy features and wavelet packet entropy featues that are calculated from raw time domain data. Dataset description and time domain preprocessing steps can be found here. Steps to compute time domain features are explained in this notebook. The procedure detailing calculation of wavelet packet energy features can be found at this link and similar calculations for wavelet packet entropy features can be found at this link. Also see the following two notebooks for computation of wavelet packet features in Python: Wavelet packet energy features in Python and Wavelet packet entropy features in Python.\n  SVM on time domain features (10 classes, sampling frequency: 48k) (Overall accuracy: 96.5%) (Python code) (R code)\n  SVM on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 99.3%) (Python code) (R code)\n  Visualizing High Dimensional Data Using Dimensionality Reduction Techniques (Python Code) (R Code)\n  SVM on wavelet packet entropy features (10 classes, sampling frequency: 48k) (Overall accuracy: 99.3%) (Python code) (R code)\n  SVM on time and wavelet packet features (12 classes, sampling frequency: 12k) (Achieves 100% test accuracy in one case) (Python code) (R code)\n  Multiclass Logistic Regression on wavelet packet energy features (10 classes, sampling frequency: 48k) (Overall accuracy: 98.5%) (Python code) (R code)\n  Multiclass Logistic Regression on wavelet packet energy features (12 classes, sampling frequency: 12k) (Overall accuracy: 99.7%) (Python code) (R code)\n  LDA on wavelet packet energy features (10 classes, sampling …","date":1555545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555545600,"objectID":"842e6602c4f49a5d6534e54f013236fc","permalink":"https://biswajitsahoo1111.github.io/project/cbm_codes_open/","publishdate":"2019-04-18T00:00:00Z","relpermalink":"/project/cbm_codes_open/","section":"project","summary":"Aim of this project is to produce reproducible results in condition monitoring. We will apply some of the standard machine learning techniques to publicly available machinery datasets and show the results with code for fault diagnosis task. This is an ongoing project and will evolve over time. Related notebooks and data can be found at this [github page](https://biswajitsahoo1111.github.io/cbm_codes_open/).","tags":["Machine Learning","Deep Learning","Condition Monitoring"],"title":"Data-Driven Machinery Fault Diagnosis","type":"project"},{"authors":null,"categories":["Blog"],"content":"\r\r\rThis story was originally written for “Augmenting Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background. The story also featured in the top 100 list of stories for the award. Full list of awardees and their stories can be found here.\n\rPrelude\rRising sun with its gentle light marks the arrival of morning. Birds’ chirp as well as time on our clock, sometimes with a blaring alarm, confirm the arrival of morning. Each of these, among several others, is an indicator of the morning. But can we know about morning by following only one indicator? Let’s deliberate. What if the sky is cloudy and we don’t see the sun rising, will this mean that morning is yet to come? Of course not! Our alarm will remind us of morning irrespective of whether there is sun or not. But what if, on some occasion, our clock doesn’t work. In that case, birds may chirp or sun may rise or our near and dear ones may remind us that it’s morning already. So in essence, we usually don’t look for only one indicator. Rather, we consider several indicators. If one indicator fails, we can check another and thus be sure. It is very unlikely that all the indicators will fail simultaneously.\nSo the best way to get an idea about an event, it seems, is not to rely on only one indicator. Rather, observe several indicators and depending on their collective state, arrive at some conclusion. In this way, we deliberately add redundancy in order to get reliable results. This is exactly what we do in fault diagnosis of machines. Fault diagnosis is a broad term that addresses mainly three questions. First, find out whether fault is there in the machine or not. If fault is present, next question is to find the location of the fault. Once location of the fault is found, finally, find out the type of fault and its severity. In this article, we will only limit ourselves to the last aspect. But for simplicity, we will still use the term fault diagnosis to address that particular problem.\n\rThe method\rTo determine the health of a machine, we collect a set of indicators that best explain the condition of the machine. In scientific jargon, we call those features. Before discussing further, let’s first discuss what are those features and how they are calculated.\nFirst, data needs to be collected from a machine whose health needs to be assessed. Data might pertain to vibration level of the machine or its temperature distribution or the sound produced by the machine or something else. Sensors are needed to collect each type of data. By analogy, a thermometer, which is used to measure body temperature of humans, is a sensor that measures temperature. Likewise, different types of sensors are available to measure different quantities of interest related to the machine. From research it has been found that vibration based data are more suitable for fault diagnosis as compared to other types of data, say, temperature or sound. So in this article, we will limit our attention to vibration based fault diagnosis. And the sensor that is most commonly used to measure the vibration of a machine is called an accelerometer. Form the data collected by accelerometer(s) we calculate features like the maximum level of vibration, similarly, the minimum level and other statistical features like skewness, kurtosis, etc. It is not uncommon to collect 10-15 features.\nAfter feature collection, the next task is to find out what type of faults are present by using those features. One way to do this is by comparing the obtained feature values to pre-existing standards. But standards are available for few specialized cases when each feature is considered in isolation. For multiple features, no concrete information can be obtained from standards. The way out of this problem is to come up with an algorithm that takes all feature values as input and produces the output related to the type of fault present.\nConstruction of such an algorithm requires prior faulty and non-faulty data of similar machines be fed to it. The algorithm should ideally work well on this prior data. Once fine-tuning of its parameters are done, new data are fed into the algorithm and from its output, we infer the fault type. If the algorithm is carefully constructed, error in prediction of fault type will be very small. In some cases, it is also possible to get perfect accuracy. The approach just considered is a sub-class of a broad field called pattern recognition. In pattern recognition, we try to find underlying patterns in features that correspond to different fault types. This type of pattern recognition tasks are best performed by machine learning algorithms. The simple technique just described works fine for a large class of problems. But there exist some problems for which the features previously calculated are not sufficient to identify fault. However, it is possible to modify the …","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"c636f2ab35e1bb6e28ad0e75017c080f","permalink":"https://biswajitsahoo1111.github.io/post/fault-diagnosis-of-machines/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/fault-diagnosis-of-machines/","section":"post","summary":"This story was originally written for “Augmenting Writing Skills for Articulating Research (AWSAR)” award 2018. It is written in a non-technical way so as to be accessible to as many people as possible irrespective of their educational background.","tags":["Story"],"title":"Fault Diagnosis of Machines","type":"post"},{"authors":null,"categories":["Blog","Linear Algebra"],"content":"\r\rAlmost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution. In this way, three equations with three variables(or unknowns) gets transformed to two equations in two variables and one further step of reduction gives us an equation with only one variable which is readily solvable. Then the final solution is obtained by back substituting the obtained value of the variable into remaining equations. This method, in mathematical jargon, is called Gaussian elimination and back substitution.\nIt turns out (surprisingly) that linear systems form the basis of many interesting engineering applications. Ultimately the problem boils down to solution (or approximate solution) of a system of linear equations. So a thorough understanding of linear systems is essential to appreciate the applications. In this post we will outline all possible cases of finding solutions to linear systems and briefly outline two most important applications.\nWe will use matrix notation to represent the equations succinctly. It also gives us better insight into their solution. Using matrix notation the system can be represented as\r\\[\\textbf{Ax}=\\textbf{b}\\]\rWhere \\(\\textbf{A}\\) is the matrix of coefficients of size \\((m\\times n)\\), \\(\\textbf{x}\\) is a vector of variables of size \\((n\\times 1)\\), and \\(\\textbf{b}\\)\ris a vector of size \\((m\\times 1)\\) representing constant right hand sides. Note that \\(\\textbf{b}\\) can be a vector of all zeros, i.e., \\(\\textbf{b} = \\textbf{0}\\) or it can be any arbitrary vector with some nonzero values, i.e.,\\(\\textbf{b}\\neq \\textbf{0}\\). The solution(s) of linear systems depend to a large extent on what the right hand side is as we will see shortly.\nApart from notation, we need two other concepts from matrix theory. One is of rank and other is the range space (or column space) of a matrix. Rank \\((Rank(\\textbf{A}))\\) of a matrix (say, \\(\\textbf{A}\\)) is defined as number of independent rows or columns of a matrix. It is a well known result in matrix theory that row rank (number of independent rows) is equal to column rank (number of independent columns) and \\(Rank(\\textbf{A})\\leq min(m,n)\\).\nRange space \\((\\mathcal{R}(A))\\)(in short, Range) of a matrix is the vector space of all possible linear combinations of columns of the matrix. As we take all possible linear combination of columns, it is also known as column space. Readers who are slightly more familiar with linear algebra may know that Range is the span of columns of \\(\\textbf{A}\\). Zero vector \\((\\textbf{0})\\) is always in the range of \\(\\textbf{A}\\) because if we take linear combination of columns of \\(\\textbf{A}\\) with all coefficients as 0’s, we get zero vector. Hence \\(\\textbf{b}=0 \\in \\mathcal{R}(\\textbf{A})\\) is always true.\nLet’s now discuss different cases separately and their solutions. We will assume that our system of equations has real entries.\n\rCase - I: \\((m = n)\\)\n\r\\(Rank(\\textbf{A}) = m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution (for any \\(\\textbf{b}\\)). For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 8 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : Impossible (This case will never happen because \\(Rank(\\textbf{A})=m\\))\r\r\\(Rank(\\textbf{A}) \u0026lt; m\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Infinitely many solutions. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r6 \\\\\r8\r\\end{bmatrix}\r\\end{equation}\\] This system has infinitely many solutions.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \u0026amp; 3 \\\\\r2 \u0026amp; 4 \u0026amp; 6 \\\\\r3 \u0026amp; 5 \u0026amp; 7 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3\r\\end{bmatrix}\r= \\begin{bmatrix}\r1 \\\\\r5 \\\\\r7\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution.\r\r\rCase - II: \\((m \u0026gt; n)\\)\n\r\\(Rank(\\textbf{A}) = n\\)\n\r\\(\\textbf{b} \\in \\mathcal{R}(\\textbf{A})\\) : Unique solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has unique solution.\r\\(\\textbf{b} \\not\\in \\mathcal{R}(\\textbf{A})\\) : No solution. For example,\\[ \\begin{equation}\r\\begin{bmatrix}\r1 \u0026amp; 2 \\\\\r2 \u0026amp; 7 \\\\\r3 \u0026amp; 8 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\end{bmatrix}\r= \\begin{bmatrix}\r3 \\\\\r9 \\\\\r11\r\\end{bmatrix}\r\\end{equation}\\] This system has no solution. But this case is immensely useful from application point of view. Sometimes it is not desirable to obtain the exact solution. Rather an approximate solution …","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"9c0ffacff5acf60c78af4bb9028f3cad","permalink":"https://biswajitsahoo1111.github.io/post/revisiting-systems-of-linear-equations/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/post/revisiting-systems-of-linear-equations/","section":"post","summary":"Almost every reader would have seen systems of linear equations from their high school days. Whether they liked it or not is a separate story. But, in all likelihood, they would have solved these equations by gradually removing variables one by one by substitution.","tags":["Linear Algebra"],"title":"Revisiting Systems of Linear Equations","type":"post"},{"authors":null,"categories":["Blog","PCA","Machine Learning","R","Python"],"content":"\r\rRun Python code in Google Colab\r\rDownload Python code\r\rDownload R code (R Markdown)\r\r\rIn this post, we will reproduce the results of a popular paper on PCA. The paper is titled ‘Principal component analysis’ and is authored by Herve Abdi and Lynne J. Williams. It got published in 2010 and since then its popularity has only grown. Its number of citations are more than 4800 as per Google Scholar data (This was the number when this post was last revised).\nThis post is Part-III of a three part series on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-II: PCA Implementation with and without using built-in functions\r\rThis post contains code snippets in R. Equivalent MATLAB codes can be written using commands of Part-II. For figures, the reader has to write his/her own code in MATLAB.\nStructure of the paper\rAlong with basic theory, the paper contains three examples on PCA, one example on correspondence analysis, and one example on multiple factor analysis. We will only focus on PCA examples in this post.\nTo run following R codes seamlessly, readers have to load following packages. If these packages have not been installed previously, use install.packages(\u0026#34;package_name\u0026#34;) to install those.\nlibrary(ggplot2)\rlibrary(ggrepel)\r\rHow to get data\rData for the examples have been taken from the paper [1]. The datasets are pretty small. So one way to read the data is to create a dataframe itself in R using the values given in paper. Otherwise, the values can first be stored in a csv file and then read into R. To make this post self-sufficient, we will adopt the former approach.\nNote: Throughout this article, additional comments have been made beside code segments. It would be a good idea to read those commented lines along with the codes.\n# Table 1\r# Create a dataframe\rWords = c(\u0026#34;Bag\u0026#34;, \u0026#34;Across\u0026#34;, \u0026#34;On\u0026#34;, \u0026#34;Insane\u0026#34;, \u0026#34;By\u0026#34;, \u0026#34;Monastery\u0026#34;, \u0026#34;Relief\u0026#34;, \u0026#34;Slope\u0026#34;, \u0026#34;Scoundrel\u0026#34;, \u0026#34;With\u0026#34;, \u0026#34;Neither\u0026#34;, \u0026#34;Pretentious\u0026#34;, \u0026#34;Solid\u0026#34;, \u0026#34;This\u0026#34;, \u0026#34;For\u0026#34;, \u0026#34;Therefore\u0026#34;, \u0026#34;Generality\u0026#34;, \u0026#34;Arise\u0026#34;, \u0026#34;Blot\u0026#34;, \u0026#34;Infectious\u0026#34;)\rWord_length = c(3, 6, 2, 6, 2, 9, 6, 5, 9, 4, 7, 11, 5, 4, 3, 9, 10, 5, 4, 10)\rLines_in_dict = c(14, 7, 11, 9, 9, 4, 8, 11, 5, 8, 2, 4, 12, 9, 8, 1, 4, 13, 15, 6)\rwords = data.frame(Words, Word_length, Lines_in_dict, stringsAsFactors = F)\rwords\r Words Word_length Lines_in_dict\r1 Bag 3 14\r2 Across 6 7\r3 On 2 11\r4 Insane 6 9\r5 By 2 9\r6 Monastery 9 4\r7 Relief 6 8\r8 Slope 5 11\r9 Scoundrel 9 5\r10 With 4 8\r11 Neither 7 2\r12 Pretentious 11 4\r13 Solid 5 12\r14 This 4 9\r15 For 3 8\r16 Therefore 9 1\r17 Generality 10 4\r18 Arise 5 13\r19 Blot 4 15\r20 Infectious 10 6\r(words_centered = scale(words[,2:3],scale = F)) # Centering after reemoving the first column\r Word_length Lines_in_dict\r[1,] -3 6\r[2,] 0 -1\r[3,] -4 3\r[4,] 0 1\r[5,] -4 1\r[6,] 3 -4\r[7,] 0 0\r[8,] -1 3\r[9,] 3 -3\r[10,] -2 0\r[11,] 1 -6\r[12,] 5 -4\r[13,] -1 4\r[14,] -2 1\r[15,] -3 0\r[16,] 3 -7\r[17,] 4 -4\r[18,] -1 5\r[19,] -2 7\r[20,] 4 -2\rattr(,\u0026#34;scaled:center\u0026#34;)\rWord_length Lines_in_dict 6 8 \r\rCovariance PCA\rCovariance PCA uses centered data matrix. But data matrix is not scaled. prcomp() centers data by default.\npca_words_cov = prcomp(words[,2:3],scale = F) # cov stands for Covariance PCA\rfactor_scores_words = pca_words_cov$x\rround(factor_scores_words,2)\r PC1 PC2\r[1,] -6.67 0.69\r[2,] 0.84 -0.54\r[3,] -4.68 -1.76\r[4,] -0.84 0.54\r[5,] -2.99 -2.84\r[6,] 4.99 0.38\r[7,] 0.00 0.00\r[8,] -3.07 0.77\r[9,] 4.14 0.92\r[10,] -1.07 -1.69\r[11,] 5.60 -2.38\r[12,] 6.06 2.07\r[13,] -3.91 1.30\r[14,] -1.92 -1.15\r[15,] -1.61 -2.53\r[16,] 7.52 -1.23\r[17,] 5.52 1.23\r[18,] -4.76 1.84\r[19,] -6.98 2.07\r[20,] 3.83 2.30\rObserver that factor scores for PC1 are negatives of what has been given in the paper. This is not a problem as principal directions are orthogonal.\n\rPrincipal directions are orthogonal\r# It can also be checked that both the principal components are orthogonal.\rsum(factor_scores_words[,1]*factor_scores_words[,2]) # PCs are orthogonal\r[1] -4.773959e-15\r\rContribution of each factor\rIt is defined as square of factor score divided by sum of squares of factor scores in that column.\nround(factor_scores_words[,1]^2/sum(factor_scores_words[,1]^2)*100,2)\r [1] 11.36 0.18 5.58 0.18 2.28 6.34 0.00 2.40 4.38 0.29 8.00 9.37\r[13] 3.90 0.94 0.66 14.41 7.78 5.77 12.43 3.75\rround(factor_scores_words[,2]^2/sum(factor_scores_words[,2]^2)*100,2)\r [1] 0.92 0.55 5.98 0.55 15.49 0.28 0.00 1.13 1.63 5.48 10.87 8.25\r[13] 3.27 2.55 12.32 2.90 2.90 6.52 8.25 10.18\rThe calculations in above two lines can be done in a single line\nround(factor_scores_words^2/matrix(rep(colSums(factor_scores_words^2),nrow(words)),ncol = 2,byrow = T)*100,2)\r PC1 PC2\r[1,] 11.36 0.92\r[2,] 0.18 0.55\r[3,] 5.58 5.98\r[4,] 0.18 0.55\r[5,] 2.28 15.49\r[6,] 6.34 0.28\r[7,] 0.00 0.00\r[8,] 2.40 1.13\r[9,] 4.38 1.63\r[10,] 0.29 5.48\r[11,] 8.00 10.87\r[12,] 9.37 8.25\r[13,] 3.90 3.27\r[14,] 0.94 2.55\r[15,] 0.66 12.32\r[16,] 14.41 2.90\r[17,] 7.78 2.90\r[18,] 5.77 6.52\r[19,] 12.43 8.25\r[20,] 3.75 10.18\r\rSquared distance to center of gravity\r(dist = …","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"ddbd45a4637152e3e7f2ee5ff651f975","permalink":"https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-iii/","section":"post","summary":"Run Python code in Google Colab\r\rDownload Python code\r\rDownload R code (R Markdown)\r\r\rIn this post, we will reproduce the results of a popular paper on PCA.","tags":["PCA","Machine Learning","R","Python"],"title":"Principal Component Analysis - Part III","type":"post"},{"authors":null,"categories":["Blog","PCA","Machine Learning","R","Python","MATLAB"],"content":"\r\rRun Python code in Google Colab\r\rDownload Python code\r\rDownload R code (R Markdown)\r\rDownload MATLAB code\r\r\rThis post is Part-II of a three part series post on PCA. Other parts of the series can be found at the links below.\n\rPart-I: Basic Theory of PCA\rPart-III: Reproducing results of a published paper on PCA\r\rIn this post, we will first apply built in commands to obtain results and then show how the same results can be obtained without using built-in commands. Through this post our aim is not to advocate the use of non-built-in functions. Rather, in our opinion, it enhances understanding by knowing what happens under the hood when a built-in function is called. In actual applications, readers should always use built functions as they are robust(almost always) and tested for efficiency.\nThis post is written in R. Equivalent MATLAB code for the same can be obtained from this link.\nWe will use French food data form reference [2]. Refer to the paper to know about the original source of the data. We will apply different methods to this data and compare the result. As the dataset is pretty small, one way to load the data into R is to create a dataframe in R using the values in the paper. Another way is to first create a csv file and then read the file into R/MATLAB. We have used the later approach.\nLoad Data\r#Create a dataframe of food data\rclass = rep(c(\u0026#34;Blue_collar\u0026#34;, \u0026#34;White_collar\u0026#34;, \u0026#34;Upper_class\u0026#34;), times = 4)\rchildren = rep(c(2,3,4,5), each = 3)\rbread = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515)\rvegetables = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097)\rfruit = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887)\rmeat = c(1437, 1527, 1948, 1507, 1501, 2345, 1620, 1856, 2366, 1848, 2056, 2630)\rpoultry = c(526, 567, 927, 544, 558, 1148, 638, 762, 1149, 759, 893, 1167)\rmilk = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561)\rwine = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)\rfood = data.frame(class, children, bread, vegetables, fruit, meat, poultry, milk, wine, stringsAsFactors = F)\rfood\r class children bread vegetables fruit meat poultry milk wine\r1 Blue_collar 2 332 428 354 1437 526 247 427\r2 White_collar 2 293 559 388 1527 567 239 258\r3 Upper_class 2 372 767 562 1948 927 235 433\r4 Blue_collar 3 406 563 341 1507 544 324 407\r5 White_collar 3 386 608 396 1501 558 319 363\r6 Upper_class 3 438 843 689 2345 1148 243 341\r7 Blue_collar 4 534 660 367 1620 638 414 407\r8 White_collar 4 460 699 484 1856 762 400 416\r9 Upper_class 4 385 789 621 2366 1149 304 282\r10 Blue_collar 5 655 776 423 1848 759 495 486\r11 White_collar 5 584 995 548 2056 893 518 319\r12 Upper_class 5 515 1097 887 2630 1167 561 284\r# Centerd data matrix\rcent_food = scale(food[,3:9],scale = F)\r# Scaled data matrix\rscale_food = scale(food[,3:9],scale = T)\r\rCovariance PCA\rUsing built-in function\r# Using built-in function\rpca_food_cov = prcomp(food[,3:9],scale = F)\r# Loading scores (we have printed only four columns out of seven)\r(round(pca_food_cov$rotation[,1:4],2))\r PC1 PC2 PC3 PC4\rbread 0.07 -0.58 -0.40 0.11\rvegetables 0.33 -0.41 0.29 0.61\rfruit 0.30 0.10 0.34 -0.40\rmeat 0.75 0.11 -0.07 -0.29\rpoultry 0.47 0.24 -0.38 0.33\rmilk 0.09 -0.63 0.23 -0.41\rwine -0.06 -0.14 -0.66 -0.31\r# Factor score (we have printed only four PCs out of seven)\rWe have printed only four columns of loading scores out of seven.\n(round(pca_food_cov$x[,1:4],2))\r PC1 PC2 PC3 PC4\r[1,] -635.05 120.89 -21.14 -68.97\r[2,] -488.56 142.33 132.37 34.91\r[3,] 112.03 139.75 -61.86 44.19\r[4,] -520.01 -12.05 2.85 -13.70\r[5,] -485.94 -1.17 65.75 11.51\r[6,] 588.17 188.44 -71.85 28.56\r[7,] -333.95 -144.54 -34.94 10.07\r[8,] -57.51 -42.86 -26.26 -46.55\r[9,] 571.32 206.76 -38.45 3.69\r[10,] -39.38 -264.47 -126.43 -12.74\r[11,] 296.04 -235.92 58.84 87.43\r[12,] 992.83 -97.15 121.13 -78.39\rWe have printed only four principal components out of seven.\n# Variances using built-in function\r(round(pca_food_cov$sdev^2,2))\r[1] 274831.02 26415.99 6254.11 2299.90 2090.20 338.39 65.81\r# Total variance\r(sum(round(pca_food_cov$sdev^2,2)))\r[1] 312295.4\r\r\rComparison of variance before and after transformation\r# Total variance before transformation\rsum(diag(cov(food[,3:9])))\r[1] 312295.4\r# Total variance after transformation\rsum(diag(cov(pca_food_cov$x)))\r[1] 312295.4\rAnother important observation is to see how variance of each variable before transformation changes into variance of principal components. Note that total variance in this process remains same as seen from above codes.\n# Variance along variables before transformation\rround(diag(cov(food[,3:9])),2)\r bread vegetables fruit meat poultry milk wine 11480.61 35789.09 27255.45 156618.39 62280.52 13718.75 5152.63 \rNote that calculation of variance is unaffected by centering data matrix. So variance of original data matrix as well as centered data matrix is same. Check it for yourself. Now see how PCA transforms these variance.\n# Variance along principal compoennts …","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549238400,"objectID":"f29eb67c962111ec5533911c55bc9ada","permalink":"https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-ii/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-ii/","section":"post","summary":"Run Python code in Google Colab\r\rDownload Python code\r\rDownload R code (R Markdown)\r\rDownload MATLAB code\r\r\rThis post is Part-II of a three part series post on PCA.","tags":["PCA","Machine Learning","R","Python","MATLAB"],"title":"Principal Component Analysis - Part II","type":"post"},{"authors":null,"categories":["Blog","PCA","Machine Learning","R","Python"],"content":"\r\rIn this post, we will discuss about Principal Component Analysis (PCA),\rone of the most popular dimensionality reduction techniques used in\rmachine learning. Applications of PCA and its variants are ubiquitous.\rThus, a through understanding of PCA is considered essential to start\rone’s journey into machine learning. In this and subsequent posts, we\rwill first briefly discuss relevant theory of PCA. Then we will\rimplement PCA from scratch without using any built-in function. This\rwill give us an idea as to what happens under the hood when a built-in\rfunction is called in any software environment. Simultaneously, we will\ralso show how to use built-in commands to obtain results. Finally, we\rwill reproduce the results of a popular paper on PCA. Including all this\rin a single post will make it very very long. Therefore, the post has\rbeen divided into three parts. Readers totally familiar with PCA should\rread none and leave this page immediately to save their precious time.\rOther readers, who have a passing knowledge of PCA and want to see\rdifferent implementations, should pick and choose material from\rdifferent parts as per their need. Absolute beginners should start with\rPart-I and work their way through gradually. Beginners are also\rencouraged to explore the references at the end of this post for further\rinformation. Here is the outline of different parts:\n\rPart-I: Basic Theory of\rPCA\rPart-II: PCA Implementation with and without using built-in\rfunctions\rPart-III: Reproducing results of a published paper on\rPCA\r\rFor\rPart-II,\rPython, R, and MATLAB code are available to reproduce all the results.\rPart-III\rcontains both R and Python code to reproduce results of the paper. In\rthis post, we will discuss the theory behind PCA in brief.\nPrincipal Component Analysis\rTheory:\rGiven a data matrix, we apply PCA to transform it in a way such that the\rtransformed data reveals maximum information. So we have to first get\rthe data on which we want to perform PCA. The usual convention in\rstoring data is to place variables as columns and different observations\ras rows (Data frames in R follow this convention by default). For\rexample, let’s suppose we are collecting data about daily weather for a\ryear. Our variables of interest may include maximum temperature in a\rday, minimum temperature, humidity, max. wind speed, etc. Everyday we\rcollect observations for each of these variables. In vector form, our\rdata point for one day will contain number of observations equal to the\rnumber of variables under study and this becomes one row of our data\rmatrix. Assuming that we are observing 10 variables everyday, our data\rmatrix for one year (assuming it’s not a leap year) will contain 365\rrows and 10 columns. Once data matrix is obtained, further analysis is\rdone on this data matrix to obtain important hidden information\rregarding the data. We will use notations from matrix theory to simplify\rour analysis.\nLet \\(\\textbf{X}\\) be the data matrix of size \\(n\\times p\\), where \\(n\\) is\rthe number of data points and \\(p\\) is the number of variables. We can\rassume without any loss of generality that data is centered, meaning its\rcolumn means are zero. This only shifts the data towards the origin\rwithout changing their relative orientation. So if originally not\rcentered, it is first centered before doing PCA. From now onward we will\rassume that data matrix is always centered.\nVariance of a variable (a column) in \\(\\textbf{X}\\) is equal to sum of\rsquares of entries (because the column is centered) of that column\rdivided by (n - 1) (to make it unbiased). So sum of variance of all\rvariables is \\(\\frac{1}{n - 1}\\) times sum of squares of all elements of\rthe matrix . Readers who are familiar with matrix norms would instantly\rrecognize that total variance is \\(\\frac{1}{n - 1}\\) times the square of\rFrobenius norm of \\(\\textbf{X}\\). Frobenius norm is nothing but square\rroot of sum of squares of all elements of a matrix.\r\\[ \\|\\textbf{X}\\|_{F} = (\\sum_{i,j}{x_{ij}^2})^{\\frac{1}{2}}=\\sqrt{trace(\\textbf{X}^T\\textbf{X}})=\\sqrt{trace(\\textbf{X}\\textbf{X}^T})\\]\nUsing this definition, total variance before transformation =\r\\[\\begin{aligned}\\frac{1}{n-1}\\sum_{i,j}{x_{ij}^2}=\\frac{1}{n-1}trace(\\textbf{X}^T\\textbf{X}) \u0026amp;= trace(\\frac{1}{n-1}\\textbf{X}^T\\textbf{X}) \\\\ \u0026amp;= \\frac{1}{n-1}\\|\\textbf{X}\\|_{F}^2\\end{aligned}\\]\nWhere, trace of a matrix is the sum of its diagonal entries and\r\\(\\|\\textbf{X}\\|_{F}^2\\) is the square of Frobenius norm.\nThe aim of PCA is to transform the data in such a way that along first\rprincipal direction, variance of transformed data is maximum. It\rsubsequently finds second principal direction orthogonal to the first\rone in such a way that it explains maximum of the remaining variance\ramong all possible direction in the orthogonal subspace.\nIn matrix form the transformation can be written as\r\\[\\textbf{Y}_{n\\times p}=\\textbf{X}_{n\\times p}\\textbf{P}_{p\\times p}\\]\rWhere \\(\\textbf{Y}\\) is the transformed data matrix. The columns of\r\\(\\textbf{Y}\\) …","date":1549152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549152000,"objectID":"e55d5e5aff1a836024ccd0020892b48d","permalink":"https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/","publishdate":"2019-02-03T00:00:00Z","relpermalink":"/post/principal-component-analysis-part-i/","section":"post","summary":"In this post, we will discuss about Principal Component Analysis (PCA),\rone of the most popular dimensionality reduction techniques used in\rmachine learning. Applications of PCA and its variants are ubiquitous.","tags":["PCA","Machine Learning","R","Python"],"title":"Principal Component Analysis - Part I","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://biswajitsahoo1111.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]